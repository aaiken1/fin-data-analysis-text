
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>12. Regression topics &#8212; Data Analysis in Finance</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="13. Logit models" href="13_logit_credit.html" />
    <link rel="prev" title="11. Factor models" href="11_factor_models.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/cat.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Data Analysis in Finance</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Welcome
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Getting Started
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="1_finpyth.html">
   1. Why Python and finance?
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="2_1_intro.html">
   2. Python set-up
   <a id="python_set-up">
   </a>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="2_2_git.html">
     2.2. Using Github and Github Codespaces
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="2_3_install.html">
     2.3. Local Installation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="2_4_jupyter.html">
     2.4. Jupyter notebooks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="2_5_vscode.html">
     2.5. VS Code
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="2_6_googlecolab.html">
     2.6. Google Colab
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="2_7_markdown.html">
     2.7. Markdown
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="2_8_packages.html">
     2.8. Packages
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="2_9_style.html">
     2.9. Code style, PEP8, and linting
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="2_10_using_chatgpt.html">
     2.10. Using ChatGPT
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  The Basics
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="3_1_intro.html">
   3. CompSci 101: Types, control, and numpy arrays
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="3_2_comp101.html">
     3.1. The Basics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="3_3_numpy.html">
     3.2. Numpy and arrays
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="4_intro.html">
   4. Working with data
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="4_0_dataframes.html">
     4.2. Importing data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="4_1_pandas.html">
     4.3. pandas
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="4_2_cleaning_data.html">
     4.4. Cleaning our data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="4_3_eda.html">
     4.5. Exploratory data analysis (EDA)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="4_4_merge_shape.html">
     4.6. Merging and reshaping data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="4_5_sql.html">
     4.7. Using SQL in Python
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="4_6_polar.html">
     4.8. polars: A fast, fancy pandas alternative
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="5_intro.html">
   5. Data visualization
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="5_0_seaborn.html">
     5.1. seaborn
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="5_1_matplotlib.html">
     5.2. matplotlib
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="5_2_plotly.html">
     5.3. plotly
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="6_nasdaq_api.html">
   6. Using APIs for Data Imports
   <a id="btc-sim">
   </a>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="7_financial_ts.html">
   7. Financial time series
   <a id="financial-time-series">
   </a>
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Applications
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="8_port_math.html">
   8. Essential portfolio math
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="9_portfolio_opt.html">
   9. Portfolio optimization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="10_unsupervised_learning.html">
   10. Unsupervised Learning
   <a id="unsupervised_learning">
   </a>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="11_factor_models.html">
   11. Factor models
   <a id="factor_models">
   </a>
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   12. Regression topics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="13_logit_credit.html">
   13. Logit models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="14_decision_trees.html">
   14. Decision Trees
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="15_svm.html">
   15. SVM
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="16_risk_mang.html">
   16. Risk management
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="17_monte_carlo.html">
   17. Monte Carlo and portfolios
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="18_options.html">
   18. Option basics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="19_reinforcement_learning.html">
   19. Reinforcement Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="20_trading_bt.html">
   20. Trading Strategies and the BT Package
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/docs/chapters/12_regression_topics.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>
<a href="https://github.com/executablebooks/jupyter-book"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="bottom"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/chapters/12_regression_topics.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#supervised-learning-and-the-machine-learning-process-hull-chapter-1">
   12.1. Supervised Learning and the Machine Learning Process. Hull, Chapter 1.
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#statsmodel-vs-sklearn">
   12.2. statsmodel vs. sklearn
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#workflow">
   12.3. Workflow
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#example-using-zillow-pricing-errors">
   12.4. Example using Zillow pricing errors
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#indicators-and-categorical-variables">
   12.5. Indicators and categorical variables
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#using-get-dummies">
     12.5.1. Using get_dummies
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#hull-chapters-3-and-10-linear-regression-and-prediction">
   12.6. Hull, Chapters 3 and 10. Linear regression and prediction.
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#machine-learning-and-prediction">
     12.6.1. Machine learning and prediction
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#important-regression-statistics">
     12.6.2. Important regression statistics
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#categorical-features-again">
     12.6.3. Categorical features, again
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#regularization-and-ridge-regression">
     12.6.4. Regularization and Ridge regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#lasso-regression">
     12.6.5. LASSO regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#elastic-net-regression">
     12.6.6. Elastic net regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#using-our-tools-to-predict-housing-prices">
     12.6.7. Using our tools to predict housing prices
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#making-predictions">
     12.6.8. Making predictions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#ridge-regression">
     12.6.9. Ridge regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     12.6.10. Lasso regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#elastic-net">
     12.6.11. Elastic net
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#prediction-workflow-with-sklearn">
   12.7. Prediction workflow with sklearn
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#standardizing-your-data">
     12.7.1. Standardizing your data
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#splitting-your-data">
     12.7.2. Splitting your data
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cross-validation-and-hyperparamters">
     12.7.3. Cross-validation and hyperparamters
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pipelines">
     12.7.4. Pipelines
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hyperparamters-and-model-overfit">
     12.7.5. Hyperparamters and model overfit
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#interpreting-linear-regression-models">
     12.7.6. Interpreting linear regression models
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#some-worked-examples">
     12.7.7. Some worked examples
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Regression topics</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#supervised-learning-and-the-machine-learning-process-hull-chapter-1">
   12.1. Supervised Learning and the Machine Learning Process. Hull, Chapter 1.
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#statsmodel-vs-sklearn">
   12.2. statsmodel vs. sklearn
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#workflow">
   12.3. Workflow
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#example-using-zillow-pricing-errors">
   12.4. Example using Zillow pricing errors
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#indicators-and-categorical-variables">
   12.5. Indicators and categorical variables
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#using-get-dummies">
     12.5.1. Using get_dummies
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#hull-chapters-3-and-10-linear-regression-and-prediction">
   12.6. Hull, Chapters 3 and 10. Linear regression and prediction.
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#machine-learning-and-prediction">
     12.6.1. Machine learning and prediction
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#important-regression-statistics">
     12.6.2. Important regression statistics
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#categorical-features-again">
     12.6.3. Categorical features, again
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#regularization-and-ridge-regression">
     12.6.4. Regularization and Ridge regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#lasso-regression">
     12.6.5. LASSO regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#elastic-net-regression">
     12.6.6. Elastic net regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#using-our-tools-to-predict-housing-prices">
     12.6.7. Using our tools to predict housing prices
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#making-predictions">
     12.6.8. Making predictions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#ridge-regression">
     12.6.9. Ridge regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     12.6.10. Lasso regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#elastic-net">
     12.6.11. Elastic net
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#prediction-workflow-with-sklearn">
   12.7. Prediction workflow with sklearn
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#standardizing-your-data">
     12.7.1. Standardizing your data
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#splitting-your-data">
     12.7.2. Splitting your data
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cross-validation-and-hyperparamters">
     12.7.3. Cross-validation and hyperparamters
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pipelines">
     12.7.4. Pipelines
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hyperparamters-and-model-overfit">
     12.7.5. Hyperparamters and model overfit
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#interpreting-linear-regression-models">
     12.7.6. Interpreting linear regression models
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#some-worked-examples">
     12.7.7. Some worked examples
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="regression-topics">
<h1><span class="section-number">12. </span>Regression topics<a class="headerlink" href="#regression-topics" title="Permalink to this headline">#</a></h1>
<p>This section will go into more detail on running regressions in Python. We already saw an example using <a class="reference external" href="10_factor_models.html#factor_models">factor models</a>, like the CAPM and Fama-French 3-factor models.</p>
<p>We could spend an entire semester going over linear regression, how to put together models, how to interpret models, and all of the adjustments that we can make. In fact, this is basically what a first-semester Econometrics class is!</p>
<p>This chapter is divided into two parts. I start by following code examples from <a class="reference external" href="https://aeturrell.github.io/coding-for-economists/econmt-regression.html">Coding for Economists</a>, which has just about everything you need to know to do basic linear regression (OLS) in Python. This gets us our first look at a linear model and some output.</p>
<p>I then turn to <strong>machine learning</strong> and <strong>Chapters 3 and 10 from our Hull textbook</strong>. These chapters also focus on linear regression and how to interpret these types of model. However, we’ll also see some newer regression topics from the machine learning perspective, such as <strong>regularization, Ridge regression, and LASSO regression</strong>.  They also discuss logit models, something that we get into in the next set of notes.</p>
<p><a class="reference external" href="https://theeffectbook.net">The Effect</a> is a great book for getting starting with econometrics, regression, and how to add meaning to the regressions that we’re running. <a class="reference external" href="https://theeffectbook.net/ch-StatisticalAdjustment.html">Chapter 13</a> of that book covers regression (with code in R). Like most econometrics, that book is focused on <strong>causality</strong> - did some intervention or change <strong>cause</strong> a change in behavior or another outcome? These tools are beyond the scope of this course, but we have a new course on Causal Inference if you’re interested.</p>
<p>You’ll also see categorical variables and logs. Thinking about how to define our variables is an example of <strong>feature engineering</strong> in the machine learning world.</p>
<section id="supervised-learning-and-the-machine-learning-process-hull-chapter-1">
<h2><span class="section-number">12.1. </span>Supervised Learning and the Machine Learning Process. Hull, Chapter 1.<a class="headerlink" href="#supervised-learning-and-the-machine-learning-process-hull-chapter-1" title="Permalink to this headline">#</a></h2>
<p>Chapter 1 of Hull starts us the Hull book starts our discussion of machine learning. Tradtional econometrics is about <strong>explaining</strong>. Machine learning is about <strong>predicting</strong>. Roughly speaking.</p>
<p>We saw <strong>unsupervised learning</strong> when looking at clustering. Regression gets us into <strong>supervised learning</strong>. From Hull, Chapter 1:</p>
<blockquote>
<div><p>Supervised learning is concerned with using data to make <strong>predictions</strong>. In the next section, we will show how a simple regression model can be used to predict salaries. This is an example of supervised learning. In Chapter 3, we will consider how a similar model can be used to  predict house prices. We can distinguish between supervised learning models used to predict a variable that can take a continuum of values (such as an individual’s salary or the price of a house) and supervised learning models that are used for classification.</p>
</div></blockquote>
<blockquote>
<div><p>The data for supervised learning contains what are referred to as  features and labels. The labels are the values of the target (e.g., the value  of a house or whether a particular loan was repaid). The features are  the variables from which the predictions about the target are made.</p>
</div></blockquote>
<p>Prediction means that we’re going to approach things a bit differently. In particular, we are going to think carefully about <strong>in-sample</strong> vs. <strong>out-of-sample</strong> prediction. From Hull, Chapter 1:</p>
<blockquote>
<div><p>When a data set is used for forecasting or determining a decision strategy, there is a danger that the machine learning model will work well for the data set but will not generalize well to other data. As statisticians have realized for a long time, it is important to test a model out-of-sample. By this we mean that the model should be tested on data that is different from the data used to determine the parameters of the model.</p>
</div></blockquote>
<p>Our process is going to have us <strong>fit</strong> a model using <strong>training</strong> data. We might use some <strong>validation</strong> data to <strong>fine-tune</strong> the model. Finally, we <strong>evaluate</strong> the model using our <strong>testing</strong> data.</p>
</section>
<section id="statsmodel-vs-sklearn">
<h2><span class="section-number">12.2. </span>statsmodel vs. sklearn<a class="headerlink" href="#statsmodel-vs-sklearn" title="Permalink to this headline">#</a></h2>
<p>I run linear regressions (OLS) two ways below. I first use <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code>. This method and output looks like “traditional” regression analysis. You specify the y-variable, the x-variables you want to include, and get a nice table with output. Very much like what you’d get using Excel and the Data Analysis Toolkit.</p>
<p>The Hull textbook uses <code class="docutils literal notranslate"><span class="pre">sklearn</span></code>. This library is focused on <strong>machine learning</strong>. The set-up is going to look a lot different. This library will help you process your data, define features, and split it up into training and test data sets. Crucially, you’ll want to end up with <strong>X</strong> and <strong>y</strong> data frames that contain <strong>features</strong> and <strong>target</strong> values that you are trying to predict, respectively.</p>
<p>Two libraries, two ways to do regression. The Hull textbook uses <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> for the <strong>ridge, LASSO, and Elastic Net</strong> models.</p>
<p>You can read more about <a class="reference external" href="https://www.statsmodels.org/dev/regression.html">the statsmodels library on their help page</a>. Here’s <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html">linear regression from the sklearn libary</a>.</p>
</section>
<section id="workflow">
<h2><span class="section-number">12.3. </span>Workflow<a class="headerlink" href="#workflow" title="Permalink to this headline">#</a></h2>
<p>A lot of the data that we’re using in these examples is already clean. There are no missing values. The columns are what we want. The features and target have been standardized.</p>
<p>In reality, things don’t work that way. You need to <strong>carefully look at your data</strong>. The regressions, the models, the machine learning is “easy”, in a sense. The tools are built-in. You don’t exactly have to do a lot of linear algebra or optimization math by hand.</p>
<p>The real work is in the details. It is your judgement and domain expertise that tells you what features you should be looking for. You need to know how your data were collected. You need the background knowledge to understand your task.</p>
<p>You’ll spend a lot of time cleaning and organizing your data in our labs and exams. In general, your workflow is going to be something like this:</p>
<ul class="simple">
<li><p><strong>Import your data</strong>. Is it a CSV or Excel file? Is it what you want? Do you need to remove any extra columns? Skip rows?</p></li>
<li><p><strong>Keep the columns that your want.</strong> What’s your target, the thing you predicting? What features do you want to include?</p></li>
<li><p><strong>Feature engineering time!</strong> Any missing values? Anything to clean? Check all of you columns. Are some text that you want numeric? Everything we’re doing here, for now, is with numeric values. Do you need to create new variables from existing ones? Combine variables into a new feature? Do you want to create dummy variables for categorical features? <strong>You’ll spend most of your time doing this.</strong></p></li>
<li><p><strong>Split your data.</strong> Once your data is clean, you can split it into <strong>training</strong> and <strong>testing</strong> samples. Hull will also sometimes split things into an intermediate <strong>validation</strong> sample. Most of your data will be in the training sample. You’ll hold out maybe 20% of your data for testing.</p></li>
<li><p><strong>Standardize your data.</strong> Turn the actual numerical values for your targets and features into z-scores. A general recommendation is to standardize your training data and then use the means and standard deviations from your training data to standardize your testing data. More on this below.</p></li>
</ul>
<p>Hull often provides data that is already cleaned, split, and standardized. But, we need to know how to do this.</p>
<ul class="simple">
<li><p><strong>Train your model.</strong> Use your training data to fit your model. Use a validation data set or something like cross-fold validation to look for optimal hyperparameter(s), if needed.</p></li>
<li><p><strong>Predict your target.</strong> Take the fitted model that uses the optimal hyperparamter(s) and use it to fit your testing data. Get predicted values and compare to the actual y targets in your testing sample.</p></li>
<li><p><strong>Evaluate your model.</strong> How good a job did it do? See Hull for more on the numbers you might use to evaluate a model. I like to make a scatter plot of actual vs. predicted values.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">scipy.stats</span> <span class="k">as</span> <span class="nn">st</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="nn">sm</span>
<span class="kn">import</span> <span class="nn">statsmodels.formula.api</span> <span class="k">as</span> <span class="nn">smf</span>

<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span> <span class="k">as</span> <span class="n">mse</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">r2_score</span> <span class="k">as</span> <span class="n">r2</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="c1"># Importing Ridge</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span>

<span class="c1"># Import Lasso</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Lasso</span>

<span class="c1"># Import Elastic Net</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">ElasticNet</span>

<span class="c1"># Import train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># Import StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="c1"># Import GridSearchCV</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>

<span class="c1">#Need for AIC/BIC cross validation example</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LassoLarsIC</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LassoCV</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">RepeatedKFold</span>


<span class="c1"># Include this to have plots show up in your Jupyter notebook.</span>
<span class="o">%</span><span class="k">matplotlib</span> inline 

<span class="n">pd</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">display</span><span class="o">.</span><span class="n">max_columns</span> <span class="o">=</span> <span class="kc">None</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="example-using-zillow-pricing-errors">
<h2><span class="section-number">12.4. </span>Example using Zillow pricing errors<a class="headerlink" href="#example-using-zillow-pricing-errors" title="Permalink to this headline">#</a></h2>
<p>Let’s start with a typical regression and output. So, no prediction. Let’s run a regression like we might using the Data Analysis Toolkit in Excel. We’ll look at the ouput and interpret the coefficients and statistical significance.</p>
<p>I’ll be using our Zillow pricing error data in this example. The <code class="docutils literal notranslate"><span class="pre">statsmodel</span></code> library will let us run the regressions and give us nicely formatted output.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">housing</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;https://raw.githubusercontent.com/aaiken1/fin-data-analysis-python/main/data/properties_2016_sample10_1.csv&#39;</span><span class="p">)</span>
<span class="n">pricing</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;https://raw.githubusercontent.com/aaiken1/fin-data-analysis-python/main/data/train_2016_v2.csv&#39;</span><span class="p">)</span>

<span class="n">zillow_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">housing</span><span class="p">,</span> <span class="n">pricing</span><span class="p">,</span> <span class="n">how</span><span class="o">=</span><span class="s1">&#39;inner&#39;</span><span class="p">,</span> <span class="n">on</span><span class="o">=</span><span class="s1">&#39;parcelid&#39;</span><span class="p">)</span>
<span class="n">zillow_data</span><span class="p">[</span><span class="s1">&#39;transactiondate&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">zillow_data</span><span class="p">[</span><span class="s1">&#39;transactiondate&#39;</span><span class="p">],</span> <span class="nb">format</span><span class="o">=</span><span class="s1">&#39;%Y-%m-</span><span class="si">%d</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/var/folders/kx/y8vj3n6n5kq_d74vj24jsnh40000gn/T/ipykernel_37159/4278326240.py:1: DtypeWarning: Columns (49) have mixed types. Specify dtype option on import or set low_memory=False.
  housing = pd.read_csv(&#39;https://raw.githubusercontent.com/aaiken1/fin-data-analysis-python/main/data/properties_2016_sample10_1.csv&#39;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">zillow_data</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>parcelid</th>
      <th>airconditioningtypeid</th>
      <th>architecturalstyletypeid</th>
      <th>basementsqft</th>
      <th>bathroomcnt</th>
      <th>bedroomcnt</th>
      <th>buildingclasstypeid</th>
      <th>buildingqualitytypeid</th>
      <th>calculatedbathnbr</th>
      <th>decktypeid</th>
      <th>finishedfloor1squarefeet</th>
      <th>calculatedfinishedsquarefeet</th>
      <th>finishedsquarefeet12</th>
      <th>finishedsquarefeet13</th>
      <th>finishedsquarefeet15</th>
      <th>finishedsquarefeet50</th>
      <th>finishedsquarefeet6</th>
      <th>fips</th>
      <th>fireplacecnt</th>
      <th>fullbathcnt</th>
      <th>garagecarcnt</th>
      <th>garagetotalsqft</th>
      <th>heatingorsystemtypeid</th>
      <th>latitude</th>
      <th>longitude</th>
      <th>lotsizesquarefeet</th>
      <th>poolcnt</th>
      <th>poolsizesum</th>
      <th>pooltypeid7</th>
      <th>propertylandusetypeid</th>
      <th>rawcensustractandblock</th>
      <th>regionidcity</th>
      <th>regionidcounty</th>
      <th>regionidneighborhood</th>
      <th>regionidzip</th>
      <th>roomcnt</th>
      <th>storytypeid</th>
      <th>threequarterbathnbr</th>
      <th>typeconstructiontypeid</th>
      <th>unitcnt</th>
      <th>yardbuildingsqft17</th>
      <th>yardbuildingsqft26</th>
      <th>yearbuilt</th>
      <th>numberofstories</th>
      <th>structuretaxvaluedollarcnt</th>
      <th>taxvaluedollarcnt</th>
      <th>assessmentyear</th>
      <th>landtaxvaluedollarcnt</th>
      <th>taxamount</th>
      <th>taxdelinquencyyear</th>
      <th>censustractandblock</th>
      <th>logerror</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>9.071000e+03</td>
      <td>2871.000000</td>
      <td>0.0</td>
      <td>5.00000</td>
      <td>9071.000000</td>
      <td>9071.000000</td>
      <td>3.0</td>
      <td>5694.000000</td>
      <td>8948.000000</td>
      <td>64.0</td>
      <td>695.000000</td>
      <td>9001.000000</td>
      <td>8612.000000</td>
      <td>3.000000</td>
      <td>337.000000</td>
      <td>695.000000</td>
      <td>49.000000</td>
      <td>9071.000000</td>
      <td>993.000000</td>
      <td>8948.000000</td>
      <td>3076.000000</td>
      <td>3076.000000</td>
      <td>5574.000000</td>
      <td>9.071000e+03</td>
      <td>9.071000e+03</td>
      <td>8.020000e+03</td>
      <td>1810.0</td>
      <td>99.000000</td>
      <td>1685.0</td>
      <td>9071.000000</td>
      <td>9.071000e+03</td>
      <td>8912.000000</td>
      <td>9071.000000</td>
      <td>3601.000000</td>
      <td>9070.000000</td>
      <td>9071.000000</td>
      <td>5.0</td>
      <td>1208.000000</td>
      <td>0.0</td>
      <td>5794.000000</td>
      <td>280.000000</td>
      <td>7.000000</td>
      <td>8991.000000</td>
      <td>2138.000000</td>
      <td>9.022000e+03</td>
      <td>9.071000e+03</td>
      <td>9071.0</td>
      <td>9.071000e+03</td>
      <td>9071.000000</td>
      <td>168.000000</td>
      <td>9.009000e+03</td>
      <td>9071.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>1.298764e+07</td>
      <td>1.838036</td>
      <td>NaN</td>
      <td>516.00000</td>
      <td>2.266233</td>
      <td>3.013670</td>
      <td>4.0</td>
      <td>5.572708</td>
      <td>2.296826</td>
      <td>66.0</td>
      <td>1348.981295</td>
      <td>1767.239307</td>
      <td>1740.108918</td>
      <td>1408.000000</td>
      <td>2393.350148</td>
      <td>1368.942446</td>
      <td>2251.428571</td>
      <td>6049.128982</td>
      <td>1.197382</td>
      <td>2.228990</td>
      <td>1.800715</td>
      <td>342.415475</td>
      <td>3.909760</td>
      <td>3.400230e+07</td>
      <td>-1.181977e+08</td>
      <td>3.150909e+04</td>
      <td>1.0</td>
      <td>520.424242</td>
      <td>1.0</td>
      <td>261.835520</td>
      <td>6.049436e+07</td>
      <td>33944.006845</td>
      <td>2511.879727</td>
      <td>193520.398223</td>
      <td>96547.689195</td>
      <td>1.531364</td>
      <td>7.0</td>
      <td>1.004967</td>
      <td>NaN</td>
      <td>1.104764</td>
      <td>290.335714</td>
      <td>496.714286</td>
      <td>1968.380047</td>
      <td>1.428438</td>
      <td>1.768673e+05</td>
      <td>4.523049e+05</td>
      <td>2015.0</td>
      <td>2.763930e+05</td>
      <td>5906.696988</td>
      <td>13.327381</td>
      <td>6.049368e+13</td>
      <td>0.010703</td>
    </tr>
    <tr>
      <th>std</th>
      <td>1.757451e+06</td>
      <td>3.001723</td>
      <td>NaN</td>
      <td>233.49197</td>
      <td>0.989863</td>
      <td>1.118468</td>
      <td>0.0</td>
      <td>1.908379</td>
      <td>0.960557</td>
      <td>0.0</td>
      <td>664.508053</td>
      <td>918.999586</td>
      <td>880.213401</td>
      <td>55.425626</td>
      <td>1434.457485</td>
      <td>709.622839</td>
      <td>1352.034747</td>
      <td>20.794593</td>
      <td>0.480794</td>
      <td>0.951007</td>
      <td>0.598328</td>
      <td>263.642761</td>
      <td>3.678727</td>
      <td>2.654493e+05</td>
      <td>3.631575e+05</td>
      <td>1.824345e+05</td>
      <td>0.0</td>
      <td>146.537109</td>
      <td>0.0</td>
      <td>5.781663</td>
      <td>2.063550e+05</td>
      <td>47178.373342</td>
      <td>810.417898</td>
      <td>169701.596819</td>
      <td>412.732130</td>
      <td>2.856603</td>
      <td>0.0</td>
      <td>0.070330</td>
      <td>NaN</td>
      <td>0.459551</td>
      <td>172.987812</td>
      <td>506.445033</td>
      <td>23.469997</td>
      <td>0.536698</td>
      <td>1.909207e+05</td>
      <td>5.229433e+05</td>
      <td>0.0</td>
      <td>3.901131e+05</td>
      <td>6388.966672</td>
      <td>1.796527</td>
      <td>2.053649e+11</td>
      <td>0.158364</td>
    </tr>
    <tr>
      <th>min</th>
      <td>1.071186e+07</td>
      <td>1.000000</td>
      <td>NaN</td>
      <td>162.00000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>4.0</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>66.0</td>
      <td>49.000000</td>
      <td>214.000000</td>
      <td>214.000000</td>
      <td>1344.000000</td>
      <td>716.000000</td>
      <td>49.000000</td>
      <td>438.000000</td>
      <td>6037.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>3.334420e+07</td>
      <td>-1.194143e+08</td>
      <td>4.350000e+02</td>
      <td>1.0</td>
      <td>207.000000</td>
      <td>1.0</td>
      <td>31.000000</td>
      <td>6.037101e+07</td>
      <td>3491.000000</td>
      <td>1286.000000</td>
      <td>6952.000000</td>
      <td>95982.000000</td>
      <td>0.000000</td>
      <td>7.0</td>
      <td>1.000000</td>
      <td>NaN</td>
      <td>1.000000</td>
      <td>41.000000</td>
      <td>37.000000</td>
      <td>1885.000000</td>
      <td>1.000000</td>
      <td>1.516000e+03</td>
      <td>7.837000e+03</td>
      <td>2015.0</td>
      <td>2.178000e+03</td>
      <td>96.740000</td>
      <td>7.000000</td>
      <td>6.037101e+13</td>
      <td>-2.365000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>1.157119e+07</td>
      <td>1.000000</td>
      <td>NaN</td>
      <td>485.00000</td>
      <td>2.000000</td>
      <td>2.000000</td>
      <td>4.0</td>
      <td>4.000000</td>
      <td>2.000000</td>
      <td>66.0</td>
      <td>938.000000</td>
      <td>1187.000000</td>
      <td>1173.000000</td>
      <td>1392.000000</td>
      <td>1668.000000</td>
      <td>938.000000</td>
      <td>1009.000000</td>
      <td>6037.000000</td>
      <td>1.000000</td>
      <td>2.000000</td>
      <td>2.000000</td>
      <td>0.000000</td>
      <td>2.000000</td>
      <td>3.380545e+07</td>
      <td>-1.184080e+08</td>
      <td>5.746500e+03</td>
      <td>1.0</td>
      <td>435.000000</td>
      <td>1.0</td>
      <td>261.000000</td>
      <td>6.037400e+07</td>
      <td>12447.000000</td>
      <td>1286.000000</td>
      <td>46736.000000</td>
      <td>96193.000000</td>
      <td>0.000000</td>
      <td>7.0</td>
      <td>1.000000</td>
      <td>NaN</td>
      <td>1.000000</td>
      <td>175.750000</td>
      <td>110.500000</td>
      <td>1953.000000</td>
      <td>1.000000</td>
      <td>8.028525e+04</td>
      <td>1.926595e+05</td>
      <td>2015.0</td>
      <td>8.060700e+04</td>
      <td>2828.645000</td>
      <td>13.000000</td>
      <td>6.037400e+13</td>
      <td>-0.025300</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>1.259048e+07</td>
      <td>1.000000</td>
      <td>NaN</td>
      <td>515.00000</td>
      <td>2.000000</td>
      <td>3.000000</td>
      <td>4.0</td>
      <td>7.000000</td>
      <td>2.000000</td>
      <td>66.0</td>
      <td>1249.000000</td>
      <td>1539.000000</td>
      <td>1513.000000</td>
      <td>1440.000000</td>
      <td>2157.000000</td>
      <td>1257.000000</td>
      <td>1835.000000</td>
      <td>6037.000000</td>
      <td>1.000000</td>
      <td>2.000000</td>
      <td>2.000000</td>
      <td>430.000000</td>
      <td>2.000000</td>
      <td>3.401408e+07</td>
      <td>-1.181670e+08</td>
      <td>7.200000e+03</td>
      <td>1.0</td>
      <td>504.000000</td>
      <td>1.0</td>
      <td>261.000000</td>
      <td>6.037621e+07</td>
      <td>25218.000000</td>
      <td>3101.000000</td>
      <td>118887.000000</td>
      <td>96401.000000</td>
      <td>0.000000</td>
      <td>7.0</td>
      <td>1.000000</td>
      <td>NaN</td>
      <td>1.000000</td>
      <td>248.500000</td>
      <td>268.000000</td>
      <td>1969.000000</td>
      <td>1.000000</td>
      <td>1.315530e+05</td>
      <td>3.416920e+05</td>
      <td>2015.0</td>
      <td>1.910000e+05</td>
      <td>4521.580000</td>
      <td>14.000000</td>
      <td>6.037621e+13</td>
      <td>0.007000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>1.423676e+07</td>
      <td>1.000000</td>
      <td>NaN</td>
      <td>616.00000</td>
      <td>3.000000</td>
      <td>4.000000</td>
      <td>4.0</td>
      <td>7.000000</td>
      <td>3.000000</td>
      <td>66.0</td>
      <td>1612.000000</td>
      <td>2090.000000</td>
      <td>2055.000000</td>
      <td>1440.000000</td>
      <td>2806.000000</td>
      <td>1617.500000</td>
      <td>3732.000000</td>
      <td>6059.000000</td>
      <td>1.000000</td>
      <td>3.000000</td>
      <td>2.000000</td>
      <td>484.000000</td>
      <td>7.000000</td>
      <td>3.417153e+07</td>
      <td>-1.179195e+08</td>
      <td>1.161675e+04</td>
      <td>1.0</td>
      <td>600.000000</td>
      <td>1.0</td>
      <td>266.000000</td>
      <td>6.059052e+07</td>
      <td>45457.000000</td>
      <td>3101.000000</td>
      <td>274815.000000</td>
      <td>96987.000000</td>
      <td>0.000000</td>
      <td>7.0</td>
      <td>1.000000</td>
      <td>NaN</td>
      <td>1.000000</td>
      <td>360.000000</td>
      <td>792.500000</td>
      <td>1986.000000</td>
      <td>2.000000</td>
      <td>2.076458e+05</td>
      <td>5.361120e+05</td>
      <td>2015.0</td>
      <td>3.428715e+05</td>
      <td>6865.565000</td>
      <td>15.000000</td>
      <td>6.059052e+13</td>
      <td>0.040200</td>
    </tr>
    <tr>
      <th>max</th>
      <td>1.730050e+07</td>
      <td>13.000000</td>
      <td>NaN</td>
      <td>802.00000</td>
      <td>12.000000</td>
      <td>12.000000</td>
      <td>4.0</td>
      <td>12.000000</td>
      <td>12.000000</td>
      <td>66.0</td>
      <td>5416.000000</td>
      <td>22741.000000</td>
      <td>10680.000000</td>
      <td>1440.000000</td>
      <td>22741.000000</td>
      <td>6906.000000</td>
      <td>5229.000000</td>
      <td>6111.000000</td>
      <td>3.000000</td>
      <td>12.000000</td>
      <td>9.000000</td>
      <td>2685.000000</td>
      <td>24.000000</td>
      <td>3.477509e+07</td>
      <td>-1.175604e+08</td>
      <td>6.971010e+06</td>
      <td>1.0</td>
      <td>1052.000000</td>
      <td>1.0</td>
      <td>275.000000</td>
      <td>6.111009e+07</td>
      <td>396556.000000</td>
      <td>3101.000000</td>
      <td>764166.000000</td>
      <td>97344.000000</td>
      <td>13.000000</td>
      <td>7.0</td>
      <td>2.000000</td>
      <td>NaN</td>
      <td>9.000000</td>
      <td>1018.000000</td>
      <td>1366.000000</td>
      <td>2015.000000</td>
      <td>3.000000</td>
      <td>4.588745e+06</td>
      <td>1.275000e+07</td>
      <td>2015.0</td>
      <td>1.200000e+07</td>
      <td>152152.220000</td>
      <td>15.000000</td>
      <td>6.111009e+13</td>
      <td>2.953000</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>I’ll print a list of the columns, just to see what our variables are. There’s a lot in this data set.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">zillow_data</span><span class="o">.</span><span class="n">columns</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Index([&#39;parcelid&#39;, &#39;airconditioningtypeid&#39;, &#39;architecturalstyletypeid&#39;,
       &#39;basementsqft&#39;, &#39;bathroomcnt&#39;, &#39;bedroomcnt&#39;, &#39;buildingclasstypeid&#39;,
       &#39;buildingqualitytypeid&#39;, &#39;calculatedbathnbr&#39;, &#39;decktypeid&#39;,
       &#39;finishedfloor1squarefeet&#39;, &#39;calculatedfinishedsquarefeet&#39;,
       &#39;finishedsquarefeet12&#39;, &#39;finishedsquarefeet13&#39;, &#39;finishedsquarefeet15&#39;,
       &#39;finishedsquarefeet50&#39;, &#39;finishedsquarefeet6&#39;, &#39;fips&#39;, &#39;fireplacecnt&#39;,
       &#39;fullbathcnt&#39;, &#39;garagecarcnt&#39;, &#39;garagetotalsqft&#39;, &#39;hashottuborspa&#39;,
       &#39;heatingorsystemtypeid&#39;, &#39;latitude&#39;, &#39;longitude&#39;, &#39;lotsizesquarefeet&#39;,
       &#39;poolcnt&#39;, &#39;poolsizesum&#39;, &#39;pooltypeid10&#39;, &#39;pooltypeid2&#39;, &#39;pooltypeid7&#39;,
       &#39;propertycountylandusecode&#39;, &#39;propertylandusetypeid&#39;,
       &#39;propertyzoningdesc&#39;, &#39;rawcensustractandblock&#39;, &#39;regionidcity&#39;,
       &#39;regionidcounty&#39;, &#39;regionidneighborhood&#39;, &#39;regionidzip&#39;, &#39;roomcnt&#39;,
       &#39;storytypeid&#39;, &#39;threequarterbathnbr&#39;, &#39;typeconstructiontypeid&#39;,
       &#39;unitcnt&#39;, &#39;yardbuildingsqft17&#39;, &#39;yardbuildingsqft26&#39;, &#39;yearbuilt&#39;,
       &#39;numberofstories&#39;, &#39;fireplaceflag&#39;, &#39;structuretaxvaluedollarcnt&#39;,
       &#39;taxvaluedollarcnt&#39;, &#39;assessmentyear&#39;, &#39;landtaxvaluedollarcnt&#39;,
       &#39;taxamount&#39;, &#39;taxdelinquencyflag&#39;, &#39;taxdelinquencyyear&#39;,
       &#39;censustractandblock&#39;, &#39;logerror&#39;, &#39;transactiondate&#39;],
      dtype=&#39;object&#39;)
</pre></div>
</div>
</div>
</div>
<p>Let’s run a really simple regression. Can we explain pricing errors using the size of the house? I’ll take the natural log of <code class="docutils literal notranslate"><span class="pre">calculatedfinishedsquarefeet</span></code> and use that as my independent (<strong>X</strong>) variable. My dependent (<strong>Y</strong>) variable will be <code class="docutils literal notranslate"><span class="pre">logerror</span></code>. I’m taking the natural log of the square footage, in order to have what’s called a “log-log” model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">zillow_data</span><span class="p">[</span><span class="s1">&#39;ln_calculatedfinishedsquarefeet&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">zillow_data</span><span class="p">[</span><span class="s1">&#39;calculatedfinishedsquarefeet&#39;</span><span class="p">])</span>

<span class="n">results</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s2">&quot;logerror ~ ln_calculatedfinishedsquarefeet&quot;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">zillow_data</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                            OLS Regression Results                            
==============================================================================
Dep. Variable:               logerror   R-squared:                       0.001
Model:                            OLS   Adj. R-squared:                  0.001
Method:                 Least Squares   F-statistic:                     13.30
Date:                Fri, 19 Apr 2024   Prob (F-statistic):           0.000267
Time:                        16:29:21   Log-Likelihood:                 3831.8
No. Observations:                9001   AIC:                            -7660.
Df Residuals:                    8999   BIC:                            -7645.
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
===================================================================================================
                                      coef    std err          t      P&gt;|t|      [0.025      0.975]
---------------------------------------------------------------------------------------------------
Intercept                          -0.0911      0.028     -3.244      0.001      -0.146      -0.036
ln_calculatedfinishedsquarefeet     0.0139      0.004      3.647      0.000       0.006       0.021
==============================================================================
Omnibus:                     4055.877   Durbin-Watson:                   2.005
Prob(Omnibus):                  0.000   Jarque-Bera (JB):          2595715.665
Skew:                           0.737   Prob(JB):                         0.00
Kurtosis:                      86.180   Cond. No.                         127.
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre></div>
</div>
</div>
</div>
<p>One y variable and one X variable. That’s the full summary of the regression. This is a “log-log” model, so we can say that a 1% change in square footage leads to a 1.39% increase in pricing error. The coefficient is positive and statistically significant at conventional levels (e.g. 1%).</p>
<p>We can pull out just a piece of this full result if we like.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">results</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span><span class="o">.</span><span class="n">tables</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table class="simpletable">
<tr>
                 <td></td>                    <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>                       <td>   -0.0911</td> <td>    0.028</td> <td>   -3.244</td> <td> 0.001</td> <td>   -0.146</td> <td>   -0.036</td>
</tr>
<tr>
  <th>ln_calculatedfinishedsquarefeet</th> <td>    0.0139</td> <td>    0.004</td> <td>    3.647</td> <td> 0.000</td> <td>    0.006</td> <td>    0.021</td>
</tr>
</table></div></div>
</div>
<p>We can, of course, include multiple <strong>X</strong> variables in a regression. I’ll add bathroom and bedroom counts to the regression model.</p>
<p>Pay attention to the <strong>syntax</strong> here. I am giving <code class="docutils literal notranslate"><span class="pre">smf.ols</span></code> the name of my data frame. I can then <strong>write the formula</strong> for my regression using the names of my columns (variabes or features).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">results</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s2">&quot;logerror ~ ln_calculatedfinishedsquarefeet + bathroomcnt + bedroomcnt&quot;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">zillow_data</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                            OLS Regression Results                            
==============================================================================
Dep. Variable:               logerror   R-squared:                       0.002
Model:                            OLS   Adj. R-squared:                  0.002
Method:                 Least Squares   F-statistic:                     6.718
Date:                Fri, 19 Apr 2024   Prob (F-statistic):           0.000159
Time:                        16:29:21   Log-Likelihood:                 3835.2
No. Observations:                9001   AIC:                            -7662.
Df Residuals:                    8997   BIC:                            -7634.
Df Model:                           3                                         
Covariance Type:            nonrobust                                         
===================================================================================================
                                      coef    std err          t      P&gt;|t|      [0.025      0.975]
---------------------------------------------------------------------------------------------------
Intercept                          -0.0140      0.041     -0.339      0.735      -0.095       0.067
ln_calculatedfinishedsquarefeet     0.0006      0.006      0.095      0.925      -0.012       0.013
bathroomcnt                         0.0040      0.003      1.493      0.135      -0.001       0.009
bedroomcnt                          0.0038      0.002      1.740      0.082      -0.000       0.008
==============================================================================
Omnibus:                     4050.508   Durbin-Watson:                   2.005
Prob(Omnibus):                  0.000   Jarque-Bera (JB):          2598102.584
Skew:                           0.733   Prob(JB):                         0.00
Kurtosis:                      86.219   Cond. No.                         211.
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre></div>
</div>
</div>
</div>
<p>Hey, all of my significance went away! Welcome to the world of <a class="reference external" href="https://en.wikipedia.org/wiki/Multicollinearity">multicollinearity</a>. All of these variables are very correlated, so the coefficient estimates become difficult to interpret.</p>
<p>We’re going to use <strong>machine learning</strong> below to help with this issue.</p>
<p>Watch what happens when I just run the model with the bedroom count. The <span class="math notranslate nohighlight">\(t\)</span>-statistic is quite large again.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">results</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s2">&quot;logerror ~ bedroomcnt&quot;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">zillow_data</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                            OLS Regression Results                            
==============================================================================
Dep. Variable:               logerror   R-squared:                       0.002
Model:                            OLS   Adj. R-squared:                  0.002
Method:                 Least Squares   F-statistic:                     21.69
Date:                Fri, 19 Apr 2024   Prob (F-statistic):           3.24e-06
Time:                        16:29:21   Log-Likelihood:                 3856.7
No. Observations:                9071   AIC:                            -7709.
Df Residuals:                    9069   BIC:                            -7695.
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept     -0.0101      0.005     -2.125      0.034      -0.019      -0.001
bedroomcnt     0.0069      0.001      4.658      0.000       0.004       0.010
==============================================================================
Omnibus:                     4021.076   Durbin-Watson:                   2.006
Prob(Omnibus):                  0.000   Jarque-Bera (JB):          2560800.149
Skew:                           0.697   Prob(JB):                         0.00
Kurtosis:                      85.301   Cond. No.                         10.0
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre></div>
</div>
</div>
</div>
</section>
<section id="indicators-and-categorical-variables">
<h2><span class="section-number">12.5. </span>Indicators and categorical variables<a class="headerlink" href="#indicators-and-categorical-variables" title="Permalink to this headline">#</a></h2>
<p>The variables used above are measured numerically. Some are <strong>continuous</strong>, like square footage, while others are <strong>counts</strong>, like the number of bedrooms. Sometimes, though, we want to include an <strong>indicator</strong> for something? For example, does this house have a pool or not?</p>
<p>There is a variable in the data called <code class="docutils literal notranslate"><span class="pre">poolcnt</span></code>. It seems to be either missing (NaN) or set equal to 1. I believe that a value of 1 means that the house has a pool and that <code class="docutils literal notranslate"><span class="pre">NaN</span></code> means that it does not. This is bit of a tricky assumption, because <code class="docutils literal notranslate"><span class="pre">NaN</span></code> could mean no pool or that we don’t know either way. But, I’ll make that assumption for illustrative purposes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">zillow_data</span><span class="p">[</span><span class="s1">&#39;poolcnt&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>count    1810.0
mean        1.0
std         0.0
min         1.0
25%         1.0
50%         1.0
75%         1.0
max         1.0
Name: poolcnt, dtype: float64
</pre></div>
</div>
</div>
</div>
<p>I am going to create a new variable, <code class="docutils literal notranslate"><span class="pre">pool_d</span></code>, that is set equal to 1 if <code class="docutils literal notranslate"><span class="pre">poolcnt</span> <span class="pre">&gt;=</span> <span class="pre">1</span></code> and 0 otherwise. This type of 1/0 categorical variable is sometimes called an <strong>indicator</strong>, or <strong>dummy</strong> variable.</p>
<p>This is an example of making the indicator variable <strong>by hand</strong>. I’ll use <code class="docutils literal notranslate"><span class="pre">pd.get_dummies</span></code> below in a second.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">zillow_data</span><span class="p">[</span><span class="s1">&#39;pool_d&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">zillow_data</span><span class="o">.</span><span class="n">poolcnt</span><span class="o">.</span><span class="n">isnull</span><span class="p">(),</span> <span class="mi">0</span><span class="p">,</span> <span class="n">zillow_data</span><span class="o">.</span><span class="n">poolcnt</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">zillow_data</span><span class="p">[</span><span class="s1">&#39;pool_d&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>count    9071.000000
mean        0.199537
std         0.399674
min         0.000000
25%         0.000000
50%         0.000000
75%         0.000000
max         1.000000
Name: pool_d, dtype: float64
</pre></div>
</div>
</div>
</div>
<p>I can then use this 1/0 variable in my regression.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">results</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s2">&quot;logerror ~ ln_calculatedfinishedsquarefeet + pool_d&quot;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">zillow_data</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                            OLS Regression Results                            
==============================================================================
Dep. Variable:               logerror   R-squared:                       0.001
Model:                            OLS   Adj. R-squared:                  0.001
Method:                 Least Squares   F-statistic:                     6.684
Date:                Fri, 19 Apr 2024   Prob (F-statistic):            0.00126
Time:                        16:29:21   Log-Likelihood:                 3831.8
No. Observations:                9001   AIC:                            -7658.
Df Residuals:                    8998   BIC:                            -7636.
Df Model:                           2                                         
Covariance Type:            nonrobust                                         
===================================================================================================
                                      coef    std err          t      P&gt;|t|      [0.025      0.975]
---------------------------------------------------------------------------------------------------
Intercept                          -0.0898      0.029     -3.150      0.002      -0.146      -0.034
ln_calculatedfinishedsquarefeet     0.0137      0.004      3.519      0.000       0.006       0.021
pool_d                              0.0011      0.004      0.262      0.794      -0.007       0.009
==============================================================================
Omnibus:                     4055.061   Durbin-Watson:                   2.006
Prob(Omnibus):                  0.000   Jarque-Bera (JB):          2593138.691
Skew:                           0.737   Prob(JB):                         0.00
Kurtosis:                      86.139   Cond. No.                         129.
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre></div>
</div>
</div>
</div>
<p>Pools don’t seem to influence pricing errors.</p>
<p>We can also create more general <strong>categorical</strong> variables. For example, instead of treating bedrooms like a count, we can create new categories for each number of bedrooms. This type of model is helpful when dealing states or regions. For example, you could turn a zip code into a categorical variable. This would allow zip codes, or a location, to explain the pricing errors.</p>
<p>When using <code class="docutils literal notranslate"><span class="pre">statsmodel</span></code> to run your regressions, you can turn something into a categorical variable by using <code class="docutils literal notranslate"><span class="pre">C()</span></code> in the regression formula.</p>
<p>I’ll try the count of bedrooms first.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">results</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s2">&quot;logerror ~ ln_calculatedfinishedsquarefeet + C(bedroomcnt)&quot;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">zillow_data</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                            OLS Regression Results                            
==============================================================================
Dep. Variable:               logerror   R-squared:                       0.004
Model:                            OLS   Adj. R-squared:                  0.003
Method:                 Least Squares   F-statistic:                     3.118
Date:                Fri, 19 Apr 2024   Prob (F-statistic):           0.000196
Time:                        16:29:21   Log-Likelihood:                 3843.8
No. Observations:                9001   AIC:                            -7662.
Df Residuals:                    8988   BIC:                            -7569.
Df Model:                          12                                         
Covariance Type:            nonrobust                                         
===================================================================================================
                                      coef    std err          t      P&gt;|t|      [0.025      0.975]
---------------------------------------------------------------------------------------------------
Intercept                          -0.0680      0.045     -1.523      0.128      -0.155       0.020
C(bedroomcnt)[T.1.0]                0.0370      0.021      1.756      0.079      -0.004       0.078
C(bedroomcnt)[T.2.0]                0.0279      0.020      1.428      0.153      -0.010       0.066
C(bedroomcnt)[T.3.0]                0.0319      0.019      1.648      0.099      -0.006       0.070
C(bedroomcnt)[T.4.0]                0.0357      0.020      1.825      0.068      -0.003       0.074
C(bedroomcnt)[T.5.0]                0.0580      0.021      2.799      0.005       0.017       0.099
C(bedroomcnt)[T.6.0]                0.0491      0.024      2.007      0.045       0.001       0.097
C(bedroomcnt)[T.7.0]                0.0903      0.040      2.266      0.023       0.012       0.168
C(bedroomcnt)[T.8.0]               -0.0165      0.043     -0.383      0.702      -0.101       0.068
C(bedroomcnt)[T.9.0]               -0.1190      0.081     -1.461      0.144      -0.279       0.041
C(bedroomcnt)[T.10.0]               0.0312      0.159      0.196      0.845      -0.281       0.343
C(bedroomcnt)[T.12.0]               0.0399      0.114      0.351      0.725      -0.183       0.262
ln_calculatedfinishedsquarefeet     0.0062      0.005      1.134      0.257      -0.005       0.017
==============================================================================
Omnibus:                     4046.896   Durbin-Watson:                   2.006
Prob(Omnibus):                  0.000   Jarque-Bera (JB):          2594171.124
Skew:                           0.731   Prob(JB):                         0.00
Kurtosis:                      86.156   Cond. No.                         716.
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre></div>
</div>
</div>
</div>
<p>And here are zip codes as a categorical variable. This is saying: Is the house in this zip code or no? If it is, the indicator for that zip code gets a 1, and a 0 otherwise. If we didn’t do this, then the zip code would get treated like a numerical variable in the regression, like square footage, which makes no sense!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">results</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s2">&quot;logerror ~ ln_calculatedfinishedsquarefeet + C(regionidzip)&quot;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">zillow_data</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                            OLS Regression Results                            
==============================================================================
Dep. Variable:               logerror   R-squared:                       0.054
Model:                            OLS   Adj. R-squared:                  0.012
Method:                 Least Squares   F-statistic:                     1.300
Date:                Fri, 19 Apr 2024   Prob (F-statistic):           0.000104
Time:                        16:29:21   Log-Likelihood:                 4075.3
No. Observations:                9001   AIC:                            -7391.
Df Residuals:                    8621   BIC:                            -4691.
Df Model:                         379                                         
Covariance Type:            nonrobust                                         
===================================================================================================
                                      coef    std err          t      P&gt;|t|      [0.025      0.975]
---------------------------------------------------------------------------------------------------
Intercept                          -0.1624      0.050     -3.261      0.001      -0.260      -0.065
C(regionidzip)[T.95983.0]           0.0855      0.053      1.621      0.105      -0.018       0.189
C(regionidzip)[T.95984.0]          -0.0735      0.050     -1.481      0.139      -0.171       0.024
C(regionidzip)[T.95985.0]           0.0679      0.051      1.326      0.185      -0.032       0.168
C(regionidzip)[T.95986.0]           0.0356      0.060      0.593      0.553      -0.082       0.153
C(regionidzip)[T.95987.0]           0.1185      0.066      1.807      0.071      -0.010       0.247
C(regionidzip)[T.95988.0]           0.1128      0.088      1.283      0.199      -0.060       0.285
C(regionidzip)[T.95989.0]           0.0356      0.058      0.618      0.537      -0.077       0.148
C(regionidzip)[T.95991.0]           0.0894      0.162      0.551      0.581      -0.228       0.407
C(regionidzip)[T.95992.0]          -0.0856      0.052     -1.641      0.101      -0.188       0.017
C(regionidzip)[T.95993.0]           0.0710      0.099      0.718      0.473      -0.123       0.265
C(regionidzip)[T.95994.0]           0.1135      0.088      1.291      0.197      -0.059       0.286
C(regionidzip)[T.95996.0]           0.1052      0.071      1.476      0.140      -0.034       0.245
C(regionidzip)[T.95997.0]           0.0820      0.049      1.674      0.094      -0.014       0.178
C(regionidzip)[T.95998.0]           0.0517      0.088      0.589      0.556      -0.121       0.224
C(regionidzip)[T.95999.0]           0.1369      0.057      2.423      0.015       0.026       0.248
C(regionidzip)[T.96000.0]           0.1968      0.050      3.937      0.000       0.099       0.295
C(regionidzip)[T.96001.0]           0.1037      0.063      1.636      0.102      -0.021       0.228
C(regionidzip)[T.96003.0]           0.0724      0.060      1.205      0.228      -0.045       0.190
C(regionidzip)[T.96004.0]           0.0467      0.088      0.531      0.596      -0.126       0.219
C(regionidzip)[T.96005.0]           0.1988      0.048      4.106      0.000       0.104       0.294
C(regionidzip)[T.96006.0]           0.0866      0.049      1.770      0.077      -0.009       0.183
C(regionidzip)[T.96007.0]           0.0738      0.051      1.454      0.146      -0.026       0.173
C(regionidzip)[T.96008.0]           0.0746      0.052      1.430      0.153      -0.028       0.177
C(regionidzip)[T.96009.0]           0.0495      0.088      0.564      0.573      -0.123       0.222
C(regionidzip)[T.96010.0]           0.6989      0.162      4.312      0.000       0.381       1.017
C(regionidzip)[T.96012.0]           0.0608      0.059      1.036      0.300      -0.054       0.176
C(regionidzip)[T.96013.0]           0.1014      0.053      1.900      0.057      -0.003       0.206
C(regionidzip)[T.96014.0]           0.0780      0.063      1.231      0.218      -0.046       0.202
C(regionidzip)[T.96015.0]           0.0950      0.051      1.857      0.063      -0.005       0.195
C(regionidzip)[T.96016.0]           0.0383      0.053      0.726      0.468      -0.065       0.142
C(regionidzip)[T.96017.0]           0.0911      0.066      1.390      0.164      -0.037       0.220
C(regionidzip)[T.96018.0]           0.0910      0.054      1.684      0.092      -0.015       0.197
C(regionidzip)[T.96019.0]           0.0107      0.068      0.156      0.876      -0.123       0.144
C(regionidzip)[T.96020.0]          -0.0248      0.055     -0.452      0.651      -0.132       0.083
C(regionidzip)[T.96021.0]           0.0026      0.118      0.022      0.982      -0.229       0.234
C(regionidzip)[T.96022.0]           0.0655      0.054      1.212      0.225      -0.040       0.171
C(regionidzip)[T.96023.0]           0.0959      0.047      2.021      0.043       0.003       0.189
C(regionidzip)[T.96024.0]           0.0828      0.048      1.736      0.083      -0.011       0.176
C(regionidzip)[T.96025.0]          -0.0347      0.049     -0.705      0.481      -0.131       0.062
C(regionidzip)[T.96026.0]           0.0124      0.050      0.249      0.804      -0.086       0.110
C(regionidzip)[T.96027.0]           0.1040      0.047      2.201      0.028       0.011       0.197
C(regionidzip)[T.96028.0]           0.0745      0.050      1.492      0.136      -0.023       0.172
C(regionidzip)[T.96029.0]           0.0818      0.051      1.598      0.110      -0.019       0.182
C(regionidzip)[T.96030.0]           0.0535      0.046      1.153      0.249      -0.037       0.144
C(regionidzip)[T.96037.0]           0.2007      0.099      2.028      0.043       0.007       0.395
C(regionidzip)[T.96038.0]           0.0674      0.099      0.681      0.496      -0.127       0.261
C(regionidzip)[T.96040.0]           0.1530      0.058      2.660      0.008       0.040       0.266
C(regionidzip)[T.96042.0]           0.0235      0.118      0.199      0.842      -0.208       0.255
C(regionidzip)[T.96043.0]           0.2073      0.055      3.784      0.000       0.100       0.315
C(regionidzip)[T.96044.0]           0.0728      0.057      1.289      0.197      -0.038       0.184
C(regionidzip)[T.96045.0]           0.0826      0.053      1.566      0.117      -0.021       0.186
C(regionidzip)[T.96046.0]           0.0574      0.048      1.186      0.236      -0.037       0.152
C(regionidzip)[T.96047.0]           0.0564      0.047      1.200      0.230      -0.036       0.149
C(regionidzip)[T.96048.0]           0.0823      0.062      1.337      0.181      -0.038       0.203
C(regionidzip)[T.96049.0]           0.0682      0.050      1.355      0.175      -0.030       0.167
C(regionidzip)[T.96050.0]           0.0114      0.046      0.249      0.803      -0.078       0.101
C(regionidzip)[T.96058.0]          -0.1212      0.062     -1.966      0.049      -0.242      -0.000
C(regionidzip)[T.96072.0]           0.0750      0.063      1.183      0.237      -0.049       0.199
C(regionidzip)[T.96083.0]          -0.0567      0.062     -0.920      0.357      -0.177       0.064
C(regionidzip)[T.96086.0]           0.0744      0.050      1.486      0.137      -0.024       0.172
C(regionidzip)[T.96087.0]           0.0877      0.081      1.088      0.277      -0.070       0.246
C(regionidzip)[T.96088.0]           0.0001      0.088      0.002      0.999      -0.172       0.172
C(regionidzip)[T.96090.0]           0.0834      0.051      1.643      0.100      -0.016       0.183
C(regionidzip)[T.96091.0]           0.0911      0.057      1.612      0.107      -0.020       0.202
C(regionidzip)[T.96092.0]           0.0375      0.058      0.652      0.514      -0.075       0.150
C(regionidzip)[T.96095.0]           0.0977      0.048      2.030      0.042       0.003       0.192
C(regionidzip)[T.96097.0]           0.0627      0.066      0.957      0.338      -0.066       0.191
C(regionidzip)[T.96100.0]           0.0843      0.062      1.369      0.171      -0.036       0.205
C(regionidzip)[T.96101.0]           0.0751      0.058      1.305      0.192      -0.038       0.188
C(regionidzip)[T.96102.0]           0.1371      0.056      2.466      0.014       0.028       0.246
C(regionidzip)[T.96103.0]           0.1050      0.058      1.825      0.068      -0.008       0.218
C(regionidzip)[T.96104.0]           0.1278      0.059      2.176      0.030       0.013       0.243
C(regionidzip)[T.96106.0]           0.0884      0.071      1.241      0.215      -0.051       0.228
C(regionidzip)[T.96107.0]           0.0371      0.048      0.778      0.436      -0.056       0.131
C(regionidzip)[T.96109.0]           0.0408      0.052      0.789      0.430      -0.061       0.142
C(regionidzip)[T.96110.0]           0.1071      0.062      1.739      0.082      -0.014       0.228
C(regionidzip)[T.96111.0]           0.1558      0.059      2.654      0.008       0.041       0.271
C(regionidzip)[T.96113.0]           0.0793      0.056      1.427      0.154      -0.030       0.188
C(regionidzip)[T.96116.0]           0.1362      0.049      2.761      0.006       0.039       0.233
C(regionidzip)[T.96117.0]           0.0503      0.049      1.033      0.302      -0.045       0.146
C(regionidzip)[T.96119.0]           0.0233      0.118      0.198      0.843      -0.208       0.254
C(regionidzip)[T.96120.0]           0.1071      0.050      2.140      0.032       0.009       0.205
C(regionidzip)[T.96121.0]           0.1159      0.048      2.402      0.016       0.021       0.210
C(regionidzip)[T.96122.0]           0.1085      0.047      2.322      0.020       0.017       0.200
C(regionidzip)[T.96123.0]           0.1051      0.045      2.332      0.020       0.017       0.193
C(regionidzip)[T.96124.0]           0.0644      0.049      1.323      0.186      -0.031       0.160
C(regionidzip)[T.96125.0]           0.1242      0.050      2.505      0.012       0.027       0.221
C(regionidzip)[T.96126.0]           0.1040      0.066      1.587      0.113      -0.024       0.232
C(regionidzip)[T.96127.0]           0.0621      0.053      1.164      0.245      -0.042       0.167
C(regionidzip)[T.96128.0]           0.0877      0.049      1.779      0.075      -0.009       0.184
C(regionidzip)[T.96129.0]           0.0729      0.051      1.425      0.154      -0.027       0.173
C(regionidzip)[T.96133.0]           0.0630      0.081      0.782      0.434      -0.095       0.221
C(regionidzip)[T.96134.0]           0.0643      0.062      1.044      0.296      -0.056       0.185
C(regionidzip)[T.96135.0]           0.0370      0.081      0.459      0.646      -0.121       0.195
C(regionidzip)[T.96136.0]           0.1674      0.075      2.225      0.026       0.020       0.315
C(regionidzip)[T.96137.0]           0.0436      0.063      0.688      0.492      -0.081       0.168
C(regionidzip)[T.96148.0]           0.0509      0.162      0.314      0.753      -0.267       0.369
C(regionidzip)[T.96149.0]          -0.0477      0.075     -0.633      0.527      -0.195       0.100
C(regionidzip)[T.96150.0]           0.1066      0.053      2.022      0.043       0.003       0.210
C(regionidzip)[T.96151.0]           0.0819      0.062      1.330      0.183      -0.039       0.203
C(regionidzip)[T.96152.0]           0.0879      0.053      1.648      0.099      -0.017       0.192
C(regionidzip)[T.96159.0]           0.0828      0.052      1.603      0.109      -0.018       0.184
C(regionidzip)[T.96160.0]           0.0337      0.062      0.548      0.584      -0.087       0.154
C(regionidzip)[T.96161.0]           0.0840      0.048      1.746      0.081      -0.010       0.178
C(regionidzip)[T.96162.0]           0.0054      0.048      0.112      0.911      -0.089       0.100
C(regionidzip)[T.96163.0]           0.0960      0.048      1.995      0.046       0.002       0.190
C(regionidzip)[T.96169.0]           0.0683      0.050      1.367      0.172      -0.030       0.166
C(regionidzip)[T.96170.0]           0.0916      0.060      1.525      0.127      -0.026       0.209
C(regionidzip)[T.96171.0]           0.0789      0.053      1.478      0.139      -0.026       0.183
C(regionidzip)[T.96172.0]           0.0922      0.050      1.858      0.063      -0.005       0.189
C(regionidzip)[T.96173.0]           0.0911      0.052      1.745      0.081      -0.011       0.193
C(regionidzip)[T.96174.0]           0.1300      0.051      2.540      0.011       0.030       0.230
C(regionidzip)[T.96180.0]           0.0854      0.049      1.755      0.079      -0.010       0.181
C(regionidzip)[T.96181.0]           0.0544      0.060      0.906      0.365      -0.063       0.172
C(regionidzip)[T.96183.0]           0.0825      0.059      1.405      0.160      -0.033       0.198
C(regionidzip)[T.96185.0]           0.0657      0.048      1.365      0.172      -0.029       0.160
C(regionidzip)[T.96186.0]           0.0918      0.046      2.000      0.045       0.002       0.182
C(regionidzip)[T.96190.0]           0.0561      0.045      1.242      0.214      -0.032       0.145
C(regionidzip)[T.96192.0]           0.0947      0.059      1.613      0.107      -0.020       0.210
C(regionidzip)[T.96193.0]           0.0999      0.045      2.243      0.025       0.013       0.187
C(regionidzip)[T.96197.0]           0.0841      0.048      1.756      0.079      -0.010       0.178
C(regionidzip)[T.96201.0]           0.0929      0.066      1.419      0.156      -0.035       0.221
C(regionidzip)[T.96203.0]           0.0877      0.051      1.714      0.087      -0.013       0.188
C(regionidzip)[T.96204.0]           0.0453      0.088      0.515      0.606      -0.127       0.218
C(regionidzip)[T.96206.0]           0.0701      0.048      1.470      0.142      -0.023       0.164
C(regionidzip)[T.96207.0]          -0.3899      0.118     -3.306      0.001      -0.621      -0.159
C(regionidzip)[T.96208.0]           0.1149      0.049      2.332      0.020       0.018       0.211
C(regionidzip)[T.96210.0]           0.0715      0.054      1.324      0.186      -0.034       0.177
C(regionidzip)[T.96212.0]           0.0383      0.049      0.786      0.432      -0.057       0.134
C(regionidzip)[T.96213.0]           0.0722      0.049      1.474      0.141      -0.024       0.168
C(regionidzip)[T.96215.0]           0.0528      0.060      0.880      0.379      -0.065       0.171
C(regionidzip)[T.96216.0]           0.0220      0.088      0.250      0.803      -0.150       0.194
C(regionidzip)[T.96217.0]           0.0975      0.053      1.827      0.068      -0.007       0.202
C(regionidzip)[T.96218.0]           0.0441      0.068      0.648      0.517      -0.089       0.178
C(regionidzip)[T.96220.0]           0.0614      0.053      1.151      0.250      -0.043       0.166
C(regionidzip)[T.96221.0]           0.0645      0.050      1.291      0.197      -0.033       0.162
C(regionidzip)[T.96222.0]           0.0449      0.052      0.861      0.389      -0.057       0.147
C(regionidzip)[T.96225.0]           0.0998      0.060      1.662      0.097      -0.018       0.217
C(regionidzip)[T.96226.0]          -0.1985      0.162     -1.224      0.221      -0.516       0.119
C(regionidzip)[T.96228.0]           0.1288      0.063      2.032      0.042       0.005       0.253
C(regionidzip)[T.96229.0]           0.0691      0.048      1.429      0.153      -0.026       0.164
C(regionidzip)[T.96230.0]           0.0721      0.066      1.101      0.271      -0.056       0.201
C(regionidzip)[T.96234.0]           0.1105      0.060      1.841      0.066      -0.007       0.228
C(regionidzip)[T.96236.0]           0.0862      0.046      1.866      0.062      -0.004       0.177
C(regionidzip)[T.96237.0]           0.1019      0.047      2.183      0.029       0.010       0.193
C(regionidzip)[T.96238.0]           0.0286      0.056      0.514      0.607      -0.080       0.138
C(regionidzip)[T.96239.0]           0.0991      0.052      1.917      0.055      -0.002       0.200
C(regionidzip)[T.96240.0]           0.1147      0.062      1.863      0.062      -0.006       0.235
C(regionidzip)[T.96241.0]           0.0724      0.050      1.449      0.147      -0.026       0.170
C(regionidzip)[T.96242.0]           0.1168      0.047      2.483      0.013       0.025       0.209
C(regionidzip)[T.96244.0]           0.1150      0.060      1.915      0.055      -0.003       0.233
C(regionidzip)[T.96245.0]           0.0974      0.060      1.623      0.105      -0.020       0.215
C(regionidzip)[T.96246.0]           0.0864      0.057      1.528      0.126      -0.024       0.197
C(regionidzip)[T.96247.0]           0.0571      0.047      1.219      0.223      -0.035       0.149
C(regionidzip)[T.96265.0]           0.0586      0.046      1.268      0.205      -0.032       0.149
C(regionidzip)[T.96267.0]           0.1051      0.050      2.102      0.036       0.007       0.203
C(regionidzip)[T.96268.0]           0.0626      0.050      1.253      0.210      -0.035       0.161
C(regionidzip)[T.96270.0]           0.0777      0.052      1.490      0.136      -0.025       0.180
C(regionidzip)[T.96271.0]           0.0959      0.053      1.795      0.073      -0.009       0.201
C(regionidzip)[T.96273.0]           0.0728      0.049      1.495      0.135      -0.023       0.168
C(regionidzip)[T.96275.0]           0.1095      0.062      1.778      0.075      -0.011       0.230
C(regionidzip)[T.96278.0]           0.1404      0.060      2.337      0.019       0.023       0.258
C(regionidzip)[T.96280.0]           0.1266      0.058      2.200      0.028       0.014       0.239
C(regionidzip)[T.96282.0]           0.0850      0.053      1.612      0.107      -0.018       0.188
C(regionidzip)[T.96284.0]           0.0955      0.048      1.972      0.049       0.001       0.190
C(regionidzip)[T.96289.0]           0.0774      0.057      1.369      0.171      -0.033       0.188
C(regionidzip)[T.96291.0]           0.1160      0.066      1.770      0.077      -0.012       0.244
C(regionidzip)[T.96292.0]           0.1121      0.050      2.260      0.024       0.015       0.209
C(regionidzip)[T.96293.0]           0.0799      0.054      1.478      0.139      -0.026       0.186
C(regionidzip)[T.96294.0]           0.0692      0.052      1.326      0.185      -0.033       0.171
C(regionidzip)[T.96295.0]           0.0803      0.049      1.640      0.101      -0.016       0.176
C(regionidzip)[T.96296.0]          -0.0208      0.059     -0.354      0.723      -0.136       0.094
C(regionidzip)[T.96320.0]           0.0807      0.062      1.311      0.190      -0.040       0.201
C(regionidzip)[T.96321.0]           0.0436      0.056      0.785      0.433      -0.065       0.153
C(regionidzip)[T.96322.0]           0.0874      0.071      1.226      0.220      -0.052       0.227
C(regionidzip)[T.96323.0]           0.1231      0.081      1.528      0.126      -0.035       0.281
C(regionidzip)[T.96324.0]           0.1592      0.063      2.511      0.012       0.035       0.283
C(regionidzip)[T.96325.0]           0.1137      0.054      2.105      0.035       0.008       0.220
C(regionidzip)[T.96326.0]           0.0942      0.062      1.529      0.126      -0.027       0.215
C(regionidzip)[T.96327.0]           0.0636      0.063      1.004      0.315      -0.061       0.188
C(regionidzip)[T.96329.0]          -0.1132      0.118     -0.960      0.337      -0.344       0.118
C(regionidzip)[T.96330.0]           0.0704      0.049      1.437      0.151      -0.026       0.166
C(regionidzip)[T.96336.0]           0.1055      0.048      2.212      0.027       0.012       0.199
C(regionidzip)[T.96337.0]           0.0480      0.049      0.990      0.322      -0.047       0.143
C(regionidzip)[T.96338.0]           0.0797      0.068      1.170      0.242      -0.054       0.213
C(regionidzip)[T.96339.0]           0.0881      0.050      1.763      0.078      -0.010       0.186
C(regionidzip)[T.96341.0]           0.1345      0.058      2.338      0.019       0.022       0.247
C(regionidzip)[T.96342.0]           0.1316      0.049      2.669      0.008       0.035       0.228
C(regionidzip)[T.96346.0]           0.0778      0.049      1.578      0.115      -0.019       0.174
C(regionidzip)[T.96349.0]           0.0885      0.047      1.903      0.057      -0.003       0.180
C(regionidzip)[T.96351.0]           0.1143      0.045      2.546      0.011       0.026       0.202
C(regionidzip)[T.96352.0]           0.0887      0.052      1.717      0.086      -0.013       0.190
C(regionidzip)[T.96354.0]           0.0671      0.062      1.089      0.276      -0.054       0.188
C(regionidzip)[T.96355.0]           0.1355      0.053      2.567      0.010       0.032       0.239
C(regionidzip)[T.96356.0]           0.1332      0.050      2.644      0.008       0.034       0.232
C(regionidzip)[T.96361.0]           0.0502      0.048      1.052      0.293      -0.043       0.144
C(regionidzip)[T.96364.0]           0.1708      0.046      3.754      0.000       0.082       0.260
C(regionidzip)[T.96366.0]           0.1077      0.058      1.872      0.061      -0.005       0.220
C(regionidzip)[T.96368.0]           0.0665      0.048      1.389      0.165      -0.027       0.160
C(regionidzip)[T.96369.0]           0.0569      0.049      1.169      0.242      -0.039       0.152
C(regionidzip)[T.96370.0]           0.0988      0.046      2.165      0.030       0.009       0.188
C(regionidzip)[T.96371.0]           0.1170      0.062      1.900      0.058      -0.004       0.238
C(regionidzip)[T.96373.0]           0.0846      0.045      1.863      0.063      -0.004       0.174
C(regionidzip)[T.96374.0]           0.0974      0.047      2.053      0.040       0.004       0.190
C(regionidzip)[T.96375.0]           0.1062      0.054      1.966      0.049       0.000       0.212
C(regionidzip)[T.96377.0]           0.0756      0.046      1.652      0.099      -0.014       0.165
C(regionidzip)[T.96378.0]           0.0866      0.049      1.779      0.075      -0.009       0.182
C(regionidzip)[T.96379.0]           0.1042      0.046      2.246      0.025       0.013       0.195
C(regionidzip)[T.96383.0]           0.0829      0.045      1.827      0.068      -0.006       0.172
C(regionidzip)[T.96384.0]           0.0821      0.052      1.588      0.112      -0.019       0.183
C(regionidzip)[T.96385.0]           0.0645      0.045      1.439      0.150      -0.023       0.152
C(regionidzip)[T.96387.0]           0.1182      0.054      2.187      0.029       0.012       0.224
C(regionidzip)[T.96389.0]           0.0993      0.045      2.215      0.027       0.011       0.187
C(regionidzip)[T.96393.0]           0.0740      0.052      1.432      0.152      -0.027       0.175
C(regionidzip)[T.96395.0]           0.0802      0.055      1.463      0.143      -0.027       0.188
C(regionidzip)[T.96398.0]           0.0911      0.047      1.935      0.053      -0.001       0.183
C(regionidzip)[T.96401.0]           0.0869      0.045      1.938      0.053      -0.001       0.175
C(regionidzip)[T.96403.0]           0.0701      0.051      1.368      0.171      -0.030       0.171
C(regionidzip)[T.96410.0]           0.0354      0.051      0.692      0.489      -0.065       0.136
C(regionidzip)[T.96411.0]           0.0725      0.051      1.429      0.153      -0.027       0.172
C(regionidzip)[T.96412.0]           0.0814      0.050      1.629      0.103      -0.017       0.179
C(regionidzip)[T.96414.0]           0.0862      0.053      1.635      0.102      -0.017       0.190
C(regionidzip)[T.96415.0]           0.0944      0.048      1.961      0.050    3.39e-05       0.189
C(regionidzip)[T.96420.0]           0.0836      0.056      1.504      0.133      -0.025       0.193
C(regionidzip)[T.96424.0]           0.0785      0.046      1.700      0.089      -0.012       0.169
C(regionidzip)[T.96426.0]           0.0113      0.057      0.201      0.841      -0.100       0.122
C(regionidzip)[T.96433.0]           0.0625      0.056      1.124      0.261      -0.046       0.171
C(regionidzip)[T.96434.0]           0.0586      0.099      0.592      0.554      -0.135       0.252
C(regionidzip)[T.96436.0]           0.0790      0.055      1.443      0.149      -0.028       0.186
C(regionidzip)[T.96437.0]           0.0777      0.053      1.473      0.141      -0.026       0.181
C(regionidzip)[T.96438.0]           0.0873      0.066      1.333      0.183      -0.041       0.216
C(regionidzip)[T.96446.0]           0.0451      0.049      0.920      0.357      -0.051       0.141
C(regionidzip)[T.96447.0]           0.0911      0.052      1.747      0.081      -0.011       0.193
C(regionidzip)[T.96449.0]           0.0817      0.050      1.634      0.102      -0.016       0.180
C(regionidzip)[T.96450.0]           0.0544      0.050      1.089      0.276      -0.044       0.152
C(regionidzip)[T.96451.0]           0.0603      0.053      1.144      0.253      -0.043       0.164
C(regionidzip)[T.96452.0]           0.0852      0.050      1.692      0.091      -0.013       0.184
C(regionidzip)[T.96464.0]           0.0958      0.047      2.036      0.042       0.004       0.188
C(regionidzip)[T.96465.0]           0.0960      0.048      1.984      0.047       0.001       0.191
C(regionidzip)[T.96469.0]           0.0676      0.048      1.418      0.156      -0.026       0.161
C(regionidzip)[T.96473.0]           0.0817      0.051      1.596      0.111      -0.019       0.182
C(regionidzip)[T.96474.0]           0.1485      0.058      2.581      0.010       0.036       0.261
C(regionidzip)[T.96475.0]           0.0903      0.052      1.730      0.084      -0.012       0.193
C(regionidzip)[T.96478.0]           0.0602      0.066      0.919      0.358      -0.068       0.189
C(regionidzip)[T.96479.0]           0.1383      0.068      2.032      0.042       0.005       0.272
C(regionidzip)[T.96480.0]           0.2379      0.063      3.753      0.000       0.114       0.362
C(regionidzip)[T.96485.0]           0.0937      0.053      1.756      0.079      -0.011       0.198
C(regionidzip)[T.96486.0]           0.0743      0.053      1.409      0.159      -0.029       0.178
C(regionidzip)[T.96488.0]           0.0843      0.047      1.807      0.071      -0.007       0.176
C(regionidzip)[T.96489.0]           0.0491      0.047      1.043      0.297      -0.043       0.141
C(regionidzip)[T.96490.0]           0.1096      0.059      1.867      0.062      -0.005       0.225
C(regionidzip)[T.96492.0]           0.0821      0.050      1.630      0.103      -0.017       0.181
C(regionidzip)[T.96494.0]           0.0792      0.051      1.560      0.119      -0.020       0.179
C(regionidzip)[T.96496.0]           0.0351      0.056      0.632      0.528      -0.074       0.144
C(regionidzip)[T.96497.0]           0.0483      0.062      0.784      0.433      -0.072       0.169
C(regionidzip)[T.96505.0]           0.0959      0.046      2.108      0.035       0.007       0.185
C(regionidzip)[T.96506.0]           0.0771      0.048      1.610      0.107      -0.017       0.171
C(regionidzip)[T.96507.0]           0.0997      0.050      1.997      0.046       0.002       0.198
C(regionidzip)[T.96508.0]           0.1185      0.060      1.973      0.048       0.001       0.236
C(regionidzip)[T.96510.0]           0.0638      0.053      1.210      0.226      -0.040       0.167
C(regionidzip)[T.96513.0]           0.0712      0.050      1.415      0.157      -0.027       0.170
C(regionidzip)[T.96514.0]           0.0766      0.062      1.243      0.214      -0.044       0.197
C(regionidzip)[T.96515.0]           0.1186      0.060      1.975      0.048       0.001       0.236
C(regionidzip)[T.96517.0]           0.1010      0.051      1.991      0.046       0.002       0.201
C(regionidzip)[T.96522.0]           0.0969      0.047      2.068      0.039       0.005       0.189
C(regionidzip)[T.96523.0]           0.0107      0.048      0.223      0.824      -0.083       0.105
C(regionidzip)[T.96524.0]           0.0808      0.050      1.629      0.103      -0.016       0.178
C(regionidzip)[T.96525.0]           0.0685      0.059      1.166      0.244      -0.047       0.184
C(regionidzip)[T.96531.0]           0.0708      0.057      1.253      0.210      -0.040       0.182
C(regionidzip)[T.96533.0]           0.0839      0.056      1.509      0.131      -0.025       0.193
C(regionidzip)[T.96939.0]           0.0675      0.055      1.233      0.218      -0.040       0.175
C(regionidzip)[T.96940.0]           0.0961      0.048      1.992      0.046       0.002       0.191
C(regionidzip)[T.96941.0]           0.0814      0.049      1.651      0.099      -0.015       0.178
C(regionidzip)[T.96943.0]           0.0663      0.056      1.192      0.233      -0.043       0.175
C(regionidzip)[T.96946.0]           0.0726      0.071      1.019      0.308      -0.067       0.212
C(regionidzip)[T.96947.0]           0.0793      0.051      1.550      0.121      -0.021       0.180
C(regionidzip)[T.96948.0]           0.0803      0.052      1.555      0.120      -0.021       0.182
C(regionidzip)[T.96951.0]           0.3394      0.088      3.862      0.000       0.167       0.512
C(regionidzip)[T.96952.0]           0.0872      0.053      1.654      0.098      -0.016       0.191
C(regionidzip)[T.96954.0]           0.0831      0.044      1.870      0.061      -0.004       0.170
C(regionidzip)[T.96956.0]           0.0063      0.057      0.112      0.911      -0.104       0.117
C(regionidzip)[T.96957.0]           0.0914      0.053      1.712      0.087      -0.013       0.196
C(regionidzip)[T.96958.0]           0.0816      0.048      1.685      0.092      -0.013       0.176
C(regionidzip)[T.96959.0]           0.0840      0.048      1.745      0.081      -0.010       0.178
C(regionidzip)[T.96961.0]           0.0818      0.045      1.802      0.072      -0.007       0.171
C(regionidzip)[T.96962.0]           0.0672      0.045      1.494      0.135      -0.021       0.155
C(regionidzip)[T.96963.0]           0.0014      0.046      0.030      0.976      -0.089       0.092
C(regionidzip)[T.96964.0]           0.0961      0.044      2.173      0.030       0.009       0.183
C(regionidzip)[T.96965.0]           0.1011      0.048      2.111      0.035       0.007       0.195
C(regionidzip)[T.96966.0]           0.0641      0.046      1.383      0.167      -0.027       0.155
C(regionidzip)[T.96967.0]           0.0810      0.047      1.721      0.085      -0.011       0.173
C(regionidzip)[T.96969.0]           0.1331      0.050      2.645      0.008       0.034       0.232
C(regionidzip)[T.96971.0]           0.0707      0.049      1.453      0.146      -0.025       0.166
C(regionidzip)[T.96973.0]          -0.0640      0.099     -0.647      0.517      -0.258       0.130
C(regionidzip)[T.96974.0]           0.0952      0.042      2.242      0.025       0.012       0.178
C(regionidzip)[T.96975.0]           0.1186      0.063      1.870      0.062      -0.006       0.243
C(regionidzip)[T.96978.0]           0.0983      0.045      2.190      0.029       0.010       0.186
C(regionidzip)[T.96979.0]           0.0752      0.088      0.855      0.392      -0.097       0.247
C(regionidzip)[T.96980.0]           0.0799      0.075      1.061      0.289      -0.068       0.227
C(regionidzip)[T.96981.0]           0.0486      0.053      0.910      0.363      -0.056       0.153
C(regionidzip)[T.96982.0]           0.0956      0.048      1.996      0.046       0.002       0.190
C(regionidzip)[T.96983.0]           0.0658      0.047      1.402      0.161      -0.026       0.158
C(regionidzip)[T.96985.0]           0.0929      0.047      1.980      0.048       0.001       0.185
C(regionidzip)[T.96986.0]           0.1024      0.162      0.632      0.528      -0.215       0.420
C(regionidzip)[T.96987.0]           0.0943      0.043      2.203      0.028       0.010       0.178
C(regionidzip)[T.96989.0]           0.0724      0.045      1.593      0.111      -0.017       0.161
C(regionidzip)[T.96990.0]           0.0590      0.046      1.272      0.203      -0.032       0.150
C(regionidzip)[T.96993.0]           0.0530      0.043      1.227      0.220      -0.032       0.138
C(regionidzip)[T.96995.0]           0.0861      0.044      1.940      0.052      -0.001       0.173
C(regionidzip)[T.96996.0]           0.0688      0.044      1.575      0.115      -0.017       0.154
C(regionidzip)[T.96998.0]           0.0960      0.045      2.129      0.033       0.008       0.184
C(regionidzip)[T.97001.0]           0.1768      0.059      3.012      0.003       0.062       0.292
C(regionidzip)[T.97003.0]           0.1598      0.058      2.777      0.006       0.047       0.273
C(regionidzip)[T.97004.0]           0.1275      0.048      2.674      0.008       0.034       0.221
C(regionidzip)[T.97005.0]           0.0989      0.047      2.101      0.036       0.007       0.191
C(regionidzip)[T.97006.0]           0.0975      0.056      1.754      0.079      -0.011       0.206
C(regionidzip)[T.97007.0]           0.0706      0.048      1.458      0.145      -0.024       0.165
C(regionidzip)[T.97008.0]           0.0578      0.047      1.229      0.219      -0.034       0.150
C(regionidzip)[T.97016.0]           0.0651      0.045      1.433      0.152      -0.024       0.154
C(regionidzip)[T.97018.0]           0.0676      0.050      1.362      0.173      -0.030       0.165
C(regionidzip)[T.97020.0]           0.0807      0.053      1.513      0.130      -0.024       0.185
C(regionidzip)[T.97021.0]           0.0869      0.053      1.629      0.103      -0.018       0.191
C(regionidzip)[T.97023.0]           0.0114      0.048      0.239      0.811      -0.082       0.105
C(regionidzip)[T.97024.0]           0.0958      0.047      2.018      0.044       0.003       0.189
C(regionidzip)[T.97025.0]           0.0797      0.063      1.258      0.208      -0.045       0.204
C(regionidzip)[T.97026.0]           0.0890      0.046      1.926      0.054      -0.002       0.180
C(regionidzip)[T.97027.0]           0.0911      0.051      1.794      0.073      -0.008       0.191
C(regionidzip)[T.97035.0]           0.0835      0.049      1.705      0.088      -0.012       0.180
C(regionidzip)[T.97037.0]           0.0635      0.081      0.788      0.431      -0.095       0.221
C(regionidzip)[T.97039.0]           0.0504      0.049      1.030      0.303      -0.046       0.146
C(regionidzip)[T.97040.0]           0.0443      0.060      0.738      0.461      -0.073       0.162
C(regionidzip)[T.97041.0]           0.0678      0.046      1.472      0.141      -0.022       0.158
C(regionidzip)[T.97043.0]           0.0745      0.050      1.490      0.136      -0.024       0.173
C(regionidzip)[T.97047.0]           0.0961      0.047      2.034      0.042       0.003       0.189
C(regionidzip)[T.97048.0]           0.0798      0.051      1.559      0.119      -0.021       0.180
C(regionidzip)[T.97050.0]           0.0749      0.051      1.464      0.143      -0.025       0.175
C(regionidzip)[T.97051.0]           0.1045      0.058      1.817      0.069      -0.008       0.217
C(regionidzip)[T.97052.0]           0.0708      0.056      1.273      0.203      -0.038       0.180
C(regionidzip)[T.97059.0]           0.0552      0.075      0.732      0.464      -0.093       0.203
C(regionidzip)[T.97063.0]           0.1158      0.053      2.170      0.030       0.011       0.220
C(regionidzip)[T.97064.0]           0.0613      0.059      1.044      0.296      -0.054       0.176
C(regionidzip)[T.97065.0]           0.0695      0.049      1.409      0.159      -0.027       0.166
C(regionidzip)[T.97066.0]           0.0919      0.068      1.350      0.177      -0.042       0.225
C(regionidzip)[T.97067.0]           0.1008      0.046      2.187      0.029       0.010       0.191
C(regionidzip)[T.97068.0]           0.0863      0.047      1.834      0.067      -0.006       0.179
C(regionidzip)[T.97078.0]           0.0657      0.047      1.406      0.160      -0.026       0.157
C(regionidzip)[T.97079.0]           0.0955      0.050      1.894      0.058      -0.003       0.194
C(regionidzip)[T.97081.0]           0.0527      0.050      1.046      0.295      -0.046       0.151
C(regionidzip)[T.97083.0]           0.0747      0.045      1.645      0.100      -0.014       0.164
C(regionidzip)[T.97084.0]           0.0804      0.049      1.632      0.103      -0.016       0.177
C(regionidzip)[T.97089.0]           0.0935      0.046      2.031      0.042       0.003       0.184
C(regionidzip)[T.97091.0]           0.0889      0.045      1.977      0.048       0.001       0.177
C(regionidzip)[T.97094.0]           0.0986      0.071      1.384      0.166      -0.041       0.238
C(regionidzip)[T.97097.0]           0.0831      0.048      1.726      0.084      -0.011       0.178
C(regionidzip)[T.97098.0]           0.1622      0.066      2.476      0.013       0.034       0.291
C(regionidzip)[T.97099.0]           0.0472      0.050      0.945      0.345      -0.051       0.145
C(regionidzip)[T.97101.0]           0.0767      0.050      1.547      0.122      -0.021       0.174
C(regionidzip)[T.97104.0]           0.1146      0.053      2.148      0.032       0.010       0.219
C(regionidzip)[T.97106.0]           0.0840      0.047      1.771      0.077      -0.009       0.177
C(regionidzip)[T.97107.0]           0.0685      0.051      1.350      0.177      -0.031       0.168
C(regionidzip)[T.97108.0]          -0.2492      0.162     -1.537      0.124      -0.567       0.069
C(regionidzip)[T.97109.0]           0.1068      0.051      2.105      0.035       0.007       0.206
C(regionidzip)[T.97111.0]           0.0898      0.162      0.554      0.579      -0.228       0.407
C(regionidzip)[T.97113.0]           0.0906      0.056      1.630      0.103      -0.018       0.200
C(regionidzip)[T.97116.0]           0.1033      0.045      2.288      0.022       0.015       0.192
C(regionidzip)[T.97118.0]           0.0796      0.044      1.795      0.073      -0.007       0.166
C(regionidzip)[T.97298.0]           0.1319      0.060      2.196      0.028       0.014       0.250
C(regionidzip)[T.97316.0]           0.1940      0.118      1.645      0.100      -0.037       0.425
C(regionidzip)[T.97317.0]          -0.0006      0.046     -0.013      0.989      -0.090       0.089
C(regionidzip)[T.97318.0]           0.0454      0.044      1.036      0.300      -0.040       0.131
C(regionidzip)[T.97319.0]           0.0748      0.043      1.749      0.080      -0.009       0.159
C(regionidzip)[T.97323.0]           0.1476      0.059      2.514      0.012       0.033       0.263
C(regionidzip)[T.97324.0]           0.1182      0.118      1.002      0.316      -0.113       0.349
C(regionidzip)[T.97328.0]           0.0797      0.043      1.853      0.064      -0.005       0.164
C(regionidzip)[T.97329.0]           0.0782      0.044      1.774      0.076      -0.008       0.165
C(regionidzip)[T.97330.0]           0.0625      0.046      1.347      0.178      -0.028       0.153
C(regionidzip)[T.97331.0]           0.1023      0.099      1.034      0.301      -0.092       0.296
C(regionidzip)[T.97344.0]           0.1660      0.071      2.329      0.020       0.026       0.306
ln_calculatedfinishedsquarefeet     0.0126      0.004      2.969      0.003       0.004       0.021
==============================================================================
Omnibus:                     4118.501   Durbin-Watson:                   2.002
Prob(Omnibus):                  0.000   Jarque-Bera (JB):          2425694.947
Skew:                           0.796   Prob(JB):                         0.00
Kurtosis:                      83.407   Cond. No.                     3.45e+03
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 3.45e+03. This might indicate that there are
strong multicollinearity or other numerical problems.
</pre></div>
</div>
</div>
</div>
<p><strong>Think about these categorical variables as switches</strong>. We also call them <strong>dummy or indicator variables</strong> when they are constructed in a 1/0 manner. If the variable is 1, then that observation has that characteristic. The predicted value changes by the coefficient if the indicator is 1. The switch is “turned on”./</p>
<section id="using-get-dummies">
<h3><span class="section-number">12.5.1. </span>Using get_dummies<a class="headerlink" href="#using-get-dummies" title="Permalink to this headline">#</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">pandas</span></code> also has a method called <code class="docutils literal notranslate"><span class="pre">pd.get_dummies</span></code>. Here’s description from Claude (my emphasis):</p>
<p>pd.get_dummies() is a function in the Pandas library that is used to transform categorical variables into a format that can be used in machine learning models.</p>
<p>The function takes a Pandas DataFrame or Series as input and creates a new DataFrame where each unique categorical value is represented as a new column. The new columns are binary, with a value of 1 if the observation had that categorical value, and 0 otherwise.</p>
<p>This is a common preprocessing step for many machine learning algorithms that require numeric input features. By converting categorical variables into this dummy or one-hot encoded format, the machine learning model can better understand and incorporate the information from those categorical variables.</p>
<p>For example, if you had a DataFrame with a ‘color’ column that contained the values ‘red’, ‘green’, and ‘blue’, pd.get_dummies() would create three new columns: ‘color_red’, ‘color_green’, and ‘color_blue’, each with 0s and 1s indicating the color of each observation.</p>
<p><strong>This transformation allows the machine learning model to treat these categorical variables as distinct features, rather than trying to interpret them as ordinal or numeric data</strong>. Using pd.get_dummies() is a crucial step in preparing many datasets for modeling.</p>
<p>One thing to note - it creates a new dataframe that you need to add back to the original data.</p>
<p>Here’s some code, also using <em>bedroomcnt</em>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create categorical variable based on LotShape</span>
<span class="n">bedroomcnt_categorical</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">zillow_data</span><span class="p">[</span><span class="s1">&#39;bedroomcnt&#39;</span><span class="p">],</span> <span class="n">prefix</span><span class="o">=</span><span class="s1">&#39;bedroomcnt&#39;</span><span class="p">)</span>

<span class="c1"># Concatenate the categorical variable with the original DataFrame</span>
<span class="n">zillow_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">zillow_data</span><span class="p">,</span> <span class="n">bedroomcnt_categorical</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>I created the new columns and added them to the original Zillow dataframe. Each is an indicator (1/0) variable for whether or not the house has that number of bedrooms. If it does, we flip the switch and the coefficient tells us if a house with that number of bedrooms has larger or smaller pricing errors.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">zillow_data</span><span class="o">.</span><span class="n">columns</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Index([&#39;parcelid&#39;, &#39;airconditioningtypeid&#39;, &#39;architecturalstyletypeid&#39;,
       &#39;basementsqft&#39;, &#39;bathroomcnt&#39;, &#39;bedroomcnt&#39;, &#39;buildingclasstypeid&#39;,
       &#39;buildingqualitytypeid&#39;, &#39;calculatedbathnbr&#39;, &#39;decktypeid&#39;,
       &#39;finishedfloor1squarefeet&#39;, &#39;calculatedfinishedsquarefeet&#39;,
       &#39;finishedsquarefeet12&#39;, &#39;finishedsquarefeet13&#39;, &#39;finishedsquarefeet15&#39;,
       &#39;finishedsquarefeet50&#39;, &#39;finishedsquarefeet6&#39;, &#39;fips&#39;, &#39;fireplacecnt&#39;,
       &#39;fullbathcnt&#39;, &#39;garagecarcnt&#39;, &#39;garagetotalsqft&#39;, &#39;hashottuborspa&#39;,
       &#39;heatingorsystemtypeid&#39;, &#39;latitude&#39;, &#39;longitude&#39;, &#39;lotsizesquarefeet&#39;,
       &#39;poolcnt&#39;, &#39;poolsizesum&#39;, &#39;pooltypeid10&#39;, &#39;pooltypeid2&#39;, &#39;pooltypeid7&#39;,
       &#39;propertycountylandusecode&#39;, &#39;propertylandusetypeid&#39;,
       &#39;propertyzoningdesc&#39;, &#39;rawcensustractandblock&#39;, &#39;regionidcity&#39;,
       &#39;regionidcounty&#39;, &#39;regionidneighborhood&#39;, &#39;regionidzip&#39;, &#39;roomcnt&#39;,
       &#39;storytypeid&#39;, &#39;threequarterbathnbr&#39;, &#39;typeconstructiontypeid&#39;,
       &#39;unitcnt&#39;, &#39;yardbuildingsqft17&#39;, &#39;yardbuildingsqft26&#39;, &#39;yearbuilt&#39;,
       &#39;numberofstories&#39;, &#39;fireplaceflag&#39;, &#39;structuretaxvaluedollarcnt&#39;,
       &#39;taxvaluedollarcnt&#39;, &#39;assessmentyear&#39;, &#39;landtaxvaluedollarcnt&#39;,
       &#39;taxamount&#39;, &#39;taxdelinquencyflag&#39;, &#39;taxdelinquencyyear&#39;,
       &#39;censustractandblock&#39;, &#39;logerror&#39;, &#39;transactiondate&#39;,
       &#39;ln_calculatedfinishedsquarefeet&#39;, &#39;pool_d&#39;, &#39;bedroomcnt_0.0&#39;,
       &#39;bedroomcnt_1.0&#39;, &#39;bedroomcnt_2.0&#39;, &#39;bedroomcnt_3.0&#39;, &#39;bedroomcnt_4.0&#39;,
       &#39;bedroomcnt_5.0&#39;, &#39;bedroomcnt_6.0&#39;, &#39;bedroomcnt_7.0&#39;, &#39;bedroomcnt_8.0&#39;,
       &#39;bedroomcnt_9.0&#39;, &#39;bedroomcnt_10.0&#39;, &#39;bedroomcnt_12.0&#39;],
      dtype=&#39;object&#39;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">zillow_data</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>parcelid</th>
      <th>airconditioningtypeid</th>
      <th>architecturalstyletypeid</th>
      <th>basementsqft</th>
      <th>bathroomcnt</th>
      <th>bedroomcnt</th>
      <th>buildingclasstypeid</th>
      <th>buildingqualitytypeid</th>
      <th>calculatedbathnbr</th>
      <th>decktypeid</th>
      <th>finishedfloor1squarefeet</th>
      <th>calculatedfinishedsquarefeet</th>
      <th>finishedsquarefeet12</th>
      <th>finishedsquarefeet13</th>
      <th>finishedsquarefeet15</th>
      <th>finishedsquarefeet50</th>
      <th>finishedsquarefeet6</th>
      <th>fips</th>
      <th>fireplacecnt</th>
      <th>fullbathcnt</th>
      <th>garagecarcnt</th>
      <th>garagetotalsqft</th>
      <th>hashottuborspa</th>
      <th>heatingorsystemtypeid</th>
      <th>latitude</th>
      <th>longitude</th>
      <th>lotsizesquarefeet</th>
      <th>poolcnt</th>
      <th>poolsizesum</th>
      <th>pooltypeid10</th>
      <th>pooltypeid2</th>
      <th>pooltypeid7</th>
      <th>propertycountylandusecode</th>
      <th>propertylandusetypeid</th>
      <th>propertyzoningdesc</th>
      <th>rawcensustractandblock</th>
      <th>regionidcity</th>
      <th>regionidcounty</th>
      <th>regionidneighborhood</th>
      <th>regionidzip</th>
      <th>roomcnt</th>
      <th>storytypeid</th>
      <th>threequarterbathnbr</th>
      <th>typeconstructiontypeid</th>
      <th>unitcnt</th>
      <th>yardbuildingsqft17</th>
      <th>yardbuildingsqft26</th>
      <th>yearbuilt</th>
      <th>numberofstories</th>
      <th>fireplaceflag</th>
      <th>structuretaxvaluedollarcnt</th>
      <th>taxvaluedollarcnt</th>
      <th>assessmentyear</th>
      <th>landtaxvaluedollarcnt</th>
      <th>taxamount</th>
      <th>taxdelinquencyflag</th>
      <th>taxdelinquencyyear</th>
      <th>censustractandblock</th>
      <th>logerror</th>
      <th>transactiondate</th>
      <th>ln_calculatedfinishedsquarefeet</th>
      <th>pool_d</th>
      <th>bedroomcnt_0.0</th>
      <th>bedroomcnt_1.0</th>
      <th>bedroomcnt_2.0</th>
      <th>bedroomcnt_3.0</th>
      <th>bedroomcnt_4.0</th>
      <th>bedroomcnt_5.0</th>
      <th>bedroomcnt_6.0</th>
      <th>bedroomcnt_7.0</th>
      <th>bedroomcnt_8.0</th>
      <th>bedroomcnt_9.0</th>
      <th>bedroomcnt_10.0</th>
      <th>bedroomcnt_12.0</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>13005045</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>3.0</td>
      <td>2.0</td>
      <td>NaN</td>
      <td>7.0</td>
      <td>3.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>1798.0</td>
      <td>1798.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>6037.0</td>
      <td>NaN</td>
      <td>3.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>2.0</td>
      <td>34091843.0</td>
      <td>-118047759.0</td>
      <td>7302.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0100</td>
      <td>261.0</td>
      <td>TCR17200*</td>
      <td>6.037432e+07</td>
      <td>14111.0</td>
      <td>3101.0</td>
      <td>NaN</td>
      <td>96517.0</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>1.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>1936.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>119366.0</td>
      <td>162212.0</td>
      <td>2015.0</td>
      <td>42846.0</td>
      <td>2246.17</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>6.037432e+13</td>
      <td>0.0962</td>
      <td>2016-05-18</td>
      <td>7.494430</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>17279551</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>3.0</td>
      <td>4.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>3.0</td>
      <td>NaN</td>
      <td>1387.0</td>
      <td>2302.0</td>
      <td>2302.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>1387.0</td>
      <td>NaN</td>
      <td>6111.0</td>
      <td>1.0</td>
      <td>3.0</td>
      <td>2.0</td>
      <td>671.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>34227297.0</td>
      <td>-118857914.0</td>
      <td>7258.0</td>
      <td>1.0</td>
      <td>500.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>1.0</td>
      <td>1111</td>
      <td>261.0</td>
      <td>NaN</td>
      <td>6.111006e+07</td>
      <td>34278.0</td>
      <td>2061.0</td>
      <td>NaN</td>
      <td>96383.0</td>
      <td>8.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>247.0</td>
      <td>NaN</td>
      <td>1980.0</td>
      <td>2.0</td>
      <td>NaN</td>
      <td>324642.0</td>
      <td>541069.0</td>
      <td>2015.0</td>
      <td>216427.0</td>
      <td>5972.72</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>6.111006e+13</td>
      <td>0.0020</td>
      <td>2016-09-02</td>
      <td>7.741534</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>12605376</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>2.0</td>
      <td>3.0</td>
      <td>NaN</td>
      <td>7.0</td>
      <td>2.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>1236.0</td>
      <td>1236.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>6037.0</td>
      <td>NaN</td>
      <td>2.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>7.0</td>
      <td>33817098.0</td>
      <td>-118283644.0</td>
      <td>5076.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0100</td>
      <td>261.0</td>
      <td>CARS*</td>
      <td>6.037544e+07</td>
      <td>10723.0</td>
      <td>3101.0</td>
      <td>NaN</td>
      <td>96229.0</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>1.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>1957.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>167010.0</td>
      <td>375907.0</td>
      <td>2015.0</td>
      <td>208897.0</td>
      <td>5160.90</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>6.037544e+13</td>
      <td>-0.0566</td>
      <td>2016-09-28</td>
      <td>7.119636</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>11713859</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>NaN</td>
      <td>4.0</td>
      <td>2.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>1413.0</td>
      <td>1413.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>6037.0</td>
      <td>NaN</td>
      <td>2.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>2.0</td>
      <td>34007703.0</td>
      <td>-118347262.0</td>
      <td>7725.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0100</td>
      <td>261.0</td>
      <td>LAR1</td>
      <td>6.037236e+07</td>
      <td>12447.0</td>
      <td>3101.0</td>
      <td>268097.0</td>
      <td>95989.0</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>1.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>1953.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>232690.0</td>
      <td>588746.0</td>
      <td>2015.0</td>
      <td>356056.0</td>
      <td>7353.80</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>6.037236e+13</td>
      <td>0.0227</td>
      <td>2016-02-04</td>
      <td>7.253470</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>17193642</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>3.5</td>
      <td>3.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>3.5</td>
      <td>NaN</td>
      <td>2878.0</td>
      <td>2878.0</td>
      <td>2878.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>2878.0</td>
      <td>NaN</td>
      <td>6111.0</td>
      <td>1.0</td>
      <td>3.0</td>
      <td>2.0</td>
      <td>426.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>34177668.0</td>
      <td>-118980561.0</td>
      <td>10963.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>1111</td>
      <td>261.0</td>
      <td>NaN</td>
      <td>6.111006e+07</td>
      <td>34278.0</td>
      <td>2061.0</td>
      <td>46736.0</td>
      <td>96351.0</td>
      <td>8.0</td>
      <td>NaN</td>
      <td>1.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>312.0</td>
      <td>NaN</td>
      <td>2003.0</td>
      <td>1.0</td>
      <td>NaN</td>
      <td>392869.0</td>
      <td>777041.0</td>
      <td>2015.0</td>
      <td>384172.0</td>
      <td>8668.90</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>6.111006e+13</td>
      <td>0.0237</td>
      <td>2016-06-28</td>
      <td>7.964851</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>9066</th>
      <td>12653293</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>3.0</td>
      <td>5.0</td>
      <td>NaN</td>
      <td>7.0</td>
      <td>3.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>2465.0</td>
      <td>2465.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>6037.0</td>
      <td>NaN</td>
      <td>3.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>2.0</td>
      <td>33726741.0</td>
      <td>-118301006.0</td>
      <td>4999.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0100</td>
      <td>261.0</td>
      <td>LAR1</td>
      <td>6.037297e+07</td>
      <td>12447.0</td>
      <td>3101.0</td>
      <td>54300.0</td>
      <td>96221.0</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>1.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>1931.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>271604.0</td>
      <td>559101.0</td>
      <td>2015.0</td>
      <td>287497.0</td>
      <td>6857.67</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>6.037297e+13</td>
      <td>-0.0305</td>
      <td>2016-03-18</td>
      <td>7.809947</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>9067</th>
      <td>11907619</td>
      <td>1.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>3.0</td>
      <td>3.0</td>
      <td>NaN</td>
      <td>4.0</td>
      <td>3.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>1726.0</td>
      <td>1726.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>6037.0</td>
      <td>NaN</td>
      <td>3.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>2.0</td>
      <td>34072600.0</td>
      <td>-118142000.0</td>
      <td>187293.0</td>
      <td>1.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>1.0</td>
      <td>010C</td>
      <td>266.0</td>
      <td>ALRPD*</td>
      <td>6.037481e+07</td>
      <td>50677.0</td>
      <td>3101.0</td>
      <td>NaN</td>
      <td>96533.0</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>1.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>1981.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>152919.0</td>
      <td>304640.0</td>
      <td>2015.0</td>
      <td>151721.0</td>
      <td>4676.24</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>6.037481e+13</td>
      <td>0.0266</td>
      <td>2016-03-22</td>
      <td>7.453562</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>9068</th>
      <td>14001605</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>2.0</td>
      <td>4.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>2.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>1393.0</td>
      <td>1393.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>6059.0</td>
      <td>NaN</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>451.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>33775036.0</td>
      <td>-118026925.0</td>
      <td>6279.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>122</td>
      <td>261.0</td>
      <td>NaN</td>
      <td>6.059110e+07</td>
      <td>24832.0</td>
      <td>1286.0</td>
      <td>NaN</td>
      <td>97052.0</td>
      <td>7.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>1963.0</td>
      <td>1.0</td>
      <td>NaN</td>
      <td>51206.0</td>
      <td>74870.0</td>
      <td>2015.0</td>
      <td>23664.0</td>
      <td>1418.96</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>6.059110e+13</td>
      <td>0.0090</td>
      <td>2016-06-21</td>
      <td>7.239215</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>9069</th>
      <td>12892836</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>2.0</td>
      <td>3.0</td>
      <td>NaN</td>
      <td>4.0</td>
      <td>2.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>1967.0</td>
      <td>1967.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>6037.0</td>
      <td>NaN</td>
      <td>2.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>2.0</td>
      <td>34080798.0</td>
      <td>-117768156.0</td>
      <td>18129.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0100</td>
      <td>261.0</td>
      <td>POR110000*</td>
      <td>6.037402e+07</td>
      <td>20008.0</td>
      <td>3101.0</td>
      <td>NaN</td>
      <td>96508.0</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>1.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>1951.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>166585.0</td>
      <td>238968.0</td>
      <td>2015.0</td>
      <td>72383.0</td>
      <td>3122.67</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>6.037402e+13</td>
      <td>0.0497</td>
      <td>2016-07-27</td>
      <td>7.584265</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>9070</th>
      <td>13859474</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>3.5</td>
      <td>3.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>3.5</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>3268.0</td>
      <td>3268.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>6059.0</td>
      <td>NaN</td>
      <td>3.0</td>
      <td>1.0</td>
      <td>396.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>33666961.0</td>
      <td>-118010325.0</td>
      <td>2875.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>122</td>
      <td>261.0</td>
      <td>NaN</td>
      <td>6.059099e+07</td>
      <td>25218.0</td>
      <td>1286.0</td>
      <td>NaN</td>
      <td>96966.0</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>1.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>1990.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>377735.0</td>
      <td>1134478.0</td>
      <td>2015.0</td>
      <td>756743.0</td>
      <td>13417.84</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>6.059099e+13</td>
      <td>-0.0747</td>
      <td>2016-08-31</td>
      <td>8.091933</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>9071 rows × 74 columns</p>
</div></div></div>
</div>
<p>If you are creating a feature data frame that <strong>only includes exactly what you need</strong>, you should drop the original variable once you’ve created the indicators that you need. In this case, you would drop <em>bedroomcnt</em>.</p>
<p><code class="docutils literal notranslate"><span class="pre">pd.get_dummies</span></code> has an option <code class="docutils literal notranslate"><span class="pre">dtype=</span></code> for the output. You can choose to get <strong>booleans</strong> (True or False) or <strong>integers</strong> (1/0) for the output. You’ll want integers for your regression models.</p>
</section>
</section>
<section id="hull-chapters-3-and-10-linear-regression-and-prediction">
<h2><span class="section-number">12.6. </span>Hull, Chapters 3 and 10. Linear regression and prediction.<a class="headerlink" href="#hull-chapters-3-and-10-linear-regression-and-prediction" title="Permalink to this headline">#</a></h2>
<p>Chapter 3.1 starts off with the math behind linear regression with one feature. This is just what we did above, when we tried to explain pricing errors with one variable. This math and how regression works should be familiar from your introductory statistics course.</p>
<section id="machine-learning-and-prediction">
<h3><span class="section-number">12.6.1. </span>Machine learning and prediction<a class="headerlink" href="#machine-learning-and-prediction" title="Permalink to this headline">#</a></h3>
<p>We’re going to introduce <strong>machine learning concepts</strong> now. Notice the idea of a <strong>training set</strong>. This is the data that we use to create our model. We are then going to take the outputs (i.e. the coefficients) from this model and use it to predict values in the <strong>out-of-sample</strong> data.</p>
<p>In machine learning applications, we have an <strong>X</strong> dataframe and a <strong>y</strong> dataframe. The <strong>X</strong> dataframe has all of our features. They are typically standardized. This means doing something like taking every numerical feature and turning it into a <strong>z-score</strong>.</p>
<p>The <strong>y</strong> is the <strong>target</strong> variable. The thing that you are trying to predict. You’ll also standardize this, typically.</p>
<p>And, crucially, the <strong>X</strong> data frame <strong>only</strong> contains the features we want. No extra columns. Nothing that we used earlier to create a new variable. If it is in <strong>X</strong>, then it is going into the model. This is different from using other packages or languages, where you might write out your regression model. In fact, I do that above when using <code class="docutils literal notranslate"><span class="pre">statsmodel</span></code>.</p>
<p><strong>Why don’t people write out their models in machine learning? Too many variables and the algo is picking your model for you!</strong></p>
<p>The also textbook introduces the idea of <strong>three different types of data sets: training, validation, and testing</strong>. Let’s ask Claude the differences:</p>
<p>In machine learning, data is typically split into three subsets: training data, validation data, and test data. Each subset serves a different purpose in the model development and evaluation process.</p>
<p><strong>Training Data</strong>: The training data is the largest portion of the dataset, and it is used to train the machine learning model. During the training process, the model learns patterns and relationships from the training data. The model’s parameters are adjusted to minimize the error or loss function on this data.</p>
<p><strong>Validation Data</strong>: The validation data is a subset of the data held out from the training process. It is used to evaluate the performance of the model during the training phase. The validation data provides an unbiased estimate of the model’s performance on unseen data. It helps tune the <strong>hyperparameters</strong> of the model, such as the learning rate, regularization strength, or the number of iterations. The validation data is used to prevent <strong>overfitting</strong>, which occurs when the model performs well on the training data but fails to generalize to new, unseen data. By monitoring the performance on the validation data, early stopping or regularization techniques can be applied to improve the model’s generalization capability.</p>
<p><strong>Test Data</strong>: The test data is a separate subset of the data that is completely isolated from the training and validation processes. It is used to evaluate the final performance of the trained model on unseen data, providing an unbiased estimate of the model’s real-world performance. The test data should be representative of the data the model will encounter in production or real-world scenarios. The performance metrics calculated on the test data, such as accuracy, precision, recall, or F1-score, are considered the final, reliable indicators of the model’s effectiveness.</p>
<p>It is crucial to keep the test data separate and untouched during the training and validation phases to avoid any data leakage or overfitting to the test data. The test data should only be used once, after the model has been fully trained and tuned, to obtain an unbiased estimate of its performance.</p>
<p>By splitting the data into these three subsets, machine learning practitioners can train models effectively, tune hyperparameters, and evaluate the final performance objectively, ensuring that the model will generalize well to new, unseen data.</p>
<p>As you’ll see, sometimes we only use a training and test data. We’ll validate the model within the training data.</p>
<p><code class="docutils literal notranslate"><span class="pre">sklearn</span></code> has many built-in features to help, like ways to automatically split your data, though the textbook tends to do things manually.</p>
<p>Chapter 3.2 introduces <strong>multiple features</strong>. Again, this just means multiple explanatory X variables. Eq. 3.1 gives us the general form of a linear regression model:</p>
<div class="amsmath math notranslate nohighlight" id="equation-c345276c-cf44-459e-87f5-07da1c52a9f2">
<span class="eqno">(12.1)<a class="headerlink" href="#equation-c345276c-cf44-459e-87f5-07da1c52a9f2" title="Permalink to this equation">#</a></span>\[\begin{align}
Y = a + b_1 X_1 + b_2 X_2 + \cdots + b_m X_m + \varepsilon
\end{align}\]</div>
<p>The <a class="reference external" href="https://www.statsmodels.org/dev/examples/notebooks/generated/ols.html">statsmodel</a> page for linear regression has helpful code examples, including how to generate dummy variables (see below).</p>
</section>
<section id="important-regression-statistics">
<h3><span class="section-number">12.6.2. </span>Important regression statistics<a class="headerlink" href="#important-regression-statistics" title="Permalink to this headline">#</a></h3>
<p>What is regression trying to do? Hull discusses these linear models from the machine learning perspective. So, rather than a discussion of the <a class="reference external" href="https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem">Gauss-Markov theorem and BLUE</a>, we get something less theoretical.</p>
<p>In short, the linear regression algorithm selects values for <span class="math notranslate nohighlight">\(a\)</span> and each <span class="math notranslate nohighlight">\(b\)</span> above that minimize the sum of the squared differences between each Y and its predicted value (i.e. the right-hand side of the equation). This is shown in Eq. 3.2. Why do we square the differences? The direction of the error doesn’t matter - we want predicted values to be close to the actual value.</p>
<p>The text notes that <span class="math notranslate nohighlight">\(a\)</span> is called the model <strong>bias</strong> and the <span class="math notranslate nohighlight">\(b\)</span> values are called the model <strong>weights</strong>. In econometrics, we call <span class="math notranslate nohighlight">\(a\)</span> the <strong>intercept</strong> and the <span class="math notranslate nohighlight">\(b\)</span> values <strong>coefficients</strong>.</p>
<p>Machine learning techniques are all about <strong>prediction</strong>. How well does the model predict or fit the data? To measure this, we can use the <strong>R-Squared</strong> value for the model.</p>
<div class="amsmath math notranslate nohighlight" id="equation-fea24690-4dcb-453d-b9ec-93c6a844362d">
<span class="eqno">(12.2)<a class="headerlink" href="#equation-fea24690-4dcb-453d-b9ec-93c6a844362d" title="Permalink to this equation">#</a></span>\[\begin{align}
R^2 = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}
\end{align}\]</div>
<p>We can also put this into words:</p>
<div class="amsmath math notranslate nohighlight" id="equation-262f3e60-847a-4b57-93be-bdad7085f09e">
<span class="eqno">(12.3)<a class="headerlink" href="#equation-262f3e60-847a-4b57-93be-bdad7085f09e" title="Permalink to this equation">#</a></span>\[\begin{align}
R^2 = 1 - \frac{\text{Variance of the errors } \epsilon}{\text{Variation of the observations on the target Y}}
\end{align}\]</div>
<p>As noted by the text:</p>
<blockquote>
<div><p>The R-squared statistic is between zero and one and measures the proportion of the variance in the target that is explained by the features.</p>
</div></blockquote>
<p>This is a statistics for the entire model. We also care about how well each feature (variable) does in explaining the variation in our outcome/target/Y variable. We use <em>t</em>-statistics for that.</p>
<p>Again, from the text:</p>
<blockquote>
<div><p>The <em>t</em>-statistic of the <span class="math notranslate nohighlight">\(a\)</span> or <span class="math notranslate nohighlight">\(𝑏_𝑗\)</span> parameters estimated by a linear regression is the value of the parameter divided by its standard error. The P-value is the probability of obtaining a <em>t</em>-statistic as large as the one observed if we were in the situation where the parameter had no explanatory power at all. A P-value of 5% or less is generally considered as indicating that the parameter is significant. If the P-value for the parameter <span class="math notranslate nohighlight">\(b_j\)</span> in equation (3.1) is less than 5%, we are over 95% confident that the feature <span class="math notranslate nohighlight">\(X_j\)</span> has some effect on Y. The critical t-statistic for 95% confidence when the data set is large is 1.96 (i.e., t-statistics greater than  1.96 are significant in the sense that they give rise to P-values less than 5%).</p>
</div></blockquote>
<p>I would take issue with some of the language in that explanation. These values are about relationships and correlations, but I see <em>effect</em> in that description. Saying that these correlations are actually causal relationships requires assumptions that go beyond pure statistics. You need a model. In our case, this would mean an economic model that describes incentives and behavior.</p>
</section>
<section id="categorical-features-again">
<h3><span class="section-number">12.6.3. </span>Categorical features, again<a class="headerlink" href="#categorical-features-again" title="Permalink to this headline">#</a></h3>
<p>We saw these above, when we used <em>bedroomcnt</em>. From the Hull text:</p>
<blockquote>
<div><p>A common practice is to use the same number of dummy variables as there are categories and associate each dummy variable with a particular category. The value of a dummy variable is one if the corresponding category applies and zero otherwise. This is referred to as <strong>one-hot encoding</strong>.</p>
</div></blockquote>
<p>We used <code class="docutils literal notranslate"><span class="pre">pd.get_dummies</span></code> above to one-hot encode some indicators.</p>
<p>The text also discusses the <strong>dummy variable trap</strong>. If you create a dummy variable for each categorical value and then add a constant to the regression model, your model is <strong>overfit</strong> and will not estimate. In economics, we usually include a constant or intercept term in our regressions. This isn’t always the case in machine learning.</p>
<p>To avoid this trap, <strong>drop one of the indicators for the set</strong>. So, suppose that you’ve taken a variable called <em>Color</em> that gets your three indicators for <em>Color_Red</em>, <em>Color_Green</em>, or
<em>Color_Blue</em>. <strong>Every</strong> observation has a 1 in one of those columns and zeroes in the others. Drop one of the indicators, like <em>Color_Red</em> - remove the column from your data frame. Your model will now run.</p>
<p>By doing this, the <em>Color_Red</em> observations are those where <em>Color_Green</em> and <em>Color_Blue</em> are both zero. <em>Color_Green</em> and <em>Color_Blue</em> are estimated relative to <em>Color_Red</em>. For example, if the coefficient for <em>Color_Green</em> is 0.5, the the y-variable is shifted by 0.5 when <em>Color_Green</em> is equal to one, relative to the default prediction for the <em>Color_Red</em>.</p>
<p>As noted above, though, in machine learning, we often don’t spend a lot of time interpreting the coefficients like this. Instead, we want to include as much information as possible in order to make a prediction. Get your model to run and focus on the predicted y variables.</p>
</section>
<section id="regularization-and-ridge-regression">
<h3><span class="section-number">12.6.4. </span>Regularization and Ridge regression<a class="headerlink" href="#regularization-and-ridge-regression" title="Permalink to this headline">#</a></h3>
<p>These next three methods come from the machine learning literature and help to deal with having <strong>too many explanatory variables</strong>. When I say this, I mean that they come more from the computer science and statistics world than the econometrics world. This distinction is meaningful if you’re trained in one set of methods vs. another, but is becoming less important over time as machine learning and econometrics borrow from each other.</p>
<aside class="margin sidebar">
<p class="sidebar-title">What is econometrics?</p>
<p>Econometrics is usually interested in causal relationships among economic agents. So, does more schooling increase wages, or do the people who are good at school also earn more? These are difficult questions to answer and economists have developed a variety of tools to help. If you’ve taken econometrics, then you’ve seen topics like instrumental variables and difference-in-differences. In economics, we’re more interested in explaining why something is happening than predicting an outcome. So, did a change in a zoning law affect housing prices, rather than predicting a pricing error.</p>
</aside>
<p>Why are too many explanatory (X) variables a problem? They make the math harder when different X variables are correlated with one another. In other words, we have different features in our data that are really explaining the same thing. The book uses the area of the basement and the area of the first floor of a house as an example. These values are different, but capture the same thing, generally speaking - house size.</p>
<p><strong>Regularization</strong> helps use reduce the number of variables used in our model. We are also going to see more <strong>scaling</strong> of features. It is important to scale (e.g. Z-score) in order for different features to be comparable with each other.</p>
<p>A larger, positive z-score means that the observation is above the mean value for that feature (X) or target (y). Vice-versa for negative values. Values are centered around zero after scaling this way. As Hull notes, there are other ways to scale variables, but this Z-score method is very common.</p>
<p>Here’s a <a class="reference external" href="https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html">sklearn article</a> on the importance of scaling your features before using them in a machine learning model. From the article:</p>
<blockquote>
<div><p>Feature scaling through standardization, also called Z-score normalization, is an important preprocessing step for many machine learning algorithms. It involves rescaling each feature such that it has a standard deviation of 1 and a mean of 0.</p>
</div></blockquote>
<p>One way to regularize our model is to use a <strong>Ridge regression</strong>. This algorithm chooses <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span> parameters to minimize this function:</p>
<div class="amsmath math notranslate nohighlight" id="equation-b254e032-5d3a-44bc-b508-edde44566484">
<span class="eqno">(12.4)<a class="headerlink" href="#equation-b254e032-5d3a-44bc-b508-edde44566484" title="Permalink to this equation">#</a></span>\[\begin{align}
\frac{1}{n}\sum_{i=1}^{n}(Y_i - a - b_1X_{i1} - b_2X_{i2} - \cdots - b_mX_{im})^2 + \lambda\sum_{j=1}^{m}b_j^2
\end{align}\]</div>
<p>Note that this looks a lot like a normal regression, except that we now have the last term with <span class="math notranslate nohighlight">\(\lambda\)</span> and <span class="math notranslate nohighlight">\(b^2\)</span>. From the text:</p>
<blockquote>
<div><p>This change has the effect of encouraging the model to keep the weights <span class="math notranslate nohighlight">\(b_j\)</span> as small as possible.</p>
</div></blockquote>
<p>This method is also called <strong>L2 regularization</strong>. Why do we do this? From the text:</p>
<blockquote>
<div><p>L2 regularization is one way of avoiding the situation such as that in equation (3.3) where there are two correlated features, one with a large positive weight and the other with a large negative weight. Its general effect is to <strong>simplify a linear regression model</strong>. Sometimes, the model obtained after regularization generalizes better than the model before regularization.</p>
</div></blockquote>
<p>How do we choose a value for <span class="math notranslate nohighlight">\(\lambda\)</span>? From the text:</p>
<blockquote>
<div><p>The parameter <span class="math notranslate nohighlight">\(\lambda\)</span> is referred to as a <strong>hyperparameter</strong> because it is used as a tool to determine the best model but is not part of the model itself. The choice of a value for <span class="math notranslate nohighlight">\(\lambda\)</span> is obviously important. A large value for λ would lead to all the <span class="math notranslate nohighlight">\(b_j\)</span> being set equal to zero. (The resulting model would then be uninteresting as it would always predict a value equal to <span class="math notranslate nohighlight">\(a\)</span> for Y.) In practice, it is desirable to try several different values for <span class="math notranslate nohighlight">\(\lambda\)</span> and choose between the resulting models.</p>
</div></blockquote>
<p>If <span class="math notranslate nohighlight">\(\lambda\)</span> is set to zero, then we have linear regression. As you increase <span class="math notranslate nohighlight">\(\lambda\)</span>, the model will become <strong>less complex</strong>.</p>
<p>Take a look at <strong>footnote 3</strong>. <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> uses the <strong>sum of squared errors</strong> rather than the <strong>mean sum of squared errors</strong>. In other words, there’s no <span class="math notranslate nohighlight">\(\frac{1}{n}\)</span> out in front of the Ridge regression formula. So, you have to multiple your lambda term by <span class="math notranslate nohighlight">\(n\)</span>, or the number of observations, when using it. See more below, where I work the example in the Hull text.</p>
</section>
<section id="lasso-regression">
<h3><span class="section-number">12.6.5. </span>LASSO regression<a class="headerlink" href="#lasso-regression" title="Permalink to this headline">#</a></h3>
<p>From the Hull text:</p>
<blockquote>
<div><p>In Ridge, we added a constant times the sum of the squared weights to the objective function. In LASSO, we add a constant times the sum of the <strong>absolute weights</strong>.</p>
</div></blockquote>
<p>LASSO is also called <strong>L1 regularization</strong>. This changes the last term in the equation that we are trying to minimize:</p>
<div class="amsmath math notranslate nohighlight" id="equation-8b27425c-78e2-4ae6-854d-c3921a9d89aa">
<span class="eqno">(12.5)<a class="headerlink" href="#equation-8b27425c-78e2-4ae6-854d-c3921a9d89aa" title="Permalink to this equation">#</a></span>\[\begin{align}
\frac{1}{n}\sum_{i=1}^{n}(Y_i - a - b_1X_{i1} - b_2X_{i2} - \cdots - b_mX_{im})^2 + \lambda\sum_{j=1}^{m}|b_j|
\end{align}\]</div>
<p>I asked the Claude GPT to explain this equation:</p>
<p>This equation represents the objective function for LASSO (Least Absolute Shrinkage and Selection Operator) regression. LASSO is a type of linear regression that uses L1 regularization to add a penalty term to the cost function. This penalty term encourages sparse solutions, driving some coefficient estimates to exactly zero, effectively performing variable selection and improving interpretability of the model. The first term is the regular least squares error term that measures the model’s fit to the data. The second term is the L1 regularization penalty on the sum of the absolute values of the coefficients (<span class="math notranslate nohighlight">\(|b_j|\)</span>), multiplied by a tuning parameter lambda that controls the strength of the penalty.</p>
<p>By minimizing this objective function, LASSO regression aims to find the best fit linear model while also promoting sparsity in the coefficients, making it useful for scenarios with many potential predictor variables.</p>
<p>Using LASSO is going to mean having more <span class="math notranslate nohighlight">\(b = 0\)</span> values. In other words, some features are “kicked out of the model” in order to simplify things. This is going to happen when <strong>certain features explain the same variation in the Y variable</strong>.</p>
<p>Again, check out footnote 4 for some details on how lASSO works in <code class="docutils literal notranslate"><span class="pre">sklearn</span></code>.</p>
</section>
<section id="elastic-net-regression">
<h3><span class="section-number">12.6.6. </span>Elastic net regression<a class="headerlink" href="#elastic-net-regression" title="Permalink to this headline">#</a></h3>
<p>Elastic Net regression is a <strong>mixture of Ridge and LASSO</strong>. The function to be minimized includes both a constant times the sum of the squared weights and a different constant times the sum of the absolute values of the weights.</p>
<div class="amsmath math notranslate nohighlight" id="equation-bd61ba70-7b6d-434f-8308-76ad01968b05">
<span class="eqno">(12.6)<a class="headerlink" href="#equation-bd61ba70-7b6d-434f-8308-76ad01968b05" title="Permalink to this equation">#</a></span>\[\begin{align}
\frac{1}{n}\sum_{i=1}^{n}(Y_i - a - b_1X_{i1} - b_2X_{i2} - \cdots - b_mX_{im})^2 + \lambda_1\sum_{j=1}^{m}b_j^2 + \lambda_2\sum_{j=1}^{m}|b_j|
\end{align}\]</div>
<p>From the text:</p>
<blockquote>
<div><p>In LASSO some weights are set to zero, but others may be quite large in magnitude. In Ridge, weights are small in magnitude, but they are not reduced to zero. The idea underlying Elastic Net is that we may be able to get the <strong>best of both worlds</strong> by making some weights zero while reducing the magnitude of the others.</p>
</div></blockquote>
<p>From Claude:</p>
<p>This equation represents the objective function for Elastic Net regularization, which is a linear regression model that combines the L1 (LASSO) and L2 (Ridge) regularization techniques. The first term is the standard mean squared error term, which measures the difference between the predicted values and the actual target values (<span class="math notranslate nohighlight">\(Y_i\)</span>) for the given data points (<span class="math notranslate nohighlight">\(X_{i1}\)</span>, <span class="math notranslate nohighlight">\(X_{i2}\)</span>, …, <span class="math notranslate nohighlight">\(X_{im}\)</span>).</p>
<p>The second term is the L2 regularization penalty, also known as the Ridge penalty, which adds the sum of the squared coefficients (<span class="math notranslate nohighlight">\(b_j^2\)</span>) multiplied by a tuning parameter <span class="math notranslate nohighlight">\(λ_1\)</span>. This penalty encourages small coefficient values and helps to prevent overfitting.</p>
<p>The third term is the L1 regularization penalty, also known as the LASSO penalty, which adds the sum of the absolute values of the coefficients (<span class="math notranslate nohighlight">\(|b_j|\)</span>) multiplied by a tuning parameter <span class="math notranslate nohighlight">\(λ_2\)</span>. This penalty promotes sparsity in the model by driving some coefficients to exactly zero, effectively performing variable selection.</p>
<p>By combining the L1 and L2 penalties, Elastic Net regularization aims to benefit from the strengths of both Ridge and LASSO regularization. It can handle correlated predictors better than LASSO and can select more predictive features than Ridge regression. The tuning parameters <span class="math notranslate nohighlight">\(λ_1\)</span> and <span class="math notranslate nohighlight">\(λ_2\)</span> control the relative importance of the L1 and L2 penalties, respectively. Elastic Net regularization is particularly useful when dealing with high-dimensional data with potentially correlated predictors, where both variable selection and coefficient shrinkage are desirable.</p>
</section>
<section id="using-our-tools-to-predict-housing-prices">
<h3><span class="section-number">12.6.7. </span>Using our tools to predict housing prices<a class="headerlink" href="#using-our-tools-to-predict-housing-prices" title="Permalink to this headline">#</a></h3>
<p>I’m going to replicate the Hull textbook results below. This data has housing sale prices and features about each house. The houses are in Iowa and represent sales over a four year period. The data has already been scaled so that each feature has a mean of 0 and a standard deviation of 1.</p>
<p>The text describes the data and its structure. We are going to create training, validation, and testing data sets. Note that the data are already cleaned up for you. If you were doing this on your own, you’d need to do the steps described on pg. 65.</p>
<p>You do this in one of our lab assignments.</p>
<p>Notice how this is a different question from the Zillow data. We’re directly predicting housing prices here, not Zillow’s pricing errors. Zillow was trying to identify the types of houses that were difficult to price using their Zestimate model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Both features and target have already been scaled: mean = 0; SD = 1</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;https://raw.githubusercontent.com/aaiken1/fin-data-analysis-python/main/data/Houseprice_data_scaled.csv&#39;</span><span class="p">)</span> 
</pre></div>
</div>
</div>
</div>
<p>We are going to split this data into <strong>training</strong> and <strong>validation</strong> sets. We’ll estimate the model on the training data and then check it using the validation set. Note the use of <code class="docutils literal notranslate"><span class="pre">.iloc</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># First 1800 data items are training set; the next 600 are the validation set</span>
<span class="n">train</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:</span><span class="mi">1800</span><span class="p">]</span> 
<span class="n">val</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">1800</span><span class="p">:</span><span class="mi">2400</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>When we split the data like this, we’re assuming that the data are already randomly shuffled. In practice, this probably isn’t a great assumption. <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> has built-in tools to do this.</p>
<p>We then separate the X and Y variables into do different data sets. This is just the way the <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> package wants things.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Creating the &quot;X&quot; and &quot;y&quot; variables. We drop sale price from &quot;X&quot;</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_val</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;Sale Price&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">val</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;Sale Price&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y_train</span><span class="p">,</span> <span class="n">y_val</span> <span class="o">=</span> <span class="n">train</span><span class="p">[[</span><span class="s1">&#39;Sale Price&#39;</span><span class="p">]],</span> <span class="n">val</span><span class="p">[[</span><span class="s1">&#39;Sale Price&#39;</span><span class="p">]]</span> 
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_train</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Sale Price</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.358489</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.008849</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.552733</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-0.528560</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.895898</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>LotArea</th>
      <th>OverallQual</th>
      <th>OverallCond</th>
      <th>YearBuilt</th>
      <th>YearRemodAdd</th>
      <th>BsmtFinSF1</th>
      <th>BsmtUnfSF</th>
      <th>TotalBsmtSF</th>
      <th>1stFlrSF</th>
      <th>2ndFlrSF</th>
      <th>GrLivArea</th>
      <th>FullBath</th>
      <th>HalfBath</th>
      <th>BedroomAbvGr</th>
      <th>TotRmsAbvGrd</th>
      <th>Fireplaces</th>
      <th>GarageCars</th>
      <th>GarageArea</th>
      <th>WoodDeckSF</th>
      <th>OpenPorchSF</th>
      <th>EnclosedPorch</th>
      <th>Blmngtn</th>
      <th>Blueste</th>
      <th>BrDale</th>
      <th>BrkSide</th>
      <th>ClearCr</th>
      <th>CollgCr</th>
      <th>Crawfor</th>
      <th>Edwards</th>
      <th>Gilbert</th>
      <th>IDOTRR</th>
      <th>MeadowV</th>
      <th>Mitchel</th>
      <th>Names</th>
      <th>NoRidge</th>
      <th>NPkVill</th>
      <th>NriddgHt</th>
      <th>NWAmes</th>
      <th>OLDTown</th>
      <th>SWISU</th>
      <th>Sawyer</th>
      <th>SawyerW</th>
      <th>Somerst</th>
      <th>StoneBr</th>
      <th>Timber</th>
      <th>Veenker</th>
      <th>Bsmt Qual</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-0.199572</td>
      <td>0.652747</td>
      <td>-0.512407</td>
      <td>1.038851</td>
      <td>0.875754</td>
      <td>0.597837</td>
      <td>-0.937245</td>
      <td>-0.482464</td>
      <td>-0.808820</td>
      <td>1.203988</td>
      <td>0.423045</td>
      <td>0.789653</td>
      <td>1.208820</td>
      <td>0.173443</td>
      <td>0.960943</td>
      <td>-0.947675</td>
      <td>0.304041</td>
      <td>0.348200</td>
      <td>-0.749963</td>
      <td>0.222420</td>
      <td>-0.357173</td>
      <td>-0.103258</td>
      <td>-0.04718</td>
      <td>-0.10597</td>
      <td>-0.19657</td>
      <td>-0.134497</td>
      <td>3.222830</td>
      <td>-0.177486</td>
      <td>-0.255054</td>
      <td>-0.253817</td>
      <td>-0.150714</td>
      <td>-0.108618</td>
      <td>-0.188821</td>
      <td>-0.446195</td>
      <td>-0.163696</td>
      <td>-0.097618</td>
      <td>-0.251329</td>
      <td>-0.23069</td>
      <td>-0.286942</td>
      <td>-0.136621</td>
      <td>-0.2253</td>
      <td>-0.214192</td>
      <td>-0.268378</td>
      <td>-0.127929</td>
      <td>-0.152629</td>
      <td>-0.091644</td>
      <td>0.584308</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-0.072005</td>
      <td>-0.072527</td>
      <td>2.189741</td>
      <td>0.136810</td>
      <td>-0.432225</td>
      <td>1.218528</td>
      <td>-0.635042</td>
      <td>0.490326</td>
      <td>0.276358</td>
      <td>-0.789421</td>
      <td>-0.490655</td>
      <td>0.789653</td>
      <td>-0.769249</td>
      <td>0.173443</td>
      <td>-0.297934</td>
      <td>0.608725</td>
      <td>0.304041</td>
      <td>-0.070795</td>
      <td>1.637145</td>
      <td>-0.718903</td>
      <td>-0.357173</td>
      <td>-0.103258</td>
      <td>-0.04718</td>
      <td>-0.10597</td>
      <td>-0.19657</td>
      <td>-0.134497</td>
      <td>-0.310114</td>
      <td>-0.177486</td>
      <td>-0.255054</td>
      <td>-0.253817</td>
      <td>-0.150714</td>
      <td>-0.108618</td>
      <td>-0.188821</td>
      <td>-0.446195</td>
      <td>-0.163696</td>
      <td>-0.097618</td>
      <td>-0.251329</td>
      <td>-0.23069</td>
      <td>-0.286942</td>
      <td>-0.136621</td>
      <td>-0.2253</td>
      <td>-0.214192</td>
      <td>-0.268378</td>
      <td>-0.127929</td>
      <td>-0.152629</td>
      <td>10.905682</td>
      <td>0.584308</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.111026</td>
      <td>0.652747</td>
      <td>-0.512407</td>
      <td>0.972033</td>
      <td>0.827310</td>
      <td>0.095808</td>
      <td>-0.296754</td>
      <td>-0.329118</td>
      <td>-0.637758</td>
      <td>1.231999</td>
      <td>0.578048</td>
      <td>0.789653</td>
      <td>1.208820</td>
      <td>0.173443</td>
      <td>-0.297934</td>
      <td>0.608725</td>
      <td>0.304041</td>
      <td>0.633878</td>
      <td>-0.749963</td>
      <td>-0.070779</td>
      <td>-0.357173</td>
      <td>-0.103258</td>
      <td>-0.04718</td>
      <td>-0.10597</td>
      <td>-0.19657</td>
      <td>-0.134497</td>
      <td>3.222830</td>
      <td>-0.177486</td>
      <td>-0.255054</td>
      <td>-0.253817</td>
      <td>-0.150714</td>
      <td>-0.108618</td>
      <td>-0.188821</td>
      <td>-0.446195</td>
      <td>-0.163696</td>
      <td>-0.097618</td>
      <td>-0.251329</td>
      <td>-0.23069</td>
      <td>-0.286942</td>
      <td>-0.136621</td>
      <td>-0.2253</td>
      <td>-0.214192</td>
      <td>-0.268378</td>
      <td>-0.127929</td>
      <td>-0.152629</td>
      <td>-0.091644</td>
      <td>0.584308</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-0.077551</td>
      <td>0.652747</td>
      <td>-0.512407</td>
      <td>-1.901135</td>
      <td>-0.722887</td>
      <td>-0.520319</td>
      <td>-0.057698</td>
      <td>-0.722067</td>
      <td>-0.528171</td>
      <td>0.975236</td>
      <td>0.437322</td>
      <td>-1.039659</td>
      <td>-0.769249</td>
      <td>0.173443</td>
      <td>0.331504</td>
      <td>0.608725</td>
      <td>1.648693</td>
      <td>0.795763</td>
      <td>-0.749963</td>
      <td>-0.178800</td>
      <td>4.059560</td>
      <td>-0.103258</td>
      <td>-0.04718</td>
      <td>-0.10597</td>
      <td>-0.19657</td>
      <td>-0.134497</td>
      <td>-0.310114</td>
      <td>5.631132</td>
      <td>-0.255054</td>
      <td>-0.253817</td>
      <td>-0.150714</td>
      <td>-0.108618</td>
      <td>-0.188821</td>
      <td>-0.446195</td>
      <td>-0.163696</td>
      <td>-0.097618</td>
      <td>-0.251329</td>
      <td>-0.23069</td>
      <td>-0.286942</td>
      <td>-0.136621</td>
      <td>-0.2253</td>
      <td>-0.214192</td>
      <td>-0.268378</td>
      <td>-0.127929</td>
      <td>-0.152629</td>
      <td>-0.091644</td>
      <td>-0.577852</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.444919</td>
      <td>1.378022</td>
      <td>-0.512407</td>
      <td>0.938624</td>
      <td>0.730423</td>
      <td>0.481458</td>
      <td>-0.170461</td>
      <td>0.209990</td>
      <td>-0.036366</td>
      <td>1.668495</td>
      <td>1.418326</td>
      <td>0.789653</td>
      <td>1.208820</td>
      <td>1.407428</td>
      <td>1.590381</td>
      <td>0.608725</td>
      <td>1.648693</td>
      <td>1.719456</td>
      <td>0.788039</td>
      <td>0.577345</td>
      <td>-0.357173</td>
      <td>-0.103258</td>
      <td>-0.04718</td>
      <td>-0.10597</td>
      <td>-0.19657</td>
      <td>-0.134497</td>
      <td>-0.310114</td>
      <td>-0.177486</td>
      <td>-0.255054</td>
      <td>-0.253817</td>
      <td>-0.150714</td>
      <td>-0.108618</td>
      <td>-0.188821</td>
      <td>-0.446195</td>
      <td>6.105502</td>
      <td>-0.097618</td>
      <td>-0.251329</td>
      <td>-0.23069</td>
      <td>-0.286942</td>
      <td>-0.136621</td>
      <td>-0.2253</td>
      <td>-0.214192</td>
      <td>-0.268378</td>
      <td>-0.127929</td>
      <td>-0.152629</td>
      <td>-0.091644</td>
      <td>0.584308</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Now we can create the model. We are creating a linear regression object from the <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> library. We then fit the model using our X and y training data. The results live inside of this object.</p>
<p><strong>See how there’s no equation?</strong> All of the features that I want to include are in the <code class="docutils literal notranslate"><span class="pre">X_train</span></code> data frame. My target variable is in <code class="docutils literal notranslate"><span class="pre">y_train</span></code>. I’m using the features to predict my target.</p>
<p>You can not do this step unless those two data frames are exactly what you want, no extra columns, everything is numeric, etc.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>

<span class="n">lr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>#sk-container-id-1 {
  /* Definition of color scheme common for light and dark mode */
  --sklearn-color-text: black;
  --sklearn-color-line: gray;
  /* Definition of color scheme for unfitted estimators */
  --sklearn-color-unfitted-level-0: #fff5e6;
  --sklearn-color-unfitted-level-1: #f6e4d2;
  --sklearn-color-unfitted-level-2: #ffe0b3;
  --sklearn-color-unfitted-level-3: chocolate;
  /* Definition of color scheme for fitted estimators */
  --sklearn-color-fitted-level-0: #f0f8ff;
  --sklearn-color-fitted-level-1: #d4ebff;
  --sklearn-color-fitted-level-2: #b3dbfd;
  --sklearn-color-fitted-level-3: cornflowerblue;

  /* Specific color for light theme */
  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));
  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-icon: #696969;

  @media (prefers-color-scheme: dark) {
    /* Redefinition of color scheme for dark theme */
    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));
    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-icon: #878787;
  }
}

#sk-container-id-1 {
  color: var(--sklearn-color-text);
}

#sk-container-id-1 pre {
  padding: 0;
}

#sk-container-id-1 input.sk-hidden--visually {
  border: 0;
  clip: rect(1px 1px 1px 1px);
  clip: rect(1px, 1px, 1px, 1px);
  height: 1px;
  margin: -1px;
  overflow: hidden;
  padding: 0;
  position: absolute;
  width: 1px;
}

#sk-container-id-1 div.sk-dashed-wrapped {
  border: 1px dashed var(--sklearn-color-line);
  margin: 0 0.4em 0.5em 0.4em;
  box-sizing: border-box;
  padding-bottom: 0.4em;
  background-color: var(--sklearn-color-background);
}

#sk-container-id-1 div.sk-container {
  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`
     but bootstrap.min.css set `[hidden] { display: none !important; }`
     so we also need the `!important` here to be able to override the
     default hidden behavior on the sphinx rendered scikit-learn.org.
     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */
  display: inline-block !important;
  position: relative;
}

#sk-container-id-1 div.sk-text-repr-fallback {
  display: none;
}

div.sk-parallel-item,
div.sk-serial,
div.sk-item {
  /* draw centered vertical line to link estimators */
  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));
  background-size: 2px 100%;
  background-repeat: no-repeat;
  background-position: center center;
}

/* Parallel-specific style estimator block */

#sk-container-id-1 div.sk-parallel-item::after {
  content: "";
  width: 100%;
  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);
  flex-grow: 1;
}

#sk-container-id-1 div.sk-parallel {
  display: flex;
  align-items: stretch;
  justify-content: center;
  background-color: var(--sklearn-color-background);
  position: relative;
}

#sk-container-id-1 div.sk-parallel-item {
  display: flex;
  flex-direction: column;
}

#sk-container-id-1 div.sk-parallel-item:first-child::after {
  align-self: flex-end;
  width: 50%;
}

#sk-container-id-1 div.sk-parallel-item:last-child::after {
  align-self: flex-start;
  width: 50%;
}

#sk-container-id-1 div.sk-parallel-item:only-child::after {
  width: 0;
}

/* Serial-specific style estimator block */

#sk-container-id-1 div.sk-serial {
  display: flex;
  flex-direction: column;
  align-items: center;
  background-color: var(--sklearn-color-background);
  padding-right: 1em;
  padding-left: 1em;
}


/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is
clickable and can be expanded/collapsed.
- Pipeline and ColumnTransformer use this feature and define the default style
- Estimators will overwrite some part of the style using the `sk-estimator` class
*/

/* Pipeline and ColumnTransformer style (default) */

#sk-container-id-1 div.sk-toggleable {
  /* Default theme specific background. It is overwritten whether we have a
  specific estimator or a Pipeline/ColumnTransformer */
  background-color: var(--sklearn-color-background);
}

/* Toggleable label */
#sk-container-id-1 label.sk-toggleable__label {
  cursor: pointer;
  display: block;
  width: 100%;
  margin-bottom: 0;
  padding: 0.5em;
  box-sizing: border-box;
  text-align: center;
}

#sk-container-id-1 label.sk-toggleable__label-arrow:before {
  /* Arrow on the left of the label */
  content: "▸";
  float: left;
  margin-right: 0.25em;
  color: var(--sklearn-color-icon);
}

#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {
  color: var(--sklearn-color-text);
}

/* Toggleable content - dropdown */

#sk-container-id-1 div.sk-toggleable__content {
  max-height: 0;
  max-width: 0;
  overflow: hidden;
  text-align: left;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content pre {
  margin: 0.2em;
  border-radius: 0.25em;
  color: var(--sklearn-color-text);
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content.fitted pre {
  /* unfitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {
  /* Expand drop-down */
  max-height: 200px;
  max-width: 100%;
  overflow: auto;
}

#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {
  content: "▾";
}

/* Pipeline/ColumnTransformer-specific style */

#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator-specific style */

/* Colorize estimator box */
#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

#sk-container-id-1 div.sk-label label.sk-toggleable__label,
#sk-container-id-1 div.sk-label label {
  /* The background is the default theme color */
  color: var(--sklearn-color-text-on-default-background);
}

/* On hover, darken the color of the background */
#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

/* Label box, darken color on hover, fitted */
#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator label */

#sk-container-id-1 div.sk-label label {
  font-family: monospace;
  font-weight: bold;
  display: inline-block;
  line-height: 1.2em;
}

#sk-container-id-1 div.sk-label-container {
  text-align: center;
}

/* Estimator-specific */
#sk-container-id-1 div.sk-estimator {
  font-family: monospace;
  border: 1px dotted var(--sklearn-color-border-box);
  border-radius: 0.25em;
  box-sizing: border-box;
  margin-bottom: 0.5em;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-estimator.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

/* on hover */
#sk-container-id-1 div.sk-estimator:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-estimator.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Specification for estimator info (e.g. "i" and "?") */

/* Common style for "i" and "?" */

.sk-estimator-doc-link,
a:link.sk-estimator-doc-link,
a:visited.sk-estimator-doc-link {
  float: right;
  font-size: smaller;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1em;
  height: 1em;
  width: 1em;
  text-decoration: none !important;
  margin-left: 1ex;
  /* unfitted */
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
  color: var(--sklearn-color-unfitted-level-1);
}

.sk-estimator-doc-link.fitted,
a:link.sk-estimator-doc-link.fitted,
a:visited.sk-estimator-doc-link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
div.sk-estimator:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover,
div.sk-label-container:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover,
div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

/* Span, style for the box shown on hovering the info icon */
.sk-estimator-doc-link span {
  display: none;
  z-index: 9999;
  position: relative;
  font-weight: normal;
  right: .2ex;
  padding: .5ex;
  margin: .5ex;
  width: min-content;
  min-width: 20ex;
  max-width: 50ex;
  color: var(--sklearn-color-text);
  box-shadow: 2pt 2pt 4pt #999;
  /* unfitted */
  background: var(--sklearn-color-unfitted-level-0);
  border: .5pt solid var(--sklearn-color-unfitted-level-3);
}

.sk-estimator-doc-link.fitted span {
  /* fitted */
  background: var(--sklearn-color-fitted-level-0);
  border: var(--sklearn-color-fitted-level-3);
}

.sk-estimator-doc-link:hover span {
  display: block;
}

/* "?"-specific style due to the `<a>` HTML tag */

#sk-container-id-1 a.estimator_doc_link {
  float: right;
  font-size: 1rem;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1rem;
  height: 1rem;
  width: 1rem;
  text-decoration: none;
  /* unfitted */
  color: var(--sklearn-color-unfitted-level-1);
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
}

#sk-container-id-1 a.estimator_doc_link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
#sk-container-id-1 a.estimator_doc_link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

#sk-container-id-1 a.estimator_doc_link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
}
</style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" checked><label for="sk-estimator-id-1" class="sk-toggleable__label fitted sk-toggleable__label-arrow fitted">&nbsp;&nbsp;LinearRegression<a class="sk-estimator-doc-link fitted" rel="noreferrer" target="_blank" href="https://scikit-learn.org/1.4/modules/generated/sklearn.linear_model.LinearRegression.html">?<span>Documentation for LinearRegression</span></a><span class="sk-estimator-doc-link fitted">i<span>Fitted</span></span></label><div class="sk-toggleable__content fitted"><pre>LinearRegression()</pre></div> </div></div></div></div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">type</span><span class="p">(</span><span class="n">lr</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>sklearn.linear_model._base.LinearRegression
</pre></div>
</div>
</div>
</div>
<p>We can pull values out of this model, but they aren’t easy to read.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr</span><span class="o">.</span><span class="n">intercept_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([-21.19019074])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr</span><span class="o">.</span><span class="n">coef_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[ 7.89996182e-02,  2.14717552e-01,  9.56481737e-02,
         1.59774166e-01,  2.53592150e-02,  9.21317505e-02,
        -3.24714879e-02,  1.37952329e-01,  1.60622470e-01,
         1.42606877e-01,  1.50644782e-01, -2.11863336e-02,
         1.65778126e-02, -8.61985315e-02,  8.55122885e-02,
         2.75430180e-02,  3.80069938e-02,  5.24721744e-02,
         2.06536750e-02,  3.37980535e-02,  7.96765227e-03,
        -2.46373502e+10, -1.13518959e+10, -2.52702906e+10,
        -4.56375077e+10, -3.18567418e+10, -6.82182158e+10,
        -4.14919931e+10, -5.77464022e+10, -5.75003236e+10,
        -3.55362475e+10, -2.58870676e+10, -4.39648876e+10,
        -8.97245268e+10, -3.84436950e+10, -2.33176817e+10,
        -5.70040358e+10, -5.28173934e+10, -6.39280776e+10,
        -3.23415240e+10, -5.17039287e+10, -4.93843358e+10,
        -6.03675110e+10, -3.03524335e+10, -3.59674861e+10,
        -2.19154290e+10,  1.12609212e-02]])
</pre></div>
</div>
</div>
</div>
<p>Let’s put that same information into an easier to read Dataframe. We can output the coefficients, or <span class="math notranslate nohighlight">\(b\)</span> values, from the model, along with the intercept, or <span class="math notranslate nohighlight">\(a\)</span> value.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create dataFrame with corresponding feature and its respective coefficients</span>
<span class="n">coeffs</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="p">[</span><span class="s1">&#39;intercept&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">columns</span><span class="p">),</span>
        <span class="nb">list</span><span class="p">(</span><span class="n">lr</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">lr</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="p">]</span>
<span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">()</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">coeffs</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>1</th>
    </tr>
    <tr>
      <th>0</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>intercept</th>
      <td>-21.190191</td>
    </tr>
    <tr>
      <th>LotArea</th>
      <td>0.079</td>
    </tr>
    <tr>
      <th>OverallQual</th>
      <td>0.214718</td>
    </tr>
    <tr>
      <th>OverallCond</th>
      <td>0.095648</td>
    </tr>
    <tr>
      <th>YearBuilt</th>
      <td>0.159774</td>
    </tr>
    <tr>
      <th>YearRemodAdd</th>
      <td>0.025359</td>
    </tr>
    <tr>
      <th>BsmtFinSF1</th>
      <td>0.092132</td>
    </tr>
    <tr>
      <th>BsmtUnfSF</th>
      <td>-0.032471</td>
    </tr>
    <tr>
      <th>TotalBsmtSF</th>
      <td>0.137952</td>
    </tr>
    <tr>
      <th>1stFlrSF</th>
      <td>0.160622</td>
    </tr>
    <tr>
      <th>2ndFlrSF</th>
      <td>0.142607</td>
    </tr>
    <tr>
      <th>GrLivArea</th>
      <td>0.150645</td>
    </tr>
    <tr>
      <th>FullBath</th>
      <td>-0.021186</td>
    </tr>
    <tr>
      <th>HalfBath</th>
      <td>0.016578</td>
    </tr>
    <tr>
      <th>BedroomAbvGr</th>
      <td>-0.086199</td>
    </tr>
    <tr>
      <th>TotRmsAbvGrd</th>
      <td>0.085512</td>
    </tr>
    <tr>
      <th>Fireplaces</th>
      <td>0.027543</td>
    </tr>
    <tr>
      <th>GarageCars</th>
      <td>0.038007</td>
    </tr>
    <tr>
      <th>GarageArea</th>
      <td>0.052472</td>
    </tr>
    <tr>
      <th>WoodDeckSF</th>
      <td>0.020654</td>
    </tr>
    <tr>
      <th>OpenPorchSF</th>
      <td>0.033798</td>
    </tr>
    <tr>
      <th>EnclosedPorch</th>
      <td>0.007968</td>
    </tr>
    <tr>
      <th>Blmngtn</th>
      <td>-24637350197.078827</td>
    </tr>
    <tr>
      <th>Blueste</th>
      <td>-11351895856.743935</td>
    </tr>
    <tr>
      <th>BrDale</th>
      <td>-25270290553.680084</td>
    </tr>
    <tr>
      <th>BrkSide</th>
      <td>-45637507690.321465</td>
    </tr>
    <tr>
      <th>ClearCr</th>
      <td>-31856741827.675114</td>
    </tr>
    <tr>
      <th>CollgCr</th>
      <td>-68218215762.424385</td>
    </tr>
    <tr>
      <th>Crawfor</th>
      <td>-41491993082.377075</td>
    </tr>
    <tr>
      <th>Edwards</th>
      <td>-57746402199.340149</td>
    </tr>
    <tr>
      <th>Gilbert</th>
      <td>-57500323644.365379</td>
    </tr>
    <tr>
      <th>IDOTRR</th>
      <td>-35536247506.6325</td>
    </tr>
    <tr>
      <th>MeadowV</th>
      <td>-25887067602.412392</td>
    </tr>
    <tr>
      <th>Mitchel</th>
      <td>-43964887601.134361</td>
    </tr>
    <tr>
      <th>Names</th>
      <td>-89724526838.225357</td>
    </tr>
    <tr>
      <th>NoRidge</th>
      <td>-38443695035.200378</td>
    </tr>
    <tr>
      <th>NPkVill</th>
      <td>-23317681658.563988</td>
    </tr>
    <tr>
      <th>NriddgHt</th>
      <td>-57004035819.630913</td>
    </tr>
    <tr>
      <th>NWAmes</th>
      <td>-52817393350.746841</td>
    </tr>
    <tr>
      <th>OLDTown</th>
      <td>-63928077560.686485</td>
    </tr>
    <tr>
      <th>SWISU</th>
      <td>-32341524013.175518</td>
    </tr>
    <tr>
      <th>Sawyer</th>
      <td>-51703928722.056145</td>
    </tr>
    <tr>
      <th>SawyerW</th>
      <td>-49384335826.616394</td>
    </tr>
    <tr>
      <th>Somerst</th>
      <td>-60367510970.28315</td>
    </tr>
    <tr>
      <th>StoneBr</th>
      <td>-30352433482.16647</td>
    </tr>
    <tr>
      <th>Timber</th>
      <td>-35967486112.807251</td>
    </tr>
    <tr>
      <th>Veenker</th>
      <td>-21915428977.273361</td>
    </tr>
    <tr>
      <th>Bsmt Qual</th>
      <td>0.011261</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>These results match what’s on pg. 66 of the Hull text. Note that there are 25 neighborhood dummy variables. Basement quality (<span class="math notranslate nohighlight">\(BsmtQual\)</span>) is also a categorical variable with ordered values.</p>
<p>We can get the <span class="math notranslate nohighlight">\(R-Squared\)</span> value from this model. This also matches what’s in the book.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.8859117782508982
</pre></div>
</div>
</div>
</div>
<p>By the way, <code class="docutils literal notranslate"><span class="pre">statsmodel</span></code> gives much nicer output. You have to manually include a constant/intercept term, the <span class="math notranslate nohighlight">\(a\)</span> value, in the Hull text.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#add constant to predictor variables</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>

<span class="c1">#fit linear regression model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1">#view model summary</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                            OLS Regression Results                            
==============================================================================
Dep. Variable:             Sale Price   R-squared:                       0.886
Model:                            OLS   Adj. R-squared:                  0.883
Method:                 Least Squares   F-statistic:                     295.9
Date:                Fri, 19 Apr 2024   Prob (F-statistic):               0.00
Time:                        16:29:22   Log-Likelihood:                -599.81
No. Observations:                1800   AIC:                             1294.
Df Residuals:                    1753   BIC:                             1552.
Df Model:                          46                                         
Covariance Type:            nonrobust                                         
=================================================================================
                    coef    std err          t      P&gt;|t|      [0.025      0.975]
---------------------------------------------------------------------------------
const         -3.063e-11      0.008   -3.8e-09      1.000      -0.016       0.016
LotArea           0.0790      0.009      8.411      0.000       0.061       0.097
OverallQual       0.2144      0.015     14.107      0.000       0.185       0.244
OverallCond       0.0965      0.010      9.224      0.000       0.076       0.117
YearBuilt         0.1608      0.021      7.488      0.000       0.119       0.203
YearRemodAdd      0.0254      0.013      1.981      0.048       0.000       0.050
BsmtFinSF1        0.0915      0.023      3.960      0.000       0.046       0.137
BsmtUnfSF        -0.0331      0.023     -1.435      0.152      -0.078       0.012
TotalBsmtSF       0.1382      0.027      5.079      0.000       0.085       0.192
1stFlrSF          0.1528      0.063      2.413      0.016       0.029       0.277
2ndFlrSF          0.1328      0.071      1.880      0.060      -0.006       0.271
GrLivArea         0.1613      0.080      2.016      0.044       0.004       0.318
FullBath         -0.0208      0.014     -1.496      0.135      -0.048       0.006
HalfBath          0.0172      0.012      1.402      0.161      -0.007       0.041
BedroomAbvGr     -0.0835      0.013     -6.444      0.000      -0.109      -0.058
TotRmsAbvGrd      0.0832      0.017      4.814      0.000       0.049       0.117
Fireplaces        0.0283      0.010      2.740      0.006       0.008       0.048
GarageCars        0.0380      0.020      1.900      0.058      -0.001       0.077
GarageArea        0.0518      0.019      2.701      0.007       0.014       0.089
WoodDeckSF        0.0208      0.009      2.336      0.020       0.003       0.038
OpenPorchSF       0.0341      0.009      3.755      0.000       0.016       0.052
EnclosedPorch     0.0068      0.009      0.751      0.452      -0.011       0.025
Blmngtn          -0.0184      0.009     -2.144      0.032      -0.035      -0.002
Blueste          -0.0129      0.008     -1.592      0.112      -0.029       0.003
BrDale           -0.0246      0.008     -2.960      0.003      -0.041      -0.008
BrkSide           0.0208      0.009      2.275      0.023       0.003       0.039
ClearCr          -0.0074      0.008     -0.877      0.381      -0.024       0.009
CollgCr          -0.0068      0.009     -0.775      0.439      -0.024       0.010
Crawfor           0.0363      0.009      4.207      0.000       0.019       0.053
Edwards          -0.0007      0.008     -0.083      0.934      -0.017       0.016
Gilbert          -0.0083      0.009     -0.895      0.371      -0.027       0.010
IDOTRR           -0.0015      0.009     -0.170      0.865      -0.019       0.016
MeadowV          -0.0164      0.008     -1.968      0.049      -0.033   -5.32e-05
Mitchel          -0.0285      0.008     -3.562      0.000      -0.044      -0.013
Names            -0.0385      0.009     -4.506      0.000      -0.055      -0.022
NoRidge           0.0516      0.009      5.848      0.000       0.034       0.069
NPkVill          -0.0220      0.008     -2.670      0.008      -0.038      -0.006
NriddgHt          0.1240      0.010     12.403      0.000       0.104       0.144
NWAmes           -0.0518      0.008     -6.398      0.000      -0.068      -0.036
OLDTown          -0.0265      0.011     -2.409      0.016      -0.048      -0.005
SWISU            -0.0041      0.009     -0.461      0.645      -0.022       0.013
Sawyer           -0.0181      0.008     -2.205      0.028      -0.034      -0.002
SawyerW          -0.0283      0.008     -3.520      0.000      -0.044      -0.013
Somerst           0.0275      0.010      2.856      0.004       0.009       0.046
StoneBr           0.0631      0.009      7.371      0.000       0.046       0.080
Timber           -0.0028      0.008     -0.326      0.744      -0.019       0.014
Veenker           0.0024      0.008      0.292      0.770      -0.014       0.019
Bsmt Qual         0.0113      0.014      0.792      0.428      -0.017       0.039
==============================================================================
Omnibus:                      499.808   Durbin-Watson:                   1.938
Prob(Omnibus):                  0.000   Jarque-Bera (JB):             5182.610
Skew:                           0.995   Prob(JB):                         0.00
Kurtosis:                      11.071   Cond. No.                     2.36e+15
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The smallest eigenvalue is 2.36e-27. This might indicate that there are
strong multicollinearity problems or that the design matrix is singular.
</pre></div>
</div>
</div>
</div>
<p>I hope that output likes this is familiar! <span class="math notranslate nohighlight">\(LotArea\)</span>, <span class="math notranslate nohighlight">\(OverallQual\)</span>, <span class="math notranslate nohighlight">\(OverallCond\)</span>, and <span class="math notranslate nohighlight">\(YearBuilt\)</span> all of strong positive relationships with sale price. That makes sense. However, note that there are some strange negative relationships, like <span class="math notranslate nohighlight">\(BedroomAbvGr\)</span>. The book notes some of these negative coefficients. This is because “kitchen sink” models like this, where you use everything to predict something, end up with a lot of <strong>multicollinearity</strong>. Our variables are related to each other. This can lead to strange coefficient values.</p>
</section>
<section id="making-predictions">
<h3><span class="section-number">12.6.8. </span>Making predictions<a class="headerlink" href="#making-predictions" title="Permalink to this headline">#</a></h3>
<p>Now, let’s use these model parameters (<span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span> values) to <strong>predict</strong> prices in the <strong>validation</strong> data set. To do this, we are going go back to <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> and use <code class="docutils literal notranslate"><span class="pre">predict</span></code>. I am taking my linear regression object <strong>lr</strong>. This object contains the model that I estimated using my <strong>training</strong> data. It has all of the coefficients and the intercept.</p>
<p>I can, in essence, multiply the coefficients by their corresponding features and get predicted values. You can see this regression linear algebra in the Hull text.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Make predictions using the validation set</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">lr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>

<span class="c1"># The coefficients</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Coefficients: </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">lr</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="c1"># The mean squared error</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Mean squared error: </span><span class="si">%.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">mse</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
<span class="c1"># The coefficient of determination: 1 is perfect prediction</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Coefficient of determination: </span><span class="si">%.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">r2</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Coefficients: 
 [[ 7.89996182e-02  2.14717552e-01  9.56481737e-02  1.59774166e-01
   2.53592150e-02  9.21317505e-02 -3.24714879e-02  1.37952329e-01
   1.60622470e-01  1.42606877e-01  1.50644782e-01 -2.11863336e-02
   1.65778126e-02 -8.61985315e-02  8.55122885e-02  2.75430180e-02
   3.80069938e-02  5.24721744e-02  2.06536750e-02  3.37980535e-02
   7.96765227e-03 -2.46373502e+10 -1.13518959e+10 -2.52702906e+10
  -4.56375077e+10 -3.18567418e+10 -6.82182158e+10 -4.14919931e+10
  -5.77464022e+10 -5.75003236e+10 -3.55362475e+10 -2.58870676e+10
  -4.39648876e+10 -8.97245268e+10 -3.84436950e+10 -2.33176817e+10
  -5.70040358e+10 -5.28173934e+10 -6.39280776e+10 -3.23415240e+10
  -5.17039287e+10 -4.93843358e+10 -6.03675110e+10 -3.03524335e+10
  -3.59674861e+10 -2.19154290e+10  1.12609212e-02]]
Mean squared error: 0.12
Coefficient of determination: 0.90
</pre></div>
</div>
</div>
</div>
<p>What is in <code class="docutils literal notranslate"><span class="pre">y_pred</span></code>? These are the predicted scaled housing prices using the house data that we “held out” in the validation sample.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_pred</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[-0.53041773],
       [-0.62749415],
       [-0.16133814],
       [-0.88292628],
       [-0.98506861],
       [-0.54711085],
       [-1.20387459],
       [-1.24238777],
       [-0.71181422],
       [-1.4379139 ]])
</pre></div>
</div>
</div>
</div>
<p>Now, those don’t look like housing prices to me! That’s because Hull already scaled the data in the original CSV file. These are <strong>predicted, scaled values</strong>. If we had scaled this data using a transformer from the <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> package, then we could easily unscale them here and get real housing prices. I discuss scaling data more below.</p>
<p>We can compare, though, how the predicted values differ from the actual values. We have computed predicted values from the model coefficients using the test data and the X features held out of sample. But, remember, we also have the actual y values. Let’s <strong>compare the two graphically</strong>. I’ll use some <code class="docutils literal notranslate"><span class="pre">matplotlib</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create a figure and axis object</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="c1"># Scatter plot of actual vs predicted values</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="c1"># Add a diagonal line for reference</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">y_val</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">y_val</span><span class="o">.</span><span class="n">max</span><span class="p">()],</span> <span class="p">[</span><span class="n">y_val</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">y_val</span><span class="o">.</span><span class="n">max</span><span class="p">()],</span> <span class="s1">&#39;k--&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Set labels and title</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Actual Values&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Predicted Values&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Actual vs Predicted Values&#39;</span><span class="p">)</span>

<span class="c1"># Show the plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/12_regression_topics_59_0.png" src="../_images/12_regression_topics_59_0.png" />
</div>
</div>
</section>
<section id="ridge-regression">
<h3><span class="section-number">12.6.9. </span>Ridge regression<a class="headerlink" href="#ridge-regression" title="Permalink to this headline">#</a></h3>
<p>Let’s try a <strong>Ridge regression</strong> now. Note that <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> uses an <span class="math notranslate nohighlight">\(\alpha\)</span> term, rather than <span class="math notranslate nohighlight">\(\lambda\)</span>, like in the text. The two are related. See the footnotes in Chapter 3.</p>
<p>This code is running different Ridge regressions with different <span class="math notranslate nohighlight">\(\alpha\)</span> terms, reporting the mean squared error for each. The error will go up as <span class="math notranslate nohighlight">\(\alpha\)</span> increases, but we’re looking for values where that increase isn’t as large.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># The alpha used by Python&#39;s ridge should be the lambda in Hull&#39;s book times the number of observations</span>
<span class="n">alphas</span><span class="o">=</span><span class="p">[</span><span class="mf">0.01</span><span class="o">*</span><span class="mi">1800</span><span class="p">,</span> <span class="mf">0.02</span><span class="o">*</span><span class="mi">1800</span><span class="p">,</span> <span class="mf">0.03</span><span class="o">*</span><span class="mi">1800</span><span class="p">,</span> <span class="mf">0.04</span><span class="o">*</span><span class="mi">1800</span><span class="p">,</span> <span class="mf">0.05</span><span class="o">*</span><span class="mi">1800</span><span class="p">,</span> <span class="mf">0.075</span><span class="o">*</span><span class="mi">1800</span><span class="p">,</span><span class="mf">0.1</span><span class="o">*</span><span class="mi">1800</span><span class="p">,</span><span class="mf">0.2</span><span class="o">*</span><span class="mi">1800</span><span class="p">,</span> <span class="mf">0.4</span><span class="o">*</span><span class="mi">1800</span><span class="p">]</span>
<span class="n">mses</span><span class="o">=</span><span class="p">[]</span>
<span class="k">for</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="n">alphas</span><span class="p">:</span>
    <span class="n">ridge</span><span class="o">=</span><span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>
    <span class="n">ridge</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
    <span class="n">pred</span><span class="o">=</span><span class="n">ridge</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>
    <span class="n">mses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mse</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span><span class="n">pred</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">mse</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span><span class="n">pred</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.11703284346091351
0.11710797319752991
0.11723952924901117
0.11741457158889518
0.1176238406871146
0.11825709631198014
0.11900057469147925
0.12254649996292954
0.13073599680747128
</pre></div>
</div>
</div>
</div>
<p>Let’s plot <span class="math notranslate nohighlight">\(\alpha\)</span> values on the y-axis and MSE on the x-axis.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">alphas</span><span class="p">,</span> <span class="n">mses</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&lt;matplotlib.lines.Line2D at 0x363c8d4f0&gt;]
</pre></div>
</div>
<img alt="../_images/12_regression_topics_63_1.png" src="../_images/12_regression_topics_63_1.png" />
</div>
</div>
<p>Note that this isn’t exactly what’s in the text. Figure 3.8 has <span class="math notranslate nohighlight">\(\lambda\)</span> on the x-axis and Variance Unexplained on the y-axis. This plot gives the same information, though.</p>
</section>
<section id="id1">
<h3><span class="section-number">12.6.10. </span>Lasso regression<a class="headerlink" href="#id1" title="Permalink to this headline">#</a></h3>
<p>Let’s look at a <strong>lasso regression</strong>. We create the lasso object with a specific alpha hyperparameter. Then, we feed that object the necessary data. We’ll use our training data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Here we produce results for alpha=0.05 which corresponds to lambda=0.1 in Hull&#39;s book</span>
<span class="n">lasso</span> <span class="o">=</span> <span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
<span class="n">lasso</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>#sk-container-id-2 {
  /* Definition of color scheme common for light and dark mode */
  --sklearn-color-text: black;
  --sklearn-color-line: gray;
  /* Definition of color scheme for unfitted estimators */
  --sklearn-color-unfitted-level-0: #fff5e6;
  --sklearn-color-unfitted-level-1: #f6e4d2;
  --sklearn-color-unfitted-level-2: #ffe0b3;
  --sklearn-color-unfitted-level-3: chocolate;
  /* Definition of color scheme for fitted estimators */
  --sklearn-color-fitted-level-0: #f0f8ff;
  --sklearn-color-fitted-level-1: #d4ebff;
  --sklearn-color-fitted-level-2: #b3dbfd;
  --sklearn-color-fitted-level-3: cornflowerblue;

  /* Specific color for light theme */
  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));
  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-icon: #696969;

  @media (prefers-color-scheme: dark) {
    /* Redefinition of color scheme for dark theme */
    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));
    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-icon: #878787;
  }
}

#sk-container-id-2 {
  color: var(--sklearn-color-text);
}

#sk-container-id-2 pre {
  padding: 0;
}

#sk-container-id-2 input.sk-hidden--visually {
  border: 0;
  clip: rect(1px 1px 1px 1px);
  clip: rect(1px, 1px, 1px, 1px);
  height: 1px;
  margin: -1px;
  overflow: hidden;
  padding: 0;
  position: absolute;
  width: 1px;
}

#sk-container-id-2 div.sk-dashed-wrapped {
  border: 1px dashed var(--sklearn-color-line);
  margin: 0 0.4em 0.5em 0.4em;
  box-sizing: border-box;
  padding-bottom: 0.4em;
  background-color: var(--sklearn-color-background);
}

#sk-container-id-2 div.sk-container {
  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`
     but bootstrap.min.css set `[hidden] { display: none !important; }`
     so we also need the `!important` here to be able to override the
     default hidden behavior on the sphinx rendered scikit-learn.org.
     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */
  display: inline-block !important;
  position: relative;
}

#sk-container-id-2 div.sk-text-repr-fallback {
  display: none;
}

div.sk-parallel-item,
div.sk-serial,
div.sk-item {
  /* draw centered vertical line to link estimators */
  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));
  background-size: 2px 100%;
  background-repeat: no-repeat;
  background-position: center center;
}

/* Parallel-specific style estimator block */

#sk-container-id-2 div.sk-parallel-item::after {
  content: "";
  width: 100%;
  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);
  flex-grow: 1;
}

#sk-container-id-2 div.sk-parallel {
  display: flex;
  align-items: stretch;
  justify-content: center;
  background-color: var(--sklearn-color-background);
  position: relative;
}

#sk-container-id-2 div.sk-parallel-item {
  display: flex;
  flex-direction: column;
}

#sk-container-id-2 div.sk-parallel-item:first-child::after {
  align-self: flex-end;
  width: 50%;
}

#sk-container-id-2 div.sk-parallel-item:last-child::after {
  align-self: flex-start;
  width: 50%;
}

#sk-container-id-2 div.sk-parallel-item:only-child::after {
  width: 0;
}

/* Serial-specific style estimator block */

#sk-container-id-2 div.sk-serial {
  display: flex;
  flex-direction: column;
  align-items: center;
  background-color: var(--sklearn-color-background);
  padding-right: 1em;
  padding-left: 1em;
}


/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is
clickable and can be expanded/collapsed.
- Pipeline and ColumnTransformer use this feature and define the default style
- Estimators will overwrite some part of the style using the `sk-estimator` class
*/

/* Pipeline and ColumnTransformer style (default) */

#sk-container-id-2 div.sk-toggleable {
  /* Default theme specific background. It is overwritten whether we have a
  specific estimator or a Pipeline/ColumnTransformer */
  background-color: var(--sklearn-color-background);
}

/* Toggleable label */
#sk-container-id-2 label.sk-toggleable__label {
  cursor: pointer;
  display: block;
  width: 100%;
  margin-bottom: 0;
  padding: 0.5em;
  box-sizing: border-box;
  text-align: center;
}

#sk-container-id-2 label.sk-toggleable__label-arrow:before {
  /* Arrow on the left of the label */
  content: "▸";
  float: left;
  margin-right: 0.25em;
  color: var(--sklearn-color-icon);
}

#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {
  color: var(--sklearn-color-text);
}

/* Toggleable content - dropdown */

#sk-container-id-2 div.sk-toggleable__content {
  max-height: 0;
  max-width: 0;
  overflow: hidden;
  text-align: left;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-2 div.sk-toggleable__content.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-2 div.sk-toggleable__content pre {
  margin: 0.2em;
  border-radius: 0.25em;
  color: var(--sklearn-color-text);
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-2 div.sk-toggleable__content.fitted pre {
  /* unfitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {
  /* Expand drop-down */
  max-height: 200px;
  max-width: 100%;
  overflow: auto;
}

#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {
  content: "▾";
}

/* Pipeline/ColumnTransformer-specific style */

#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator-specific style */

/* Colorize estimator box */
#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

#sk-container-id-2 div.sk-label label.sk-toggleable__label,
#sk-container-id-2 div.sk-label label {
  /* The background is the default theme color */
  color: var(--sklearn-color-text-on-default-background);
}

/* On hover, darken the color of the background */
#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

/* Label box, darken color on hover, fitted */
#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator label */

#sk-container-id-2 div.sk-label label {
  font-family: monospace;
  font-weight: bold;
  display: inline-block;
  line-height: 1.2em;
}

#sk-container-id-2 div.sk-label-container {
  text-align: center;
}

/* Estimator-specific */
#sk-container-id-2 div.sk-estimator {
  font-family: monospace;
  border: 1px dotted var(--sklearn-color-border-box);
  border-radius: 0.25em;
  box-sizing: border-box;
  margin-bottom: 0.5em;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-2 div.sk-estimator.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

/* on hover */
#sk-container-id-2 div.sk-estimator:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-2 div.sk-estimator.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Specification for estimator info (e.g. "i" and "?") */

/* Common style for "i" and "?" */

.sk-estimator-doc-link,
a:link.sk-estimator-doc-link,
a:visited.sk-estimator-doc-link {
  float: right;
  font-size: smaller;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1em;
  height: 1em;
  width: 1em;
  text-decoration: none !important;
  margin-left: 1ex;
  /* unfitted */
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
  color: var(--sklearn-color-unfitted-level-1);
}

.sk-estimator-doc-link.fitted,
a:link.sk-estimator-doc-link.fitted,
a:visited.sk-estimator-doc-link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
div.sk-estimator:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover,
div.sk-label-container:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover,
div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

/* Span, style for the box shown on hovering the info icon */
.sk-estimator-doc-link span {
  display: none;
  z-index: 9999;
  position: relative;
  font-weight: normal;
  right: .2ex;
  padding: .5ex;
  margin: .5ex;
  width: min-content;
  min-width: 20ex;
  max-width: 50ex;
  color: var(--sklearn-color-text);
  box-shadow: 2pt 2pt 4pt #999;
  /* unfitted */
  background: var(--sklearn-color-unfitted-level-0);
  border: .5pt solid var(--sklearn-color-unfitted-level-3);
}

.sk-estimator-doc-link.fitted span {
  /* fitted */
  background: var(--sklearn-color-fitted-level-0);
  border: var(--sklearn-color-fitted-level-3);
}

.sk-estimator-doc-link:hover span {
  display: block;
}

/* "?"-specific style due to the `<a>` HTML tag */

#sk-container-id-2 a.estimator_doc_link {
  float: right;
  font-size: 1rem;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1rem;
  height: 1rem;
  width: 1rem;
  text-decoration: none;
  /* unfitted */
  color: var(--sklearn-color-unfitted-level-1);
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
}

#sk-container-id-2 a.estimator_doc_link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
#sk-container-id-2 a.estimator_doc_link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

#sk-container-id-2 a.estimator_doc_link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
}
</style><div id="sk-container-id-2" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>Lasso(alpha=0.05)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-2" type="checkbox" checked><label for="sk-estimator-id-2" class="sk-toggleable__label fitted sk-toggleable__label-arrow fitted">&nbsp;&nbsp;Lasso<a class="sk-estimator-doc-link fitted" rel="noreferrer" target="_blank" href="https://scikit-learn.org/1.4/modules/generated/sklearn.linear_model.Lasso.html">?<span>Documentation for Lasso</span></a><span class="sk-estimator-doc-link fitted">i<span>Fitted</span></span></label><div class="sk-toggleable__content fitted"><pre>Lasso(alpha=0.05)</pre></div> </div></div></div></div></div></div>
</div>
<p>We can output the model using this bit of code. Remember, model specifications are contained in the object created when we fit, or estimate, the model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># DataFrame with corresponding feature and its respective coefficients</span>
<span class="n">coeffs</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="p">[</span><span class="s1">&#39;intercept&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">columns</span><span class="p">),</span>
        <span class="nb">list</span><span class="p">(</span><span class="n">lasso</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">lasso</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
    <span class="p">]</span>
<span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">()</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">coeffs</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>1</th>
    </tr>
    <tr>
      <th>0</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>intercept</th>
      <td>-0.0</td>
    </tr>
    <tr>
      <th>LotArea</th>
      <td>0.044304</td>
    </tr>
    <tr>
      <th>OverallQual</th>
      <td>0.298079</td>
    </tr>
    <tr>
      <th>OverallCond</th>
      <td>0.0</td>
    </tr>
    <tr>
      <th>YearBuilt</th>
      <td>0.052091</td>
    </tr>
    <tr>
      <th>YearRemodAdd</th>
      <td>0.064471</td>
    </tr>
    <tr>
      <th>BsmtFinSF1</th>
      <td>0.115875</td>
    </tr>
    <tr>
      <th>BsmtUnfSF</th>
      <td>-0.0</td>
    </tr>
    <tr>
      <th>TotalBsmtSF</th>
      <td>0.10312</td>
    </tr>
    <tr>
      <th>1stFlrSF</th>
      <td>0.032295</td>
    </tr>
    <tr>
      <th>2ndFlrSF</th>
      <td>0.0</td>
    </tr>
    <tr>
      <th>GrLivArea</th>
      <td>0.297065</td>
    </tr>
    <tr>
      <th>FullBath</th>
      <td>0.0</td>
    </tr>
    <tr>
      <th>HalfBath</th>
      <td>0.0</td>
    </tr>
    <tr>
      <th>BedroomAbvGr</th>
      <td>-0.0</td>
    </tr>
    <tr>
      <th>TotRmsAbvGrd</th>
      <td>0.0</td>
    </tr>
    <tr>
      <th>Fireplaces</th>
      <td>0.020404</td>
    </tr>
    <tr>
      <th>GarageCars</th>
      <td>0.027512</td>
    </tr>
    <tr>
      <th>GarageArea</th>
      <td>0.06641</td>
    </tr>
    <tr>
      <th>WoodDeckSF</th>
      <td>0.001029</td>
    </tr>
    <tr>
      <th>OpenPorchSF</th>
      <td>0.00215</td>
    </tr>
    <tr>
      <th>EnclosedPorch</th>
      <td>-0.0</td>
    </tr>
    <tr>
      <th>Blmngtn</th>
      <td>-0.0</td>
    </tr>
    <tr>
      <th>Blueste</th>
      <td>-0.0</td>
    </tr>
    <tr>
      <th>BrDale</th>
      <td>-0.0</td>
    </tr>
    <tr>
      <th>BrkSide</th>
      <td>0.0</td>
    </tr>
    <tr>
      <th>ClearCr</th>
      <td>0.0</td>
    </tr>
    <tr>
      <th>CollgCr</th>
      <td>-0.0</td>
    </tr>
    <tr>
      <th>Crawfor</th>
      <td>0.0</td>
    </tr>
    <tr>
      <th>Edwards</th>
      <td>-0.0</td>
    </tr>
    <tr>
      <th>Gilbert</th>
      <td>0.0</td>
    </tr>
    <tr>
      <th>IDOTRR</th>
      <td>-0.0</td>
    </tr>
    <tr>
      <th>MeadowV</th>
      <td>-0.0</td>
    </tr>
    <tr>
      <th>Mitchel</th>
      <td>-0.0</td>
    </tr>
    <tr>
      <th>Names</th>
      <td>-0.0</td>
    </tr>
    <tr>
      <th>NoRidge</th>
      <td>0.013209</td>
    </tr>
    <tr>
      <th>NPkVill</th>
      <td>-0.0</td>
    </tr>
    <tr>
      <th>NriddgHt</th>
      <td>0.084299</td>
    </tr>
    <tr>
      <th>NWAmes</th>
      <td>-0.0</td>
    </tr>
    <tr>
      <th>OLDTown</th>
      <td>-0.0</td>
    </tr>
    <tr>
      <th>SWISU</th>
      <td>-0.0</td>
    </tr>
    <tr>
      <th>Sawyer</th>
      <td>-0.0</td>
    </tr>
    <tr>
      <th>SawyerW</th>
      <td>-0.0</td>
    </tr>
    <tr>
      <th>Somerst</th>
      <td>0.0</td>
    </tr>
    <tr>
      <th>StoneBr</th>
      <td>0.016815</td>
    </tr>
    <tr>
      <th>Timber</th>
      <td>0.0</td>
    </tr>
    <tr>
      <th>Veenker</th>
      <td>0.0</td>
    </tr>
    <tr>
      <th>Bsmt Qual</th>
      <td>0.020275</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Do you see all of the zero values? The lasso model penalizes features that are either unimportant or similar to other variables in the model. Notice how all of the negative weights are gone.</p>
<p>Let’s do the same thing we did above and vary our hyperparameter.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># We now consider different lambda values. The alphas are half the lambdas</span>
<span class="n">alphas</span><span class="o">=</span><span class="p">[</span><span class="mf">0.01</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="mf">0.02</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="mf">0.03</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="mf">0.04</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="mf">0.05</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="mf">0.075</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="mf">0.1</span><span class="o">/</span><span class="mi">2</span><span class="p">]</span>
<span class="n">mses</span><span class="o">=</span><span class="p">[]</span>
<span class="k">for</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="n">alphas</span><span class="p">:</span>
    <span class="n">lasso</span><span class="o">=</span><span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>
    <span class="n">lasso</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
    <span class="n">pred</span><span class="o">=</span><span class="n">lasso</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>
    <span class="n">mses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mse</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span><span class="n">pred</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">mse</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="n">pred</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.11654751909608796
0.116826879453111
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.11803348353132033
0.12012836764958999
0.12301536903084047
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.13178576395045638
0.1401719458448378
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">alphas</span><span class="p">,</span> <span class="n">mses</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&lt;matplotlib.lines.Line2D at 0x35d3a79d0&gt;]
</pre></div>
</div>
<img alt="../_images/12_regression_topics_71_1.png" src="../_images/12_regression_topics_71_1.png" />
</div>
</div>
</section>
<section id="elastic-net">
<h3><span class="section-number">12.6.11. </span>Elastic net<a class="headerlink" href="#elastic-net" title="Permalink to this headline">#</a></h3>
<p>The text comments that the elastic net model doesn’t really give much of an improvement over lasso. Let’s estimate it here, though.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">elastic</span> <span class="o">=</span> <span class="n">ElasticNet</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">l1_ratio</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">fit_intercept</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">elastic</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>#sk-container-id-3 {
  /* Definition of color scheme common for light and dark mode */
  --sklearn-color-text: black;
  --sklearn-color-line: gray;
  /* Definition of color scheme for unfitted estimators */
  --sklearn-color-unfitted-level-0: #fff5e6;
  --sklearn-color-unfitted-level-1: #f6e4d2;
  --sklearn-color-unfitted-level-2: #ffe0b3;
  --sklearn-color-unfitted-level-3: chocolate;
  /* Definition of color scheme for fitted estimators */
  --sklearn-color-fitted-level-0: #f0f8ff;
  --sklearn-color-fitted-level-1: #d4ebff;
  --sklearn-color-fitted-level-2: #b3dbfd;
  --sklearn-color-fitted-level-3: cornflowerblue;

  /* Specific color for light theme */
  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));
  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-icon: #696969;

  @media (prefers-color-scheme: dark) {
    /* Redefinition of color scheme for dark theme */
    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));
    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-icon: #878787;
  }
}

#sk-container-id-3 {
  color: var(--sklearn-color-text);
}

#sk-container-id-3 pre {
  padding: 0;
}

#sk-container-id-3 input.sk-hidden--visually {
  border: 0;
  clip: rect(1px 1px 1px 1px);
  clip: rect(1px, 1px, 1px, 1px);
  height: 1px;
  margin: -1px;
  overflow: hidden;
  padding: 0;
  position: absolute;
  width: 1px;
}

#sk-container-id-3 div.sk-dashed-wrapped {
  border: 1px dashed var(--sklearn-color-line);
  margin: 0 0.4em 0.5em 0.4em;
  box-sizing: border-box;
  padding-bottom: 0.4em;
  background-color: var(--sklearn-color-background);
}

#sk-container-id-3 div.sk-container {
  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`
     but bootstrap.min.css set `[hidden] { display: none !important; }`
     so we also need the `!important` here to be able to override the
     default hidden behavior on the sphinx rendered scikit-learn.org.
     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */
  display: inline-block !important;
  position: relative;
}

#sk-container-id-3 div.sk-text-repr-fallback {
  display: none;
}

div.sk-parallel-item,
div.sk-serial,
div.sk-item {
  /* draw centered vertical line to link estimators */
  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));
  background-size: 2px 100%;
  background-repeat: no-repeat;
  background-position: center center;
}

/* Parallel-specific style estimator block */

#sk-container-id-3 div.sk-parallel-item::after {
  content: "";
  width: 100%;
  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);
  flex-grow: 1;
}

#sk-container-id-3 div.sk-parallel {
  display: flex;
  align-items: stretch;
  justify-content: center;
  background-color: var(--sklearn-color-background);
  position: relative;
}

#sk-container-id-3 div.sk-parallel-item {
  display: flex;
  flex-direction: column;
}

#sk-container-id-3 div.sk-parallel-item:first-child::after {
  align-self: flex-end;
  width: 50%;
}

#sk-container-id-3 div.sk-parallel-item:last-child::after {
  align-self: flex-start;
  width: 50%;
}

#sk-container-id-3 div.sk-parallel-item:only-child::after {
  width: 0;
}

/* Serial-specific style estimator block */

#sk-container-id-3 div.sk-serial {
  display: flex;
  flex-direction: column;
  align-items: center;
  background-color: var(--sklearn-color-background);
  padding-right: 1em;
  padding-left: 1em;
}


/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is
clickable and can be expanded/collapsed.
- Pipeline and ColumnTransformer use this feature and define the default style
- Estimators will overwrite some part of the style using the `sk-estimator` class
*/

/* Pipeline and ColumnTransformer style (default) */

#sk-container-id-3 div.sk-toggleable {
  /* Default theme specific background. It is overwritten whether we have a
  specific estimator or a Pipeline/ColumnTransformer */
  background-color: var(--sklearn-color-background);
}

/* Toggleable label */
#sk-container-id-3 label.sk-toggleable__label {
  cursor: pointer;
  display: block;
  width: 100%;
  margin-bottom: 0;
  padding: 0.5em;
  box-sizing: border-box;
  text-align: center;
}

#sk-container-id-3 label.sk-toggleable__label-arrow:before {
  /* Arrow on the left of the label */
  content: "▸";
  float: left;
  margin-right: 0.25em;
  color: var(--sklearn-color-icon);
}

#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {
  color: var(--sklearn-color-text);
}

/* Toggleable content - dropdown */

#sk-container-id-3 div.sk-toggleable__content {
  max-height: 0;
  max-width: 0;
  overflow: hidden;
  text-align: left;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-3 div.sk-toggleable__content.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-3 div.sk-toggleable__content pre {
  margin: 0.2em;
  border-radius: 0.25em;
  color: var(--sklearn-color-text);
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-3 div.sk-toggleable__content.fitted pre {
  /* unfitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {
  /* Expand drop-down */
  max-height: 200px;
  max-width: 100%;
  overflow: auto;
}

#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {
  content: "▾";
}

/* Pipeline/ColumnTransformer-specific style */

#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-3 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator-specific style */

/* Colorize estimator box */
#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-3 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

#sk-container-id-3 div.sk-label label.sk-toggleable__label,
#sk-container-id-3 div.sk-label label {
  /* The background is the default theme color */
  color: var(--sklearn-color-text-on-default-background);
}

/* On hover, darken the color of the background */
#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

/* Label box, darken color on hover, fitted */
#sk-container-id-3 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator label */

#sk-container-id-3 div.sk-label label {
  font-family: monospace;
  font-weight: bold;
  display: inline-block;
  line-height: 1.2em;
}

#sk-container-id-3 div.sk-label-container {
  text-align: center;
}

/* Estimator-specific */
#sk-container-id-3 div.sk-estimator {
  font-family: monospace;
  border: 1px dotted var(--sklearn-color-border-box);
  border-radius: 0.25em;
  box-sizing: border-box;
  margin-bottom: 0.5em;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-3 div.sk-estimator.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

/* on hover */
#sk-container-id-3 div.sk-estimator:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-3 div.sk-estimator.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Specification for estimator info (e.g. "i" and "?") */

/* Common style for "i" and "?" */

.sk-estimator-doc-link,
a:link.sk-estimator-doc-link,
a:visited.sk-estimator-doc-link {
  float: right;
  font-size: smaller;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1em;
  height: 1em;
  width: 1em;
  text-decoration: none !important;
  margin-left: 1ex;
  /* unfitted */
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
  color: var(--sklearn-color-unfitted-level-1);
}

.sk-estimator-doc-link.fitted,
a:link.sk-estimator-doc-link.fitted,
a:visited.sk-estimator-doc-link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
div.sk-estimator:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover,
div.sk-label-container:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover,
div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

/* Span, style for the box shown on hovering the info icon */
.sk-estimator-doc-link span {
  display: none;
  z-index: 9999;
  position: relative;
  font-weight: normal;
  right: .2ex;
  padding: .5ex;
  margin: .5ex;
  width: min-content;
  min-width: 20ex;
  max-width: 50ex;
  color: var(--sklearn-color-text);
  box-shadow: 2pt 2pt 4pt #999;
  /* unfitted */
  background: var(--sklearn-color-unfitted-level-0);
  border: .5pt solid var(--sklearn-color-unfitted-level-3);
}

.sk-estimator-doc-link.fitted span {
  /* fitted */
  background: var(--sklearn-color-fitted-level-0);
  border: var(--sklearn-color-fitted-level-3);
}

.sk-estimator-doc-link:hover span {
  display: block;
}

/* "?"-specific style due to the `<a>` HTML tag */

#sk-container-id-3 a.estimator_doc_link {
  float: right;
  font-size: 1rem;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1rem;
  height: 1rem;
  width: 1rem;
  text-decoration: none;
  /* unfitted */
  color: var(--sklearn-color-unfitted-level-1);
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
}

#sk-container-id-3 a.estimator_doc_link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
#sk-container-id-3 a.estimator_doc_link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

#sk-container-id-3 a.estimator_doc_link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
}
</style><div id="sk-container-id-3" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>ElasticNet(alpha=0.05, l1_ratio=0.8)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-3" type="checkbox" checked><label for="sk-estimator-id-3" class="sk-toggleable__label fitted sk-toggleable__label-arrow fitted">&nbsp;&nbsp;ElasticNet<a class="sk-estimator-doc-link fitted" rel="noreferrer" target="_blank" href="https://scikit-learn.org/1.4/modules/generated/sklearn.linear_model.ElasticNet.html">?<span>Documentation for ElasticNet</span></a><span class="sk-estimator-doc-link fitted">i<span>Fitted</span></span></label><div class="sk-toggleable__content fitted"><pre>ElasticNet(alpha=0.05, l1_ratio=0.8)</pre></div> </div></div></div></div></div></div>
</div>
<p>We again create our model object and then fit it with data.</p>
<p>What do the <code class="docutils literal notranslate"><span class="pre">alpha</span></code> and <code class="docutils literal notranslate"><span class="pre">l1_ratio</span></code> parameters do? From Claude:</p>
<p>In the scikit-learn implementation of Elastic Net regression (ElasticNet class), the alpha and l1_ratio parameters control the strength and balance between the L1 (Lasso) and L2 (Ridge) regularization penalties, respectively.</p>
<ul class="simple">
<li><p>alpha (float): This parameter controls the overall strength of the regularization. A higher value of alpha increases the regularization effect, shrinking the coefficients more towards zero. When alpha=0, there is no regularization, and the model becomes a ordinary least squares linear regression.</p></li>
<li><p>l1_ratio (float between 0 and 1): This parameter determines the balance between the L1 and L2 penalties in the Elastic Net regularization. Specifically: l1_ratio=1 means the penalty is an L1 penalty (Lasso). l1_ratio=0 means the penalty is an L2 penalty (Ridge). 0 &lt; l1_ratio &lt; 1 means the penalty is a combination of L1 and L2 penalties (Elastic Net).</p></li>
</ul>
<p>So, alpha controls the overall regularization strength, while l1_ratio determines how much of that regularization comes from the L1 penalty vs the L2 penalty. In practice, you would tune both alpha and l1_ratio using cross-validation to find the best combination for your specific data. Common values for l1_ratio are 0.5 (balanced Elastic Net), 0.2, or 0.8 depending on whether you want more L2 or L1 regularization respectively.</p>
<p>Let’s look.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># DataFrame with corresponding feature and its respective coefficients</span>
<span class="n">coeffs</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="p">[</span><span class="s1">&#39;intercept&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">columns</span><span class="p">),</span>
        <span class="nb">list</span><span class="p">(</span><span class="n">elastic</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">elastic</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
    <span class="p">]</span>
<span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">()</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">coeffs</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>1</th>
    </tr>
    <tr>
      <th>0</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>intercept</th>
      <td>-0.0</td>
    </tr>
    <tr>
      <th>LotArea</th>
      <td>0.051311</td>
    </tr>
    <tr>
      <th>OverallQual</th>
      <td>0.283431</td>
    </tr>
    <tr>
      <th>OverallCond</th>
      <td>0.005266</td>
    </tr>
    <tr>
      <th>YearBuilt</th>
      <td>0.059446</td>
    </tr>
    <tr>
      <th>YearRemodAdd</th>
      <td>0.067086</td>
    </tr>
    <tr>
      <th>BsmtFinSF1</th>
      <td>0.11673</td>
    </tr>
    <tr>
      <th>BsmtUnfSF</th>
      <td>-0.0</td>
    </tr>
    <tr>
      <th>TotalBsmtSF</th>
      <td>0.096558</td>
    </tr>
    <tr>
      <th>1stFlrSF</th>
      <td>0.038404</td>
    </tr>
    <tr>
      <th>2ndFlrSF</th>
      <td>0.0</td>
    </tr>
    <tr>
      <th>GrLivArea</th>
      <td>0.29222</td>
    </tr>
    <tr>
      <th>FullBath</th>
      <td>0.0</td>
    </tr>
    <tr>
      <th>HalfBath</th>
      <td>0.0</td>
    </tr>
    <tr>
      <th>BedroomAbvGr</th>
      <td>-0.0</td>
    </tr>
    <tr>
      <th>TotRmsAbvGrd</th>
      <td>0.0</td>
    </tr>
    <tr>
      <th>Fireplaces</th>
      <td>0.025387</td>
    </tr>
    <tr>
      <th>GarageCars</th>
      <td>0.030232</td>
    </tr>
    <tr>
      <th>GarageArea</th>
      <td>0.066393</td>
    </tr>
    <tr>
      <th>WoodDeckSF</th>
      <td>0.005885</td>
    </tr>
    <tr>
      <th>OpenPorchSF</th>
      <td>0.011107</td>
    </tr>
    <tr>
      <th>EnclosedPorch</th>
      <td>-0.0</td>
    </tr>
    <tr>
      <th>Blmngtn</th>
      <td>-0.0</td>
    </tr>
    <tr>
      <th>Blueste</th>
      <td>-0.0</td>
    </tr>
    <tr>
      <th>BrDale</th>
      <td>-0.0</td>
    </tr>
    <tr>
      <th>BrkSide</th>
      <td>0.0</td>
    </tr>
    <tr>
      <th>ClearCr</th>
      <td>0.0</td>
    </tr>
    <tr>
      <th>CollgCr</th>
      <td>-0.0</td>
    </tr>
    <tr>
      <th>Crawfor</th>
      <td>0.00707</td>
    </tr>
    <tr>
      <th>Edwards</th>
      <td>0.0</td>
    </tr>
    <tr>
      <th>Gilbert</th>
      <td>0.0</td>
    </tr>
    <tr>
      <th>IDOTRR</th>
      <td>-0.0</td>
    </tr>
    <tr>
      <th>MeadowV</th>
      <td>-0.0</td>
    </tr>
    <tr>
      <th>Mitchel</th>
      <td>-0.0</td>
    </tr>
    <tr>
      <th>Names</th>
      <td>-0.0</td>
    </tr>
    <tr>
      <th>NoRidge</th>
      <td>0.023858</td>
    </tr>
    <tr>
      <th>NPkVill</th>
      <td>-0.0</td>
    </tr>
    <tr>
      <th>NriddgHt</th>
      <td>0.094224</td>
    </tr>
    <tr>
      <th>NWAmes</th>
      <td>-0.0</td>
    </tr>
    <tr>
      <th>OLDTown</th>
      <td>-0.0</td>
    </tr>
    <tr>
      <th>SWISU</th>
      <td>-0.0</td>
    </tr>
    <tr>
      <th>Sawyer</th>
      <td>-0.0</td>
    </tr>
    <tr>
      <th>SawyerW</th>
      <td>-0.0</td>
    </tr>
    <tr>
      <th>Somerst</th>
      <td>0.0</td>
    </tr>
    <tr>
      <th>StoneBr</th>
      <td>0.028336</td>
    </tr>
    <tr>
      <th>Timber</th>
      <td>0.0</td>
    </tr>
    <tr>
      <th>Veenker</th>
      <td>0.0</td>
    </tr>
    <tr>
      <th>Bsmt Qual</th>
      <td>0.025081</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Watch what happens when I set <code class="docutils literal notranslate"><span class="pre">l1_ratio</span></code> = 1. I get the lasso model!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">elastic</span> <span class="o">=</span> <span class="n">ElasticNet</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">l1_ratio</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">fit_intercept</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">elastic</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>#sk-container-id-4 {
  /* Definition of color scheme common for light and dark mode */
  --sklearn-color-text: black;
  --sklearn-color-line: gray;
  /* Definition of color scheme for unfitted estimators */
  --sklearn-color-unfitted-level-0: #fff5e6;
  --sklearn-color-unfitted-level-1: #f6e4d2;
  --sklearn-color-unfitted-level-2: #ffe0b3;
  --sklearn-color-unfitted-level-3: chocolate;
  /* Definition of color scheme for fitted estimators */
  --sklearn-color-fitted-level-0: #f0f8ff;
  --sklearn-color-fitted-level-1: #d4ebff;
  --sklearn-color-fitted-level-2: #b3dbfd;
  --sklearn-color-fitted-level-3: cornflowerblue;

  /* Specific color for light theme */
  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));
  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-icon: #696969;

  @media (prefers-color-scheme: dark) {
    /* Redefinition of color scheme for dark theme */
    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));
    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-icon: #878787;
  }
}

#sk-container-id-4 {
  color: var(--sklearn-color-text);
}

#sk-container-id-4 pre {
  padding: 0;
}

#sk-container-id-4 input.sk-hidden--visually {
  border: 0;
  clip: rect(1px 1px 1px 1px);
  clip: rect(1px, 1px, 1px, 1px);
  height: 1px;
  margin: -1px;
  overflow: hidden;
  padding: 0;
  position: absolute;
  width: 1px;
}

#sk-container-id-4 div.sk-dashed-wrapped {
  border: 1px dashed var(--sklearn-color-line);
  margin: 0 0.4em 0.5em 0.4em;
  box-sizing: border-box;
  padding-bottom: 0.4em;
  background-color: var(--sklearn-color-background);
}

#sk-container-id-4 div.sk-container {
  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`
     but bootstrap.min.css set `[hidden] { display: none !important; }`
     so we also need the `!important` here to be able to override the
     default hidden behavior on the sphinx rendered scikit-learn.org.
     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */
  display: inline-block !important;
  position: relative;
}

#sk-container-id-4 div.sk-text-repr-fallback {
  display: none;
}

div.sk-parallel-item,
div.sk-serial,
div.sk-item {
  /* draw centered vertical line to link estimators */
  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));
  background-size: 2px 100%;
  background-repeat: no-repeat;
  background-position: center center;
}

/* Parallel-specific style estimator block */

#sk-container-id-4 div.sk-parallel-item::after {
  content: "";
  width: 100%;
  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);
  flex-grow: 1;
}

#sk-container-id-4 div.sk-parallel {
  display: flex;
  align-items: stretch;
  justify-content: center;
  background-color: var(--sklearn-color-background);
  position: relative;
}

#sk-container-id-4 div.sk-parallel-item {
  display: flex;
  flex-direction: column;
}

#sk-container-id-4 div.sk-parallel-item:first-child::after {
  align-self: flex-end;
  width: 50%;
}

#sk-container-id-4 div.sk-parallel-item:last-child::after {
  align-self: flex-start;
  width: 50%;
}

#sk-container-id-4 div.sk-parallel-item:only-child::after {
  width: 0;
}

/* Serial-specific style estimator block */

#sk-container-id-4 div.sk-serial {
  display: flex;
  flex-direction: column;
  align-items: center;
  background-color: var(--sklearn-color-background);
  padding-right: 1em;
  padding-left: 1em;
}


/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is
clickable and can be expanded/collapsed.
- Pipeline and ColumnTransformer use this feature and define the default style
- Estimators will overwrite some part of the style using the `sk-estimator` class
*/

/* Pipeline and ColumnTransformer style (default) */

#sk-container-id-4 div.sk-toggleable {
  /* Default theme specific background. It is overwritten whether we have a
  specific estimator or a Pipeline/ColumnTransformer */
  background-color: var(--sklearn-color-background);
}

/* Toggleable label */
#sk-container-id-4 label.sk-toggleable__label {
  cursor: pointer;
  display: block;
  width: 100%;
  margin-bottom: 0;
  padding: 0.5em;
  box-sizing: border-box;
  text-align: center;
}

#sk-container-id-4 label.sk-toggleable__label-arrow:before {
  /* Arrow on the left of the label */
  content: "▸";
  float: left;
  margin-right: 0.25em;
  color: var(--sklearn-color-icon);
}

#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {
  color: var(--sklearn-color-text);
}

/* Toggleable content - dropdown */

#sk-container-id-4 div.sk-toggleable__content {
  max-height: 0;
  max-width: 0;
  overflow: hidden;
  text-align: left;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-4 div.sk-toggleable__content.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-4 div.sk-toggleable__content pre {
  margin: 0.2em;
  border-radius: 0.25em;
  color: var(--sklearn-color-text);
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-4 div.sk-toggleable__content.fitted pre {
  /* unfitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {
  /* Expand drop-down */
  max-height: 200px;
  max-width: 100%;
  overflow: auto;
}

#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {
  content: "▾";
}

/* Pipeline/ColumnTransformer-specific style */

#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-4 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator-specific style */

/* Colorize estimator box */
#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-4 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

#sk-container-id-4 div.sk-label label.sk-toggleable__label,
#sk-container-id-4 div.sk-label label {
  /* The background is the default theme color */
  color: var(--sklearn-color-text-on-default-background);
}

/* On hover, darken the color of the background */
#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

/* Label box, darken color on hover, fitted */
#sk-container-id-4 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator label */

#sk-container-id-4 div.sk-label label {
  font-family: monospace;
  font-weight: bold;
  display: inline-block;
  line-height: 1.2em;
}

#sk-container-id-4 div.sk-label-container {
  text-align: center;
}

/* Estimator-specific */
#sk-container-id-4 div.sk-estimator {
  font-family: monospace;
  border: 1px dotted var(--sklearn-color-border-box);
  border-radius: 0.25em;
  box-sizing: border-box;
  margin-bottom: 0.5em;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-4 div.sk-estimator.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

/* on hover */
#sk-container-id-4 div.sk-estimator:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-4 div.sk-estimator.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Specification for estimator info (e.g. "i" and "?") */

/* Common style for "i" and "?" */

.sk-estimator-doc-link,
a:link.sk-estimator-doc-link,
a:visited.sk-estimator-doc-link {
  float: right;
  font-size: smaller;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1em;
  height: 1em;
  width: 1em;
  text-decoration: none !important;
  margin-left: 1ex;
  /* unfitted */
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
  color: var(--sklearn-color-unfitted-level-1);
}

.sk-estimator-doc-link.fitted,
a:link.sk-estimator-doc-link.fitted,
a:visited.sk-estimator-doc-link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
div.sk-estimator:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover,
div.sk-label-container:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover,
div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

/* Span, style for the box shown on hovering the info icon */
.sk-estimator-doc-link span {
  display: none;
  z-index: 9999;
  position: relative;
  font-weight: normal;
  right: .2ex;
  padding: .5ex;
  margin: .5ex;
  width: min-content;
  min-width: 20ex;
  max-width: 50ex;
  color: var(--sklearn-color-text);
  box-shadow: 2pt 2pt 4pt #999;
  /* unfitted */
  background: var(--sklearn-color-unfitted-level-0);
  border: .5pt solid var(--sklearn-color-unfitted-level-3);
}

.sk-estimator-doc-link.fitted span {
  /* fitted */
  background: var(--sklearn-color-fitted-level-0);
  border: var(--sklearn-color-fitted-level-3);
}

.sk-estimator-doc-link:hover span {
  display: block;
}

/* "?"-specific style due to the `<a>` HTML tag */

#sk-container-id-4 a.estimator_doc_link {
  float: right;
  font-size: 1rem;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1rem;
  height: 1rem;
  width: 1rem;
  text-decoration: none;
  /* unfitted */
  color: var(--sklearn-color-unfitted-level-1);
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
}

#sk-container-id-4 a.estimator_doc_link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
#sk-container-id-4 a.estimator_doc_link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

#sk-container-id-4 a.estimator_doc_link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
}
</style><div id="sk-container-id-4" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>ElasticNet(alpha=0.05, l1_ratio=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-4" type="checkbox" checked><label for="sk-estimator-id-4" class="sk-toggleable__label fitted sk-toggleable__label-arrow fitted">&nbsp;&nbsp;ElasticNet<a class="sk-estimator-doc-link fitted" rel="noreferrer" target="_blank" href="https://scikit-learn.org/1.4/modules/generated/sklearn.linear_model.ElasticNet.html">?<span>Documentation for ElasticNet</span></a><span class="sk-estimator-doc-link fitted">i<span>Fitted</span></span></label><div class="sk-toggleable__content fitted"><pre>ElasticNet(alpha=0.05, l1_ratio=1)</pre></div> </div></div></div></div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">coeffs</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="p">[</span><span class="s1">&#39;intercept&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">columns</span><span class="p">),</span>
        <span class="nb">list</span><span class="p">(</span><span class="n">elastic</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">elastic</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
    <span class="p">]</span>
<span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">()</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">coeffs</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>1</th>
    </tr>
    <tr>
      <th>0</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>intercept</th>
      <td>-0.0</td>
    </tr>
    <tr>
      <th>LotArea</th>
      <td>0.044304</td>
    </tr>
    <tr>
      <th>OverallQual</th>
      <td>0.298079</td>
    </tr>
    <tr>
      <th>OverallCond</th>
      <td>0.0</td>
    </tr>
    <tr>
      <th>YearBuilt</th>
      <td>0.052091</td>
    </tr>
    <tr>
      <th>YearRemodAdd</th>
      <td>0.064471</td>
    </tr>
    <tr>
      <th>BsmtFinSF1</th>
      <td>0.115875</td>
    </tr>
    <tr>
      <th>BsmtUnfSF</th>
      <td>-0.0</td>
    </tr>
    <tr>
      <th>TotalBsmtSF</th>
      <td>0.10312</td>
    </tr>
    <tr>
      <th>1stFlrSF</th>
      <td>0.032295</td>
    </tr>
    <tr>
      <th>2ndFlrSF</th>
      <td>0.0</td>
    </tr>
    <tr>
      <th>GrLivArea</th>
      <td>0.297065</td>
    </tr>
    <tr>
      <th>FullBath</th>
      <td>0.0</td>
    </tr>
    <tr>
      <th>HalfBath</th>
      <td>0.0</td>
    </tr>
    <tr>
      <th>BedroomAbvGr</th>
      <td>-0.0</td>
    </tr>
    <tr>
      <th>TotRmsAbvGrd</th>
      <td>0.0</td>
    </tr>
    <tr>
      <th>Fireplaces</th>
      <td>0.020404</td>
    </tr>
    <tr>
      <th>GarageCars</th>
      <td>0.027512</td>
    </tr>
    <tr>
      <th>GarageArea</th>
      <td>0.06641</td>
    </tr>
    <tr>
      <th>WoodDeckSF</th>
      <td>0.001029</td>
    </tr>
    <tr>
      <th>OpenPorchSF</th>
      <td>0.00215</td>
    </tr>
    <tr>
      <th>EnclosedPorch</th>
      <td>-0.0</td>
    </tr>
    <tr>
      <th>Blmngtn</th>
      <td>-0.0</td>
    </tr>
    <tr>
      <th>Blueste</th>
      <td>-0.0</td>
    </tr>
    <tr>
      <th>BrDale</th>
      <td>-0.0</td>
    </tr>
    <tr>
      <th>BrkSide</th>
      <td>0.0</td>
    </tr>
    <tr>
      <th>ClearCr</th>
      <td>0.0</td>
    </tr>
    <tr>
      <th>CollgCr</th>
      <td>-0.0</td>
    </tr>
    <tr>
      <th>Crawfor</th>
      <td>0.0</td>
    </tr>
    <tr>
      <th>Edwards</th>
      <td>-0.0</td>
    </tr>
    <tr>
      <th>Gilbert</th>
      <td>0.0</td>
    </tr>
    <tr>
      <th>IDOTRR</th>
      <td>-0.0</td>
    </tr>
    <tr>
      <th>MeadowV</th>
      <td>-0.0</td>
    </tr>
    <tr>
      <th>Mitchel</th>
      <td>-0.0</td>
    </tr>
    <tr>
      <th>Names</th>
      <td>-0.0</td>
    </tr>
    <tr>
      <th>NoRidge</th>
      <td>0.013209</td>
    </tr>
    <tr>
      <th>NPkVill</th>
      <td>-0.0</td>
    </tr>
    <tr>
      <th>NriddgHt</th>
      <td>0.084299</td>
    </tr>
    <tr>
      <th>NWAmes</th>
      <td>-0.0</td>
    </tr>
    <tr>
      <th>OLDTown</th>
      <td>-0.0</td>
    </tr>
    <tr>
      <th>SWISU</th>
      <td>-0.0</td>
    </tr>
    <tr>
      <th>Sawyer</th>
      <td>-0.0</td>
    </tr>
    <tr>
      <th>SawyerW</th>
      <td>-0.0</td>
    </tr>
    <tr>
      <th>Somerst</th>
      <td>0.0</td>
    </tr>
    <tr>
      <th>StoneBr</th>
      <td>0.016815</td>
    </tr>
    <tr>
      <th>Timber</th>
      <td>0.0</td>
    </tr>
    <tr>
      <th>Veenker</th>
      <td>0.0</td>
    </tr>
    <tr>
      <th>Bsmt Qual</th>
      <td>0.020275</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
</section>
<section id="prediction-workflow-with-sklearn">
<h2><span class="section-number">12.7. </span>Prediction workflow with sklearn<a class="headerlink" href="#prediction-workflow-with-sklearn" title="Permalink to this headline">#</a></h2>
<p>The Hull textbook often does things “by hand”. For example, he splits the data into training, validation, and test samples by using <code class="docutils literal notranslate"><span class="pre">.iloc</span></code> and manually slicing the data frame. This is problem if the data aren’t sorted randomly! He standardizes the variables by using he z-score formula. He searches for optimal hyperparameters by making a graph.</p>
<p>It turns out the <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> can do a lot of this for us. In fact, <strong>if you use tools like Copilot or ChatGPT</strong> you’re likely to get code that uses the <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> workflow. So, we should understand how to use these tools. The fact that ChatGPT wants to use them means that they are extremely common.</p>
<p>This chapter starts off with a discussion of the general workflow, so I’m assuming that you’ve already done your basic data cleaning.</p>
<section id="standardizing-your-data">
<h3><span class="section-number">12.7.1. </span>Standardizing your data<a class="headerlink" href="#standardizing-your-data" title="Permalink to this headline">#</a></h3>
<p>Let’s use <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> and our Zillow data from above to <strong>standardize our data</strong>. From Claude:</p>
<p>In scikit-learn (sklearn), you can standardize your data using the StandardScaler class from the sklearn.preprocessing module. Standardization is a common preprocessing step in machine learning that scales the features to have a mean of 0 and a standard deviation of 1. This can be beneficial for algorithms that are sensitive to feature scales.</p>
<ul class="simple">
<li><p>scaler = StandardScaler(): Create an instance of the StandardScaler class.</p></li>
<li><p>X_scaled = scaler.fit_transform(X): Fit the scaler to the data and transform the data in one step. The fit_transform method first computes the mean and standard deviation of each feature based on the data in X, and then scales the features by subtracting the mean and dividing by the standard deviation.</p></li>
</ul>
<p>The fit_transform method returns the scaled feature matrix X_scaled, where each feature now has a mean of 0 and a standard deviation of 1.</p>
<p>I am going to keep a few variables from the Zillow data, including our target <em>logerror</em>.</p>
<p><code class="docutils literal notranslate"><span class="pre">scaler.fit_transform</span></code> creates an <strong>array</strong>. I convert this array back to a <strong>data frame</strong> and give the data frame the same column names as before.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">zillow_sample</span> <span class="o">=</span> <span class="n">zillow_data</span><span class="p">[[</span><span class="s1">&#39;logerror&#39;</span><span class="p">,</span> <span class="s1">&#39;calculatedfinishedsquarefeet&#39;</span><span class="p">,</span> <span class="s1">&#39;lotsizesquarefeet&#39;</span><span class="p">,</span> <span class="s1">&#39;yearbuilt&#39;</span><span class="p">]]</span>

<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">zillow_sample_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">zillow_sample</span><span class="p">)</span>

<span class="n">zillow_sample_scaled</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">zillow_sample_scaled</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">zillow_sample</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>

<span class="n">zillow_sample_scaled</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>logerror</th>
      <th>calculatedfinishedsquarefeet</th>
      <th>lotsizesquarefeet</th>
      <th>yearbuilt</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.539906</td>
      <td>0.033474</td>
      <td>-0.132697</td>
      <td>-1.379712</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-0.054958</td>
      <td>0.581927</td>
      <td>-0.132939</td>
      <td>0.495126</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-0.425011</td>
      <td>-0.578095</td>
      <td>-0.144900</td>
      <td>-0.484903</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.075761</td>
      <td>-0.385483</td>
      <td>-0.130379</td>
      <td>-0.655343</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.082076</td>
      <td>1.208730</td>
      <td>-0.112629</td>
      <td>1.475155</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>9066</th>
      <td>-0.260192</td>
      <td>0.759303</td>
      <td>-0.145322</td>
      <td>-1.592762</td>
    </tr>
    <tr>
      <th>9067</th>
      <td>0.100389</td>
      <td>-0.044877</td>
      <td>0.853970</td>
      <td>0.537736</td>
    </tr>
    <tr>
      <th>9068</th>
      <td>-0.010753</td>
      <td>-0.407247</td>
      <td>-0.138305</td>
      <td>-0.229244</td>
    </tr>
    <tr>
      <th>9069</th>
      <td>0.246263</td>
      <td>0.217380</td>
      <td>-0.073346</td>
      <td>-0.740563</td>
    </tr>
    <tr>
      <th>9070</th>
      <td>-0.539310</td>
      <td>1.633128</td>
      <td>-0.156965</td>
      <td>0.921225</td>
    </tr>
  </tbody>
</table>
<p>9071 rows × 4 columns</p>
</div></div></div>
</div>
</section>
<section id="splitting-your-data">
<h3><span class="section-number">12.7.2. </span>Splitting your data<a class="headerlink" href="#splitting-your-data" title="Permalink to this headline">#</a></h3>
<p>Let’s again use <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> and our Zillow data from above to <strong>split the data into training and testing samples</strong>. From Claude:</p>
<p>train_test_split is a function in the sklearn.model_selection module of the scikit-learn library in Python. It is used to split a dataset into two subsets: a training set and a test set. The training set is used to train a machine learning model, while the test set is used to evaluate its performance on unseen data.</p>
<p>The train_test_split function takes the following arguments:</p>
<ul class="simple">
<li><p>X: The feature matrix or data frame containing the input variables.</p></li>
<li><p>y: The target vector or data frame containing the output variable(s).</p></li>
<li><p>test_size (optional): The proportion of the dataset to include in the test split. It should be a float between 0.0 and 1.0. The default value is 0.25, which means 25% of the data will be used for testing.</p></li>
<li><p>random_state (optional): An integer value used to seed the random number generator, which ensures reproducibility of the split.</p></li>
</ul>
<p>The function returns four objects:</p>
<ul class="simple">
<li><p>X_train: The training data for the input features.</p></li>
<li><p>X_test: The testing data for the input features.</p></li>
<li><p>y_train: The training data for the target variable(s).</p></li>
<li><p>y_test: The testing data for the target variable(s).</p></li>
</ul>
<p>Notice that it doesn’t do the train, validation, and test split from the Hull book. We can do training and validation within the training data set. I’ll take my data frame <em>zillow_data</em> and split into target and features. Remember, <em>logerror</em> was the variable that we were trying to predict.</p>
<p>I added <em>z</em> to all of the names, just so I don’t overwrite my data frames from above.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">yz</span> <span class="o">=</span> <span class="n">zillow_data</span><span class="p">[</span><span class="s1">&#39;logerror&#39;</span><span class="p">]</span>
<span class="n">Xz</span> <span class="o">=</span> <span class="n">zillow_data</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;logerror&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">Xz_train</span><span class="p">,</span> <span class="n">Xz_test</span><span class="p">,</span> <span class="n">yz_train</span><span class="p">,</span> <span class="n">yz_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">Xz</span><span class="p">,</span> <span class="n">yz</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>That will create four new data frames for you. I would make sure that you’ve cleaned and standardized everything before you split your data.</p>
<p>AI always chooses 42 to seed its random number generated. The answer to everything.</p>
</section>
<section id="cross-validation-and-hyperparamters">
<h3><span class="section-number">12.7.3. </span>Cross-validation and hyperparamters<a class="headerlink" href="#cross-validation-and-hyperparamters" title="Permalink to this headline">#</a></h3>
<p>Let’s discuss an idea called <a class="reference external" href="https://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_model_selection.html">cross validation</a>. We’ve already see this a bit when we split our data into <strong>training</strong> and <strong>validation</strong> samples. From Claude:</p>
<blockquote>
<div><p>Cross-validation is a technique used in machine learning to evaluate the performance of a model and tune its hyperparameters. It helps to estimate how well the model will generalize to unseen data, and it is particularly useful when the available data is limited. The main idea behind cross-validation is to split the dataset into two parts: a training set and a validation set. The model is trained on the training set and evaluated on the validation set. This process is repeated multiple times, with different portions of the data being used for training and validation in each iteration.</p>
</div></blockquote>
<blockquote>
<div><p>There are several types of cross-validation techniques, but the most common is <strong>k-fold cross-validation</strong>. Here’s how it works:</p>
</div></blockquote>
<ol class="simple">
<li><p>The dataset is randomly partitioned into k equal-sized subsets or “folds”.</p></li>
<li><p>One of the k folds is held out as the validation set, and the remaining k-1 folds are used for training the model. The model is trained on the k-1 folds and evaluated on the held-out validation fold.</p></li>
<li><p>This process is repeated k times, with each of the k folds being used as the validation set exactly once.</p></li>
<li><p>The performance metric (e.g., accuracy, F1-score, mean squared error) is calculated for each iteration, and the final performance is the average of the k iterations.</p></li>
<li><p>Cross-validation has several advantages:</p></li>
</ol>
<ul class="simple">
<li><p>It provides a more reliable and unbiased estimate of the model’s performance compared to a single train-test split.</p></li>
<li><p>It helps to prevent overfitting by evaluating the model on data that it has not seen during training.</p></li>
<li><p>It allows for effective hyperparameter tuning by selecting the hyperparameters that yield the best average performance across the cross-validation folds.</p></li>
<li><p>It makes efficient use of the available data by using every data point for both training and validation at some point.</p></li>
<li><p>Cross-validation is widely used in machine learning competitions, research, and real-world applications. It is a crucial technique for evaluating and tuning models, especially when dealing with limited data or when it is important to estimate the model’s generalization performance accurately.</p></li>
</ul>
<p>We are going to use some <strong>built-in cross-validation tools</strong>. This means that we don’t have to manually split our data set.</p>
<p>I’m going to go back the the Hull data and <strong>combine the training and validation data sets</strong>. I’m going to use all of the observations and have <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> perform the cross-fold validation for me.</p>
<p>Let me create my new data frames. This is the Hull data, so it’s already cleaned up and standardized.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">combined_train</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">train</span><span class="p">,</span> <span class="n">val</span><span class="p">],</span> <span class="n">ignore_index</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">X_combined_train</span> <span class="o">=</span> <span class="n">combined_train</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;Sale Price&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y_combined_train</span><span class="o">=</span> <span class="n">combined_train</span><span class="p">[[</span><span class="s1">&#39;Sale Price&#39;</span><span class="p">]]</span>
</pre></div>
</div>
</div>
</div>
<p>I will now search for the optimal hyperparamter (i.e. alpha) value for a LASSO model using <strong>cross-validation</strong>. I am going to take each alpha (e.g. 0.001) and try it five times in my training data. Each time, I will “hold out” 20% of my training data, estimate a model on 80% of the data using that alpha value. I’ll save my R-Square value from that model as a measure of fit. I’ll then do that four more times for that alpha parameter, holding out a different 20% of my training data each time. That will give me five different R-Square values for each alpha. I’ll then take an average of those R-Square values as my estimate for how well that alpha value performs. Remember, a larger R-Square means that the model “fits” the data better - the model is better able to use your features to explain your targets.</p>
<p>This process is repeated for each alpha value.</p>
<p>We are going to use <code class="docutils literal notranslate"><span class="pre">GridSearchCV</span></code> to do this. From Claude:</p>
<p>GridSearchCV is a class in scikit-learn that helps you perform an exhaustive search over specified parameter values for an estimator (a machine learning model or algorithm). It is used to find the optimal combination of hyperparameters that maximizes the performance of a machine learning model on a given dataset.</p>
<p>And a visual explanation of the process.</p>
<figure class="align-center" id="cross-validation-png">
<img alt="../_images/12-cross-validation.png" src="../_images/12-cross-validation.png" />
<figcaption>
<p><span class="caption-number">Fig. 12.1 </span><span class="caption-text">Holding out different parts of our training data in order to search for optimal hyperparameters. Source:Justintodata.com</span><a class="headerlink" href="#cross-validation-png" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Tune the LASSO hyperparameter (alpha) using the validation set</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;alpha&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">,</span> <span class="mf">.05</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">]}</span>
<span class="n">grid_search</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">lasso</span><span class="p">,</span> <span class="n">param_grid</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;r2&#39;</span><span class="p">)</span>
<span class="n">grid_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_combined_train</span><span class="p">,</span> <span class="n">y_combined_train</span><span class="p">)</span>

<span class="c1"># Extract the R-squared scores for each fold and hyperparameter combination</span>
<span class="n">r2_scores</span> <span class="o">=</span> <span class="n">grid_search</span><span class="o">.</span><span class="n">cv_results_</span><span class="p">[</span><span class="s1">&#39;mean_test_score&#39;</span><span class="p">]</span>

<span class="c1"># Print the R-squared scores</span>
<span class="nb">print</span><span class="p">(</span><span class="n">r2_scores</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0.87838353 0.87833852 0.87472463 0.8567964  0.83113931]
</pre></div>
</div>
</div>
</div>
<p>As we already knew, lower alphas give us better results in our training data. We can use this method to <strong>select a single set of hyperparameters</strong> to then use on our <strong>testing data</strong>.</p>
</section>
<section id="pipelines">
<h3><span class="section-number">12.7.4. </span>Pipelines<a class="headerlink" href="#pipelines" title="Permalink to this headline">#</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">sklearn</span></code> lets us combine many of these steps together. I’m going to <a class="reference external" href="https://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_model_selection.html">follow along with this example</a> and try something a little fancier. We will use some different model fit statistics to perform cross-validation within our training sample and have the pipeline <strong>choose our hyperparameters for us</strong>.</p>
<p>Since I want to use some data that hasn’t been stadardized yet, I’ll go back to the Zillow example. I’ll keep some more numerical variables that will be easy to standardize.</p>
<p>I will not include any indicators or other data transformations. If I was doing this for real, I would spend a lot more time doing some feature engineering.</p>
<p>A few things to note:</p>
<ul class="simple">
<li><p>I’m using <code class="docutils literal notranslate"><span class="pre">make_pipeline</span></code>. This is a very common and useful function that let’s you take your features, scale them, define criteria for model selection, etc. You basically transform your data in a series of steps and then ask for the model.</p></li>
<li><p>I’m asking for the <a class="reference external" href="https://en.wikipedia.org/wiki/Akaike_information_criterion">aic criterion</a>. This is a measure of best fit. I will also ask for the <a class="reference external" href="https://en.wikipedia.org/wiki/Bayesian_information_criterion">bic criterion</a>, another measure of best fit. The smaller the value, the better the model fits the data.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">LassoLarsIC</span></code> tells the code to run the lasso model, which we then fit on our training data. This function is going to pick a bunch of alpha values for us, fit the model, and report the requested criterion back to us.</p></li>
</ul>
<p>Here’s a description of <code class="docutils literal notranslate"><span class="pre">make_pipeline</span></code> from Claude GPT:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">make_pipeline</span></code> is a utility function provided by scikit-learn that creates a Pipeline object, which is a sequence of transformers and an estimator. It is a convenient way to chain multiple estimators into a single unit. The make_pipeline function takes any number of estimators as arguments and returns a Pipeline object. The estimators are applied in the order they are passed to the function. The last estimator is always a classifier or regressor, while the other estimators are transformers (preprocessors).</p></li>
<li><p>Some key features and benefits of using make_pipeline are:</p></li>
<li><p><strong>Convenience</strong>: It provides a simple and readable way to define a sequence of estimators, making the code more concise and easier to maintain.</p></li>
<li><p><strong>Automated Data Flow</strong>: The Pipeline object automatically applies the transformers to the input data and passes the transformed data to the final estimator. This avoids the need to manually handle the data flow between estimators.</p></li>
<li><p><strong>Consistency</strong>: All estimators in the pipeline, except the last one, must be transformers (i.e., have a transform method). This consistency makes it easier to understand and reason about the data flow.</p></li>
<li><p><strong>Persistence</strong>: The entire pipeline can be persisted to disk using the joblib module, making it easy to reuse the same preprocessing and modeling steps in different environments.</p></li>
<li><p><strong>Parallelization</strong>: Some transformers in the pipeline can be parallelized using the n_jobs parameter, potentially leading to faster processing times.</p></li>
<li><p><strong>Cross-Validation</strong>: The entire pipeline can be used in cross-validation loops, ensuring that the same preprocessing steps are applied to both the training and validation data.</p></li>
<li><p>Overall, make_pipeline simplifies the process of chaining multiple estimators together and helps to create more readable, maintainable, and efficient code for machine learning workflows.</p></li>
</ul>
<p>Here’s my <code class="docutils literal notranslate"><span class="pre">make_pipeline</span></code>. I am fitting a LASSO model, where I have it use the AIC criteria to pick the “best” hyperparameter. It then fit the model using our new combined training data.</p>
<p>There’s a little bit of data manipulation that <code class="docutils literal notranslate"><span class="pre">make_pipline</span></code> wants me to do. From Claude:</p>
<p>y_train.values converts the pandas DataFrame y_train to a NumPy array.
.ravel() is then applied to the NumPy array, which flattens the array into a 1-dimensional array (required by the LassoLarsIC estimator).</p>
<p>I need to do a little data cleaning on this Zillow data to get it to work in our pipeline.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">zillow_sample</span> <span class="o">=</span> <span class="n">zillow_data</span><span class="p">[[</span><span class="s1">&#39;logerror&#39;</span><span class="p">,</span><span class="s1">&#39;bathroomcnt&#39;</span><span class="p">,</span> <span class="s1">&#39;bedroomcnt&#39;</span><span class="p">,</span> <span class="s1">&#39;calculatedfinishedsquarefeet&#39;</span><span class="p">,</span> <span class="s1">&#39;fireplacecnt&#39;</span><span class="p">,</span> 
       <span class="s1">&#39;fullbathcnt&#39;</span><span class="p">,</span> <span class="s1">&#39;garagecarcnt&#39;</span><span class="p">,</span> <span class="s1">&#39;garagetotalsqft&#39;</span><span class="p">,</span><span class="s1">&#39;lotsizesquarefeet&#39;</span><span class="p">,</span> 
       <span class="s1">&#39;poolcnt&#39;</span><span class="p">,</span> <span class="s1">&#39;poolsizesum&#39;</span><span class="p">,</span> <span class="s1">&#39;yearbuilt&#39;</span><span class="p">,</span> <span class="s1">&#39;numberofstories&#39;</span><span class="p">]]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">zillow_sample</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>logerror</th>
      <th>bathroomcnt</th>
      <th>bedroomcnt</th>
      <th>calculatedfinishedsquarefeet</th>
      <th>fireplacecnt</th>
      <th>fullbathcnt</th>
      <th>garagecarcnt</th>
      <th>garagetotalsqft</th>
      <th>lotsizesquarefeet</th>
      <th>poolcnt</th>
      <th>poolsizesum</th>
      <th>yearbuilt</th>
      <th>numberofstories</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.0962</td>
      <td>3.0</td>
      <td>2.0</td>
      <td>1798.0</td>
      <td>NaN</td>
      <td>3.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>7302.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>1936.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.0020</td>
      <td>3.0</td>
      <td>4.0</td>
      <td>2302.0</td>
      <td>1.0</td>
      <td>3.0</td>
      <td>2.0</td>
      <td>671.0</td>
      <td>7258.0</td>
      <td>1.0</td>
      <td>500.0</td>
      <td>1980.0</td>
      <td>2.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-0.0566</td>
      <td>2.0</td>
      <td>3.0</td>
      <td>1236.0</td>
      <td>NaN</td>
      <td>2.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>5076.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>1957.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.0227</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>1413.0</td>
      <td>NaN</td>
      <td>2.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>7725.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>1953.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.0237</td>
      <td>3.5</td>
      <td>3.0</td>
      <td>2878.0</td>
      <td>1.0</td>
      <td>3.0</td>
      <td>2.0</td>
      <td>426.0</td>
      <td>10963.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>2003.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>9066</th>
      <td>-0.0305</td>
      <td>3.0</td>
      <td>5.0</td>
      <td>2465.0</td>
      <td>NaN</td>
      <td>3.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>4999.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>1931.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>9067</th>
      <td>0.0266</td>
      <td>3.0</td>
      <td>3.0</td>
      <td>1726.0</td>
      <td>NaN</td>
      <td>3.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>187293.0</td>
      <td>1.0</td>
      <td>NaN</td>
      <td>1981.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>9068</th>
      <td>0.0090</td>
      <td>2.0</td>
      <td>4.0</td>
      <td>1393.0</td>
      <td>NaN</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>451.0</td>
      <td>6279.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>1963.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>9069</th>
      <td>0.0497</td>
      <td>2.0</td>
      <td>3.0</td>
      <td>1967.0</td>
      <td>NaN</td>
      <td>2.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>18129.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>1951.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>9070</th>
      <td>-0.0747</td>
      <td>3.5</td>
      <td>3.0</td>
      <td>3268.0</td>
      <td>NaN</td>
      <td>3.0</td>
      <td>1.0</td>
      <td>396.0</td>
      <td>2875.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>1990.0</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
<p>9071 rows × 13 columns</p>
</div></div></div>
</div>
<p>For some of these counts, missings are actually zeroes. I can glance at the data and see, however, that this isn’t true for all observations. For example we have a poolcnt = 1, but poolsizesum = 0. I’ll go ahead and just fill in the missings for the purpose of this exercise. But, not great!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set missing observations to zero</span>
<span class="n">zillow_sample</span> <span class="o">=</span> <span class="n">zillow_sample</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="n">zillow_sample</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>logerror</th>
      <th>bathroomcnt</th>
      <th>bedroomcnt</th>
      <th>calculatedfinishedsquarefeet</th>
      <th>fireplacecnt</th>
      <th>fullbathcnt</th>
      <th>garagecarcnt</th>
      <th>garagetotalsqft</th>
      <th>lotsizesquarefeet</th>
      <th>poolcnt</th>
      <th>poolsizesum</th>
      <th>yearbuilt</th>
      <th>numberofstories</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.0962</td>
      <td>3.0</td>
      <td>2.0</td>
      <td>1798.0</td>
      <td>0.0</td>
      <td>3.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>7302.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1936.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.0020</td>
      <td>3.0</td>
      <td>4.0</td>
      <td>2302.0</td>
      <td>1.0</td>
      <td>3.0</td>
      <td>2.0</td>
      <td>671.0</td>
      <td>7258.0</td>
      <td>1.0</td>
      <td>500.0</td>
      <td>1980.0</td>
      <td>2.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-0.0566</td>
      <td>2.0</td>
      <td>3.0</td>
      <td>1236.0</td>
      <td>0.0</td>
      <td>2.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>5076.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1957.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.0227</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>1413.0</td>
      <td>0.0</td>
      <td>2.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>7725.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1953.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.0237</td>
      <td>3.5</td>
      <td>3.0</td>
      <td>2878.0</td>
      <td>1.0</td>
      <td>3.0</td>
      <td>2.0</td>
      <td>426.0</td>
      <td>10963.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>2003.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>9066</th>
      <td>-0.0305</td>
      <td>3.0</td>
      <td>5.0</td>
      <td>2465.0</td>
      <td>0.0</td>
      <td>3.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>4999.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1931.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>9067</th>
      <td>0.0266</td>
      <td>3.0</td>
      <td>3.0</td>
      <td>1726.0</td>
      <td>0.0</td>
      <td>3.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>187293.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1981.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>9068</th>
      <td>0.0090</td>
      <td>2.0</td>
      <td>4.0</td>
      <td>1393.0</td>
      <td>0.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>451.0</td>
      <td>6279.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1963.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>9069</th>
      <td>0.0497</td>
      <td>2.0</td>
      <td>3.0</td>
      <td>1967.0</td>
      <td>0.0</td>
      <td>2.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>18129.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1951.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>9070</th>
      <td>-0.0747</td>
      <td>3.5</td>
      <td>3.0</td>
      <td>3268.0</td>
      <td>0.0</td>
      <td>3.0</td>
      <td>1.0</td>
      <td>396.0</td>
      <td>2875.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1990.0</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
<p>9071 rows × 13 columns</p>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">yz</span> <span class="o">=</span> <span class="n">zillow_sample</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s1">&#39;logerror&#39;</span><span class="p">]</span> 
<span class="n">Xz</span> <span class="o">=</span> <span class="n">zillow_sample</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;logerror&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">Xz_train</span><span class="p">,</span> <span class="n">Xz_test</span><span class="p">,</span> <span class="n">yz_train</span><span class="p">,</span> <span class="n">yz_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">Xz</span><span class="p">,</span> <span class="n">yz</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Xz_train</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>bathroomcnt</th>
      <th>bedroomcnt</th>
      <th>calculatedfinishedsquarefeet</th>
      <th>fireplacecnt</th>
      <th>fullbathcnt</th>
      <th>garagecarcnt</th>
      <th>garagetotalsqft</th>
      <th>lotsizesquarefeet</th>
      <th>poolcnt</th>
      <th>poolsizesum</th>
      <th>yearbuilt</th>
      <th>numberofstories</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2919</th>
      <td>1.0</td>
      <td>2.0</td>
      <td>1041.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>360.0</td>
      <td>6120.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1950.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>4298</th>
      <td>1.0</td>
      <td>3.0</td>
      <td>1102.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>6259.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1952.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>5082</th>
      <td>3.5</td>
      <td>6.0</td>
      <td>2856.0</td>
      <td>0.0</td>
      <td>3.0</td>
      <td>1.0</td>
      <td>360.0</td>
      <td>6000.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1959.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>1643</th>
      <td>3.0</td>
      <td>4.0</td>
      <td>1624.0</td>
      <td>0.0</td>
      <td>3.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>8799.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1947.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>7624</th>
      <td>2.0</td>
      <td>2.0</td>
      <td>961.0</td>
      <td>0.0</td>
      <td>2.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>18297.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1981.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>5734</th>
      <td>3.0</td>
      <td>2.0</td>
      <td>1558.0</td>
      <td>0.0</td>
      <td>3.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>29258.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1995.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>5191</th>
      <td>2.0</td>
      <td>4.0</td>
      <td>1405.0</td>
      <td>0.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>441.0</td>
      <td>6120.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1957.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>5390</th>
      <td>2.0</td>
      <td>3.0</td>
      <td>1546.0</td>
      <td>0.0</td>
      <td>2.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>5199.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1918.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>860</th>
      <td>1.0</td>
      <td>1.0</td>
      <td>602.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1366.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1990.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>7270</th>
      <td>1.0</td>
      <td>2.0</td>
      <td>1144.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>4780.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1924.0</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
<p>7256 rows × 12 columns</p>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">yz_train</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2919    0.0247
4298   -0.1120
5082    0.1458
1643   -0.0182
7624    0.1178
         ...  
5734    0.0478
5191    0.0611
5390   -0.0502
860    -0.0060
7270    0.0286
Name: logerror, Length: 7256, dtype: float64
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lasso_lars_ic</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">(),</span> <span class="n">LassoLarsIC</span><span class="p">(</span><span class="n">criterion</span><span class="o">=</span><span class="s2">&quot;aic&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Xz_train</span><span class="p">,</span> <span class="n">yz_train</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">ravel</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">{</span>
        <span class="s2">&quot;alphas&quot;</span><span class="p">:</span> <span class="n">lasso_lars_ic</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">alphas_</span><span class="p">,</span>
        <span class="s2">&quot;AIC criterion&quot;</span><span class="p">:</span> <span class="n">lasso_lars_ic</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">criterion_</span><span class="p">,</span>
    <span class="p">}</span>
<span class="p">)</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s2">&quot;alphas&quot;</span><span class="p">)</span>
<span class="n">alpha_aic</span> <span class="o">=</span> <span class="n">lasso_lars_ic</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">alpha_</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lasso_lars_ic</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="n">lassolarsic__criterion</span><span class="o">=</span><span class="s2">&quot;bic&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Xz_train</span><span class="p">,</span> <span class="n">yz_train</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">ravel</span><span class="p">())</span>
<span class="n">results</span><span class="p">[</span><span class="s2">&quot;BIC criterion&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lasso_lars_ic</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">criterion_</span>
<span class="n">alpha_bic</span> <span class="o">=</span> <span class="n">lasso_lars_ic</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">alpha_</span>
</pre></div>
</div>
</div>
</div>
<p>You can see below that small values of alpha give us the best model fit, no matter the criteria used. This confirms what we saw graphically above.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">highlight_min</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">x_min</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>
    <span class="k">return</span> <span class="p">[</span><span class="s2">&quot;font-weight: bold&quot;</span> <span class="k">if</span> <span class="n">v</span> <span class="o">==</span> <span class="n">x_min</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">x</span><span class="p">]</span>

<span class="n">results</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">highlight_min</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style type="text/css">
#T_3425d_row0_col1, #T_3425d_row9_col0 {
  font-weight: bold;
}
</style>
<table id="T_3425d">
  <thead>
    <tr>
      <th class="blank level0" >&nbsp;</th>
      <th id="T_3425d_level0_col0" class="col_heading level0 col0" >AIC criterion</th>
      <th id="T_3425d_level0_col1" class="col_heading level0 col1" >BIC criterion</th>
    </tr>
    <tr>
      <th class="index_name level0" >alphas</th>
      <th class="blank col0" >&nbsp;</th>
      <th class="blank col1" >&nbsp;</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th id="T_3425d_level0_row0" class="row_heading level0 row0" >0.008193</th>
      <td id="T_3425d_row0_col0" class="data row0 col0" >-6566.197538</td>
      <td id="T_3425d_row0_col1" class="data row0 col1" >-6566.197538</td>
    </tr>
    <tr>
      <th id="T_3425d_level0_row1" class="row_heading level0 row1" >0.006502</th>
      <td id="T_3425d_row1_col0" class="data row1 col0" >-6571.833764</td>
      <td id="T_3425d_row1_col1" class="data row1 col1" >-6564.944180</td>
    </tr>
    <tr>
      <th id="T_3425d_level0_row2" class="row_heading level0 row2" >0.006250</th>
      <td id="T_3425d_row2_col0" class="data row2 col0" >-6570.829907</td>
      <td id="T_3425d_row2_col1" class="data row2 col1" >-6557.050739</td>
    </tr>
    <tr>
      <th id="T_3425d_level0_row3" class="row_heading level0 row3" >0.006217</th>
      <td id="T_3425d_row3_col0" class="data row3 col0" >-6568.983921</td>
      <td id="T_3425d_row3_col1" class="data row3 col1" >-6548.315169</td>
    </tr>
    <tr>
      <th id="T_3425d_level0_row4" class="row_heading level0 row4" >0.005552</th>
      <td id="T_3425d_row4_col0" class="data row4 col0" >-6572.082467</td>
      <td id="T_3425d_row4_col1" class="data row4 col1" >-6551.413715</td>
    </tr>
    <tr>
      <th id="T_3425d_level0_row5" class="row_heading level0 row5" >0.004159</th>
      <td id="T_3425d_row5_col0" class="data row5 col0" >-6577.437720</td>
      <td id="T_3425d_row5_col1" class="data row5 col1" >-6556.768968</td>
    </tr>
    <tr>
      <th id="T_3425d_level0_row6" class="row_heading level0 row6" >0.004015</th>
      <td id="T_3425d_row6_col0" class="data row6 col0" >-6576.114858</td>
      <td id="T_3425d_row6_col1" class="data row6 col1" >-6548.556522</td>
    </tr>
    <tr>
      <th id="T_3425d_level0_row7" class="row_heading level0 row7" >0.002618</th>
      <td id="T_3425d_row7_col0" class="data row7 col0" >-6581.223880</td>
      <td id="T_3425d_row7_col1" class="data row7 col1" >-6546.775960</td>
    </tr>
    <tr>
      <th id="T_3425d_level0_row8" class="row_heading level0 row8" >0.002365</th>
      <td id="T_3425d_row8_col0" class="data row8 col0" >-6580.648359</td>
      <td id="T_3425d_row8_col1" class="data row8 col1" >-6539.310855</td>
    </tr>
    <tr>
      <th id="T_3425d_level0_row9" class="row_heading level0 row9" >0.000653</th>
      <td id="T_3425d_row9_col0" class="data row9 col0" >-6587.044949</td>
      <td id="T_3425d_row9_col1" class="data row9 col1" >-6538.817861</td>
    </tr>
    <tr>
      <th id="T_3425d_level0_row10" class="row_heading level0 row10" >0.000577</th>
      <td id="T_3425d_row10_col0" class="data row10 col0" >-6585.219803</td>
      <td id="T_3425d_row10_col1" class="data row10 col1" >-6530.103131</td>
    </tr>
    <tr>
      <th id="T_3425d_level0_row11" class="row_heading level0 row11" >0.000412</th>
      <td id="T_3425d_row11_col0" class="data row11 col0" >-6583.581262</td>
      <td id="T_3425d_row11_col1" class="data row11 col1" >-6521.575006</td>
    </tr>
    <tr>
      <th id="T_3425d_level0_row12" class="row_heading level0 row12" >0.000239</th>
      <td id="T_3425d_row12_col0" class="data row12 col0" >-6582.063000</td>
      <td id="T_3425d_row12_col1" class="data row12 col1" >-6513.167160</td>
    </tr>
    <tr>
      <th id="T_3425d_level0_row13" class="row_heading level0 row13" >0.000168</th>
      <td id="T_3425d_row13_col0" class="data row13 col0" >-6580.302964</td>
      <td id="T_3425d_row13_col1" class="data row13 col1" >-6504.517540</td>
    </tr>
    <tr>
      <th id="T_3425d_level0_row14" class="row_heading level0 row14" >0.000000</th>
      <td id="T_3425d_row14_col0" class="data row14 col0" >-6580.414299</td>
      <td id="T_3425d_row14_col1" class="data row14 col1" >-6497.739291</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
<p>This table highlights the optimal alpha hyperparameter based on two different selection criteria.</p>
<p>Let’s make a plot. We’ll use <code class="docutils literal notranslate"><span class="pre">matplotlib</span></code> and the <em>ax</em> method.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ax</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span>
    <span class="n">alpha_aic</span><span class="p">,</span>
    <span class="n">results</span><span class="p">[</span><span class="s2">&quot;AIC criterion&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span>
    <span class="n">results</span><span class="p">[</span><span class="s2">&quot;AIC criterion&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&quot;alpha: AIC estimate&quot;</span><span class="p">,</span>
    <span class="n">linestyles</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;tab:blue&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span>
    <span class="n">alpha_bic</span><span class="p">,</span>
    <span class="n">results</span><span class="p">[</span><span class="s2">&quot;BIC criterion&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span>
    <span class="n">results</span><span class="p">[</span><span class="s2">&quot;BIC criterion&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&quot;alpha: BIC estimate&quot;</span><span class="p">,</span>
    <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;tab:orange&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\alpha$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;criterion&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">&quot;Information-criterion for model selection&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/12_regression_topics_104_0.png" src="../_images/12_regression_topics_104_0.png" />
</div>
</div>
<p>This doesn’t look as continuous as it will in practice. The lines are jumping around quite a bit. The reason: We actually don’t have that many features in this Zillow data set that I’m using. Our machine learning tools will work, but are probably unnecessary. Just run a regression!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">modelcv</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">LassoCV</span><span class="p">(</span><span class="n">cv</span><span class="o">=</span><span class="mi">20</span><span class="p">))</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Xz_train</span><span class="p">,</span> <span class="n">yz_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Again, <em>modelcv</em> is a pipeline. A pipeline is an object. This object contains our lasso model fit. We’ll pull this out below and define it as <em>lasso</em>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">modelcv</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>#sk-container-id-5 {
  /* Definition of color scheme common for light and dark mode */
  --sklearn-color-text: black;
  --sklearn-color-line: gray;
  /* Definition of color scheme for unfitted estimators */
  --sklearn-color-unfitted-level-0: #fff5e6;
  --sklearn-color-unfitted-level-1: #f6e4d2;
  --sklearn-color-unfitted-level-2: #ffe0b3;
  --sklearn-color-unfitted-level-3: chocolate;
  /* Definition of color scheme for fitted estimators */
  --sklearn-color-fitted-level-0: #f0f8ff;
  --sklearn-color-fitted-level-1: #d4ebff;
  --sklearn-color-fitted-level-2: #b3dbfd;
  --sklearn-color-fitted-level-3: cornflowerblue;

  /* Specific color for light theme */
  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));
  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-icon: #696969;

  @media (prefers-color-scheme: dark) {
    /* Redefinition of color scheme for dark theme */
    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));
    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-icon: #878787;
  }
}

#sk-container-id-5 {
  color: var(--sklearn-color-text);
}

#sk-container-id-5 pre {
  padding: 0;
}

#sk-container-id-5 input.sk-hidden--visually {
  border: 0;
  clip: rect(1px 1px 1px 1px);
  clip: rect(1px, 1px, 1px, 1px);
  height: 1px;
  margin: -1px;
  overflow: hidden;
  padding: 0;
  position: absolute;
  width: 1px;
}

#sk-container-id-5 div.sk-dashed-wrapped {
  border: 1px dashed var(--sklearn-color-line);
  margin: 0 0.4em 0.5em 0.4em;
  box-sizing: border-box;
  padding-bottom: 0.4em;
  background-color: var(--sklearn-color-background);
}

#sk-container-id-5 div.sk-container {
  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`
     but bootstrap.min.css set `[hidden] { display: none !important; }`
     so we also need the `!important` here to be able to override the
     default hidden behavior on the sphinx rendered scikit-learn.org.
     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */
  display: inline-block !important;
  position: relative;
}

#sk-container-id-5 div.sk-text-repr-fallback {
  display: none;
}

div.sk-parallel-item,
div.sk-serial,
div.sk-item {
  /* draw centered vertical line to link estimators */
  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));
  background-size: 2px 100%;
  background-repeat: no-repeat;
  background-position: center center;
}

/* Parallel-specific style estimator block */

#sk-container-id-5 div.sk-parallel-item::after {
  content: "";
  width: 100%;
  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);
  flex-grow: 1;
}

#sk-container-id-5 div.sk-parallel {
  display: flex;
  align-items: stretch;
  justify-content: center;
  background-color: var(--sklearn-color-background);
  position: relative;
}

#sk-container-id-5 div.sk-parallel-item {
  display: flex;
  flex-direction: column;
}

#sk-container-id-5 div.sk-parallel-item:first-child::after {
  align-self: flex-end;
  width: 50%;
}

#sk-container-id-5 div.sk-parallel-item:last-child::after {
  align-self: flex-start;
  width: 50%;
}

#sk-container-id-5 div.sk-parallel-item:only-child::after {
  width: 0;
}

/* Serial-specific style estimator block */

#sk-container-id-5 div.sk-serial {
  display: flex;
  flex-direction: column;
  align-items: center;
  background-color: var(--sklearn-color-background);
  padding-right: 1em;
  padding-left: 1em;
}


/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is
clickable and can be expanded/collapsed.
- Pipeline and ColumnTransformer use this feature and define the default style
- Estimators will overwrite some part of the style using the `sk-estimator` class
*/

/* Pipeline and ColumnTransformer style (default) */

#sk-container-id-5 div.sk-toggleable {
  /* Default theme specific background. It is overwritten whether we have a
  specific estimator or a Pipeline/ColumnTransformer */
  background-color: var(--sklearn-color-background);
}

/* Toggleable label */
#sk-container-id-5 label.sk-toggleable__label {
  cursor: pointer;
  display: block;
  width: 100%;
  margin-bottom: 0;
  padding: 0.5em;
  box-sizing: border-box;
  text-align: center;
}

#sk-container-id-5 label.sk-toggleable__label-arrow:before {
  /* Arrow on the left of the label */
  content: "▸";
  float: left;
  margin-right: 0.25em;
  color: var(--sklearn-color-icon);
}

#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {
  color: var(--sklearn-color-text);
}

/* Toggleable content - dropdown */

#sk-container-id-5 div.sk-toggleable__content {
  max-height: 0;
  max-width: 0;
  overflow: hidden;
  text-align: left;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-5 div.sk-toggleable__content.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-5 div.sk-toggleable__content pre {
  margin: 0.2em;
  border-radius: 0.25em;
  color: var(--sklearn-color-text);
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-5 div.sk-toggleable__content.fitted pre {
  /* unfitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {
  /* Expand drop-down */
  max-height: 200px;
  max-width: 100%;
  overflow: auto;
}

#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {
  content: "▾";
}

/* Pipeline/ColumnTransformer-specific style */

#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-5 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator-specific style */

/* Colorize estimator box */
#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-5 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

#sk-container-id-5 div.sk-label label.sk-toggleable__label,
#sk-container-id-5 div.sk-label label {
  /* The background is the default theme color */
  color: var(--sklearn-color-text-on-default-background);
}

/* On hover, darken the color of the background */
#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

/* Label box, darken color on hover, fitted */
#sk-container-id-5 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator label */

#sk-container-id-5 div.sk-label label {
  font-family: monospace;
  font-weight: bold;
  display: inline-block;
  line-height: 1.2em;
}

#sk-container-id-5 div.sk-label-container {
  text-align: center;
}

/* Estimator-specific */
#sk-container-id-5 div.sk-estimator {
  font-family: monospace;
  border: 1px dotted var(--sklearn-color-border-box);
  border-radius: 0.25em;
  box-sizing: border-box;
  margin-bottom: 0.5em;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-5 div.sk-estimator.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

/* on hover */
#sk-container-id-5 div.sk-estimator:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-5 div.sk-estimator.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Specification for estimator info (e.g. "i" and "?") */

/* Common style for "i" and "?" */

.sk-estimator-doc-link,
a:link.sk-estimator-doc-link,
a:visited.sk-estimator-doc-link {
  float: right;
  font-size: smaller;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1em;
  height: 1em;
  width: 1em;
  text-decoration: none !important;
  margin-left: 1ex;
  /* unfitted */
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
  color: var(--sklearn-color-unfitted-level-1);
}

.sk-estimator-doc-link.fitted,
a:link.sk-estimator-doc-link.fitted,
a:visited.sk-estimator-doc-link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
div.sk-estimator:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover,
div.sk-label-container:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover,
div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

/* Span, style for the box shown on hovering the info icon */
.sk-estimator-doc-link span {
  display: none;
  z-index: 9999;
  position: relative;
  font-weight: normal;
  right: .2ex;
  padding: .5ex;
  margin: .5ex;
  width: min-content;
  min-width: 20ex;
  max-width: 50ex;
  color: var(--sklearn-color-text);
  box-shadow: 2pt 2pt 4pt #999;
  /* unfitted */
  background: var(--sklearn-color-unfitted-level-0);
  border: .5pt solid var(--sklearn-color-unfitted-level-3);
}

.sk-estimator-doc-link.fitted span {
  /* fitted */
  background: var(--sklearn-color-fitted-level-0);
  border: var(--sklearn-color-fitted-level-3);
}

.sk-estimator-doc-link:hover span {
  display: block;
}

/* "?"-specific style due to the `<a>` HTML tag */

#sk-container-id-5 a.estimator_doc_link {
  float: right;
  font-size: 1rem;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1rem;
  height: 1rem;
  width: 1rem;
  text-decoration: none;
  /* unfitted */
  color: var(--sklearn-color-unfitted-level-1);
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
}

#sk-container-id-5 a.estimator_doc_link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
#sk-container-id-5 a.estimator_doc_link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

#sk-container-id-5 a.estimator_doc_link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
}
</style><div id="sk-container-id-5" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>Pipeline(steps=[(&#x27;lassocv&#x27;, LassoCV(cv=20))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-5" type="checkbox" ><label for="sk-estimator-id-5" class="sk-toggleable__label fitted sk-toggleable__label-arrow fitted">&nbsp;&nbsp;Pipeline<a class="sk-estimator-doc-link fitted" rel="noreferrer" target="_blank" href="https://scikit-learn.org/1.4/modules/generated/sklearn.pipeline.Pipeline.html">?<span>Documentation for Pipeline</span></a><span class="sk-estimator-doc-link fitted">i<span>Fitted</span></span></label><div class="sk-toggleable__content fitted"><pre>Pipeline(steps=[(&#x27;lassocv&#x27;, LassoCV(cv=20))])</pre></div> </div></div><div class="sk-serial"><div class="sk-item"><div class="sk-estimator fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-6" type="checkbox" ><label for="sk-estimator-id-6" class="sk-toggleable__label fitted sk-toggleable__label-arrow fitted">&nbsp;LassoCV<a class="sk-estimator-doc-link fitted" rel="noreferrer" target="_blank" href="https://scikit-learn.org/1.4/modules/generated/sklearn.linear_model.LassoCV.html">?<span>Documentation for LassoCV</span></a></label><div class="sk-toggleable__content fitted"><pre>LassoCV(cv=20)</pre></div> </div></div></div></div></div></div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lasso</span> <span class="o">=</span> <span class="n">modelcv</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">lasso</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>#sk-container-id-6 {
  /* Definition of color scheme common for light and dark mode */
  --sklearn-color-text: black;
  --sklearn-color-line: gray;
  /* Definition of color scheme for unfitted estimators */
  --sklearn-color-unfitted-level-0: #fff5e6;
  --sklearn-color-unfitted-level-1: #f6e4d2;
  --sklearn-color-unfitted-level-2: #ffe0b3;
  --sklearn-color-unfitted-level-3: chocolate;
  /* Definition of color scheme for fitted estimators */
  --sklearn-color-fitted-level-0: #f0f8ff;
  --sklearn-color-fitted-level-1: #d4ebff;
  --sklearn-color-fitted-level-2: #b3dbfd;
  --sklearn-color-fitted-level-3: cornflowerblue;

  /* Specific color for light theme */
  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));
  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-icon: #696969;

  @media (prefers-color-scheme: dark) {
    /* Redefinition of color scheme for dark theme */
    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));
    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-icon: #878787;
  }
}

#sk-container-id-6 {
  color: var(--sklearn-color-text);
}

#sk-container-id-6 pre {
  padding: 0;
}

#sk-container-id-6 input.sk-hidden--visually {
  border: 0;
  clip: rect(1px 1px 1px 1px);
  clip: rect(1px, 1px, 1px, 1px);
  height: 1px;
  margin: -1px;
  overflow: hidden;
  padding: 0;
  position: absolute;
  width: 1px;
}

#sk-container-id-6 div.sk-dashed-wrapped {
  border: 1px dashed var(--sklearn-color-line);
  margin: 0 0.4em 0.5em 0.4em;
  box-sizing: border-box;
  padding-bottom: 0.4em;
  background-color: var(--sklearn-color-background);
}

#sk-container-id-6 div.sk-container {
  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`
     but bootstrap.min.css set `[hidden] { display: none !important; }`
     so we also need the `!important` here to be able to override the
     default hidden behavior on the sphinx rendered scikit-learn.org.
     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */
  display: inline-block !important;
  position: relative;
}

#sk-container-id-6 div.sk-text-repr-fallback {
  display: none;
}

div.sk-parallel-item,
div.sk-serial,
div.sk-item {
  /* draw centered vertical line to link estimators */
  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));
  background-size: 2px 100%;
  background-repeat: no-repeat;
  background-position: center center;
}

/* Parallel-specific style estimator block */

#sk-container-id-6 div.sk-parallel-item::after {
  content: "";
  width: 100%;
  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);
  flex-grow: 1;
}

#sk-container-id-6 div.sk-parallel {
  display: flex;
  align-items: stretch;
  justify-content: center;
  background-color: var(--sklearn-color-background);
  position: relative;
}

#sk-container-id-6 div.sk-parallel-item {
  display: flex;
  flex-direction: column;
}

#sk-container-id-6 div.sk-parallel-item:first-child::after {
  align-self: flex-end;
  width: 50%;
}

#sk-container-id-6 div.sk-parallel-item:last-child::after {
  align-self: flex-start;
  width: 50%;
}

#sk-container-id-6 div.sk-parallel-item:only-child::after {
  width: 0;
}

/* Serial-specific style estimator block */

#sk-container-id-6 div.sk-serial {
  display: flex;
  flex-direction: column;
  align-items: center;
  background-color: var(--sklearn-color-background);
  padding-right: 1em;
  padding-left: 1em;
}


/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is
clickable and can be expanded/collapsed.
- Pipeline and ColumnTransformer use this feature and define the default style
- Estimators will overwrite some part of the style using the `sk-estimator` class
*/

/* Pipeline and ColumnTransformer style (default) */

#sk-container-id-6 div.sk-toggleable {
  /* Default theme specific background. It is overwritten whether we have a
  specific estimator or a Pipeline/ColumnTransformer */
  background-color: var(--sklearn-color-background);
}

/* Toggleable label */
#sk-container-id-6 label.sk-toggleable__label {
  cursor: pointer;
  display: block;
  width: 100%;
  margin-bottom: 0;
  padding: 0.5em;
  box-sizing: border-box;
  text-align: center;
}

#sk-container-id-6 label.sk-toggleable__label-arrow:before {
  /* Arrow on the left of the label */
  content: "▸";
  float: left;
  margin-right: 0.25em;
  color: var(--sklearn-color-icon);
}

#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {
  color: var(--sklearn-color-text);
}

/* Toggleable content - dropdown */

#sk-container-id-6 div.sk-toggleable__content {
  max-height: 0;
  max-width: 0;
  overflow: hidden;
  text-align: left;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-6 div.sk-toggleable__content.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-6 div.sk-toggleable__content pre {
  margin: 0.2em;
  border-radius: 0.25em;
  color: var(--sklearn-color-text);
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-6 div.sk-toggleable__content.fitted pre {
  /* unfitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {
  /* Expand drop-down */
  max-height: 200px;
  max-width: 100%;
  overflow: auto;
}

#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {
  content: "▾";
}

/* Pipeline/ColumnTransformer-specific style */

#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-6 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator-specific style */

/* Colorize estimator box */
#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-6 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

#sk-container-id-6 div.sk-label label.sk-toggleable__label,
#sk-container-id-6 div.sk-label label {
  /* The background is the default theme color */
  color: var(--sklearn-color-text-on-default-background);
}

/* On hover, darken the color of the background */
#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

/* Label box, darken color on hover, fitted */
#sk-container-id-6 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator label */

#sk-container-id-6 div.sk-label label {
  font-family: monospace;
  font-weight: bold;
  display: inline-block;
  line-height: 1.2em;
}

#sk-container-id-6 div.sk-label-container {
  text-align: center;
}

/* Estimator-specific */
#sk-container-id-6 div.sk-estimator {
  font-family: monospace;
  border: 1px dotted var(--sklearn-color-border-box);
  border-radius: 0.25em;
  box-sizing: border-box;
  margin-bottom: 0.5em;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-6 div.sk-estimator.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

/* on hover */
#sk-container-id-6 div.sk-estimator:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-6 div.sk-estimator.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Specification for estimator info (e.g. "i" and "?") */

/* Common style for "i" and "?" */

.sk-estimator-doc-link,
a:link.sk-estimator-doc-link,
a:visited.sk-estimator-doc-link {
  float: right;
  font-size: smaller;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1em;
  height: 1em;
  width: 1em;
  text-decoration: none !important;
  margin-left: 1ex;
  /* unfitted */
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
  color: var(--sklearn-color-unfitted-level-1);
}

.sk-estimator-doc-link.fitted,
a:link.sk-estimator-doc-link.fitted,
a:visited.sk-estimator-doc-link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
div.sk-estimator:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover,
div.sk-label-container:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover,
div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

/* Span, style for the box shown on hovering the info icon */
.sk-estimator-doc-link span {
  display: none;
  z-index: 9999;
  position: relative;
  font-weight: normal;
  right: .2ex;
  padding: .5ex;
  margin: .5ex;
  width: min-content;
  min-width: 20ex;
  max-width: 50ex;
  color: var(--sklearn-color-text);
  box-shadow: 2pt 2pt 4pt #999;
  /* unfitted */
  background: var(--sklearn-color-unfitted-level-0);
  border: .5pt solid var(--sklearn-color-unfitted-level-3);
}

.sk-estimator-doc-link.fitted span {
  /* fitted */
  background: var(--sklearn-color-fitted-level-0);
  border: var(--sklearn-color-fitted-level-3);
}

.sk-estimator-doc-link:hover span {
  display: block;
}

/* "?"-specific style due to the `<a>` HTML tag */

#sk-container-id-6 a.estimator_doc_link {
  float: right;
  font-size: 1rem;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1rem;
  height: 1rem;
  width: 1rem;
  text-decoration: none;
  /* unfitted */
  color: var(--sklearn-color-unfitted-level-1);
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
}

#sk-container-id-6 a.estimator_doc_link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
#sk-container-id-6 a.estimator_doc_link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

#sk-container-id-6 a.estimator_doc_link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
}
</style><div id="sk-container-id-6" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>LassoCV(cv=20)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-7" type="checkbox" checked><label for="sk-estimator-id-7" class="sk-toggleable__label fitted sk-toggleable__label-arrow fitted">&nbsp;&nbsp;LassoCV<a class="sk-estimator-doc-link fitted" rel="noreferrer" target="_blank" href="https://scikit-learn.org/1.4/modules/generated/sklearn.linear_model.LassoCV.html">?<span>Documentation for LassoCV</span></a><span class="sk-estimator-doc-link fitted">i<span>Fitted</span></span></label><div class="sk-toggleable__content fitted"><pre>LassoCV(cv=20)</pre></div> </div></div></div></div></div></div>
</div>
<p>OK, what did we do here? Let’s have Claude tell us:</p>
<p>In scikit-learn, LassoCV(cv=20) refers to the Lasso regression model with cross-validation for tuning the regularization parameter (alpha). Here’s what the components mean:</p>
<ul class="simple">
<li><p>LassoCV: This is the class name for the Lasso regression model with built-in cross-validation for automatically tuning the regularization strength parameter (alpha).</p></li>
<li><p>cv=20: This is a parameter that specifies the number of folds for cross-validation. In this case, it means that the cross-validation process will be repeated 20 times, each time using a different 1/20th of the data as the validation set and the remaining 19/20ths as the training set.</p></li>
<li><p>The LassoCV class performs the following steps:</p></li>
</ul>
<ol class="simple">
<li><p>It splits the data into cv (20 in this case) folds or subsets.</p></li>
<li><p>For each fold:</p></li>
</ol>
<p>2a. It holds out that fold as the validation set.</p>
<p>2b. It trains the Lasso regression model on the remaining cv-1 folds, using a range of different alpha values.</p>
<p>2c. It evaluates the performance of the model on the held-out validation fold for each alpha value.</p>
<ul class="simple">
<li><p>After all folds have been processed, it selects the alpha value that gives the best average performance across all folds.</p></li>
<li><p>Finally, it retrains the Lasso regression model on the entire dataset using the optimal alpha value found through cross-validation.</p></li>
</ul>
<p>By using cross-validation, LassoCV can automatically find the optimal value of the regularization parameter (alpha) that balances the trade-off between model complexity and goodness of fit. This helps to prevent overfitting and improve the model’s generalization performance. The choice of cv = 20 is a common practice, as it provides a good balance between computational cost and reliable estimation of the model’s performance. However, the optimal value of cv can depend on the size of the dataset and the complexity of the problem.</p>
<p>I’m going to print my mse values in order to find some reasonable min and max values for the graph below. Remember, our target is the log of the pricing error. This isn’t a dollar amount for the house.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lasso</span><span class="o">.</span><span class="n">mse_path_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[0.01496657, 0.0125596 , 0.01200737, ..., 0.02217262, 0.01876269,
        0.04859113],
       [0.01496652, 0.01255985, 0.01200748, ..., 0.02217262, 0.01876246,
        0.04859252],
       [0.01496649, 0.01256036, 0.01200758, ..., 0.02217262, 0.01876225,
        0.04859384],
       ...,
       [0.01484608, 0.01278774, 0.01219113, ..., 0.02204339, 0.01876977,
        0.04869556],
       [0.01484857, 0.01278637, 0.01219214, ..., 0.02204383, 0.0187697 ,
        0.04870231],
       [0.01485083, 0.01278511, 0.0121931 , ..., 0.02204417, 0.01876976,
        0.04870873]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ymin</span><span class="p">,</span> <span class="n">ymax</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.2</span>

<span class="n">plt</span><span class="o">.</span><span class="n">semilogx</span><span class="p">(</span><span class="n">lasso</span><span class="o">.</span><span class="n">alphas_</span><span class="p">,</span> <span class="n">lasso</span><span class="o">.</span><span class="n">mse_path_</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;:&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">lasso</span><span class="o">.</span><span class="n">alphas_</span><span class="p">,</span>
    <span class="n">lasso</span><span class="o">.</span><span class="n">mse_path_</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Average across the folds&quot;</span><span class="p">,</span>
    <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">lasso</span><span class="o">.</span><span class="n">alpha_</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;alpha: CV estimate&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="n">ymin</span><span class="p">,</span> <span class="n">ymax</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\alpha$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Mean square error&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">&quot;Mean square error on each fold: coordinate descent&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/12_regression_topics_113_0.png" src="../_images/12_regression_topics_113_0.png" />
</div>
</div>
</section>
<section id="hyperparamters-and-model-overfit">
<h3><span class="section-number">12.7.5. </span>Hyperparamters and model overfit<a class="headerlink" href="#hyperparamters-and-model-overfit" title="Permalink to this headline">#</a></h3>
<p>Again, what’s the point of all of this? We’re trying to find our model <strong>hyperparameters</strong> that we can then take to data outside of our training set. We are trying to avoid <strong>overfitting</strong>. This is a problem in machine learning and, especially, in financial applications. It is easy to look at past financial and fit a model that will predict returns <strong>in sample</strong>, in the data we are using. But, will this work on new data? If you overfit your model, you risk making precisely incorrect predictions.</p>
<p>Here’s a <a class="reference external" href="https://scikit-learn.org/stable/modules/cross_validation.html">nice explanation</a> on what is going on. This is all part of a much larger discussion on <a class="reference external" href="https://scikit-learn.org/stable/model_selection.html">model selection</a></p>
</section>
<section id="interpreting-linear-regression-models">
<h3><span class="section-number">12.7.6. </span>Interpreting linear regression models<a class="headerlink" href="#interpreting-linear-regression-models" title="Permalink to this headline">#</a></h3>
<p>Chapter 10.1 discusses how to interpret our results. Traditionally, machine learning hasn’t been as interested in interpreting the model - we just want predicted values! However, as machine learning, statistics, econometrics, and specific domain knowledge (i.e. finance) mix, we are becoming more interested in exactly how to interpret our results.</p>
<p>As the test notes, you can’t tell a loan applicant that they got rejected because “the algo said so”! You need to know why. This also gets into biases in our data. Is your model doing something illegal and/or unethical?</p>
<p>From the Hull text:</p>
<blockquote>
<div><p>The weight, <span class="math notranslate nohighlight">\(b_j\)</span>, can be interpreted as the sensitivity of the prediction to the value of the feature <span class="math notranslate nohighlight">\(j\)</span>. If the value of feature <span class="math notranslate nohighlight">\(j\)</span> increases by an amount <span class="math notranslate nohighlight">\(u\)</span> with all other features remaining the same, the value of the target increases by <span class="math notranslate nohighlight">\(b_{j,u}\)</span>. In the case of categorical features that are 1 or 0, the  weight gives the impact on a prediction of the target of changing the  category of the feature when all other features are kept the same.</p>
</div></blockquote>
<blockquote>
<div><p>The bias, <span class="math notranslate nohighlight">\(a\)</span>, is a little more difficult to interpret. It is the value of the target if all the feature values are zero. However, feature values of zero make no sense in many situations.</p>
</div></blockquote>
<p>The text suggests setting all of the X features at the average value and then adding the intercept, <span class="math notranslate nohighlight">\(a\)</span>. This <span class="math notranslate nohighlight">\(a^*\)</span> value is the predicted y if all features are at their mean.</p>
<p>Note that to do this, you should use the model results with the unscaled data. From the text:</p>
<blockquote>
<div><p>To improve interpretability, the Z-score scaling used for Table 3.7 has been reversed in Table 10.1. (It will be recalled that scaling was necessary for Lasso.) This means that the feature weights in Table 3.7 are divided by the standard deviation of the feature and multiplied by the standard deviation of the house price to get the feature weights in Table 10.1.</p>
</div></blockquote>
<p>As mentioned above, going back and forth between scaled and unscaled features is easier if you just let <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> do it for you.</p>
<p>This is also a <a class="reference external" href="https://scikit-learn.org/stable/auto_examples/inspection/plot_linear_model_coefficient_interpretation.html#sphx-glr-auto-examples-inspection-plot-linear-model-coefficient-interpretation-py">nice summary</a> of how to interpret regression results.</p>
</section>
<section id="some-worked-examples">
<h3><span class="section-number">12.7.7. </span>Some worked examples<a class="headerlink" href="#some-worked-examples" title="Permalink to this headline">#</a></h3>
<p>We are just scratching the surface on machine learning in this course. Here are some complete examples that all use housing price data. Some things to pay attention to:</p>
<ul class="simple">
<li><p>How are the data organized? How is it split into training and test data?</p></li>
<li><p>How is the data quality checked? How are missing values handled? How about outliers?</p></li>
<li><p>What feature engineering steps are used? Are logs taken? Are there categorical variables in the regressions? Are the variables scaled?</p></li>
<li><p>What visualizations are used to look at the data before modeling? Why are they used?</p></li>
<li><p>What models are used? How are they compared?</p></li>
</ul>
<p><a class="reference external" href="https://www.kaggle.com/code/erick5/predicting-house-prices-with-machine-learning/notebook">Example One.</a></p>
<p><a class="reference external" href="https://www.dataquest.io/blog/kaggle-getting-started/">Example Two.</a></p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./chapters"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="11_factor_models.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">11. </span>Factor models <a id='factor_models'></a></p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="13_logit_credit.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">13. </span>Logit models</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Prof. Adam Aiken<br/>
  
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>